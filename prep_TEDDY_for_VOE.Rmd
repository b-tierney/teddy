---
title: "process_TEDDY_for_voe"
author: "Sam Zimmerman"
date: "3/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{bash}
#Analysis steps post alignment with bowtie2

#Step 1. Normalize genes in each sample by total number of aligned reads in a sample 

# location_file is a file where each line is the name of a gz file containing the number of reads that align to each gene. one line/file per sample.

cd /n/scratch3/users/a/adk9/_RESTORE/adk9/TEDDY/alignment_output

normalize_teddy.py  <location_file>

# e.g.
#normalize_teddy.py SRR7559403_alignment_data.tsv.gz
```

#Step 2. extract gene names using quick python

```{python}

cd /n/scratch3/users/a/adk9/_RESTORE/adk9/TEDDY/alignment_output

data = pd.read_csv("SRR7556756_alignment_data.tsv",sep='\t',index_col=0,header=None)
geneNames=data.index
geneNames_df = pd.DataFrame(geneNames.values,columns=['genename'])
geneNames_df.to_csv('gene-names_alignment_data_normalized.tsv',index=False)
```


#Step 3. divide each normalized abundnce file into several files of 50000 genes per file. will create new folders with file in them in current working directory

```{bash}
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments

#./parse_normalized_abundance_data.sh
./parse_normalized_abundance_data_other_path.sh
```

#Step 4. also split gene names

```{bash}

mkdir gene-names_batched
split -l 50000 /n/scratch3/users/a/adk9/TEDDY/alignment_output/normalized_data/gene-names_alignment_data_normalized.tsv gene-names_batched/gene_names_

```

#Step 5. merge normalized data. so we have Run by gene matrices. 

```{bash}

mkdir parsed_data
mkdir merge_noramized_data_input_files
ls *_locs | grep -v "all_batch_locs" | grep -v "gene_name_locs" > merge_noramized_data_input_files/all_locs
cd merge_noramized_data_input_files
split -l 5 all_locs all_locs_
cd ..
for x in merge_noramized_data_input_files/all_locs_* 
do
sbatch -c 1 -t 0-11:59 -p short --mem=20G merge_normalized_data_batch.bash ${x}
done 
# rerun one that timed out
sbatch -c 1 -t 0-11:59 -p short --mem=20G merge_normalized_data_batch.bash merge_noramized_data_input_files/all_locs_iv_only

sbatch -c 1 -t 0-11:59 -p short --mem=20G merge_normalized_data_batch.bash merge_noramized_data_input_files/all_locs_mc_only

```

# step 6 and 7 run code in process_teddy_metadatav2.R so I get the case and controls as well as mapping files of subjects to samples. Located in metadata folder of github.

#step 8. average samples that belong to the same subject 

#first prepare input files. uses output of process_teddy_metadatav2.R

```{r}
# first get input files
all_files = list.files(pattern=".csv")
abundance_files = all_files[-grep("healthy",all_files)]
mapping_files = all_files[grepl("healthy",all_files) & grepl(".mapping.",all_files)]
metadata_files = gsub(".mapping.csv",".csv",mapping_files)
mapping_metadata = data.frame(metadata_files,mapping_files)

df_lists = apply(mapping_metadata,1, function(myrow) {
  metadata_file = myrow[1]
  mapping_file = myrow[2]
  metadata_files_rep = rep(metadata_file,length(abundance_files))
  mapping_files_rep = rep(mapping_file,length(abundance_files))
  my_df = data.frame(abundance_files,metadata_files_rep,mapping_files_rep)
  # split into 2 pieces
  split_point = floor(nrow(my_df)/2)
  my_df1 = my_df[1:split_point,]
  my_df2 = my_df[(split_point +1):nrow(my_df),]
  # this extra column is just so I don't mess with the metadata since I am doing things in parallel
  my_df1$metadata_suffix = 1
  my_df2$metadata_suffix = 2
  my_label1 = gsub(".csv","_input_file_1.tsv",metadata_file)
  my_label2 = gsub(".csv","_input_file_2.tsv",metadata_file)
  write.table(my_df1,file=paste("prep_voe_input_files/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
  write.table(my_df2,file=paste("prep_voe_input_files/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
  return(my_df)
})

```

#Prepare input files for other associations besides pre-t1d

```{r}
# # first get input files
# all_files = list.files(pattern=".csv")
# abundance_files = all_files[-grep("healthy",all_files)]
# mapping_files = all_files[grepl("healthy",all_files) & grepl(".mapping.",all_files)]
# mapping_files = mapping_files[-grep("healthy_pre-t1d",mapping_files)]
# metadata_files = gsub(".mapping.csv",".csv",mapping_files)
# mapping_metadata = data.frame(metadata_files,mapping_files)
# 
# df_lists = apply(mapping_metadata,1, function(myrow) {
#   metadata_file = myrow[1]
#   mapping_file = myrow[2]
#   metadata_files_rep = rep(metadata_file,length(abundance_files))
#   mapping_files_rep = rep(mapping_file,length(abundance_files))
#   my_df = data.frame(abundance_files,metadata_files_rep,mapping_files_rep)
#   # split into 2 pieces
#   split_point = floor(nrow(my_df)/2)
#   my_df1 = my_df[1:split_point,]
#   my_df2 = my_df[(split_point +1):nrow(my_df),]
#   # this extra column is just so I don't mess with the metadata since I am doing things in parallel
#   my_df1$metadata_suffix = 1
#   my_df2$metadata_suffix = 2
#   my_label1 = gsub(".csv","_input_file_1.tsv",metadata_file)
#   my_label2 = gsub(".csv","_input_file_2.tsv",metadata_file)
#   write.table(my_df1,file=paste("prep_voe_input_files_antibody_associations/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#   write.table(my_df2,file=paste("prep_voe_input_files_antibody_associations/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#   return(my_df)
# })

```

#Also we are doing another version where I only do associations on a training set. So lets get the input for those

```{r}

# setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2")
# # first get input files
# 
# abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern=".csv",full.names = TRUE)
# abundance_files = abundance_files[-grep("healthy",abundance_files)]
# mapping_files = list.files(pattern = ".mapping.csv")
# metadata_files = gsub(".mapping.csv",".csv",mapping_files)
# training_data_files = gsub(".mapping.csv",".train_subjects.txt",mapping_files)
# testing_data_files = gsub(".mapping.csv",".test_subjects.txt",mapping_files)
# mapping_metadata = data.frame(metadata_files,mapping_files,training_data_files,testing_data_files)
# 
# df_lists = apply(mapping_metadata,1, function(myrow) {
#   metadata_file = myrow[1]
#   mapping_file = myrow[2]
#   training_file = myrow[3]
#   testing_file = myrow[4]
#   metadata_files_rep = rep(metadata_file,length(abundance_files))
#   mapping_files_rep = rep(mapping_file,length(abundance_files))
#   training_files_rep = rep(training_file,length(abundance_files))
#   testing_files_rep = rep(testing_file,length(abundance_files))
#   my_df = data.frame(abundance_files,metadata_files_rep,mapping_files_rep,training_files_rep,testing_files_rep)
#   # split into 2 pieces
#   split_point = floor(nrow(my_df)/2)
#   my_df1 = my_df[1:split_point,]
#   my_df2 = my_df[(split_point +1):nrow(my_df),]
#   # this extra column is just so I don't mess with the metadata since I am doing things in parallel
#   my_df1$metadata_suffix = 1
#   my_df2$metadata_suffix = 2
#   my_label1 = gsub(".csv","_input_file_1.tsv",metadata_file)
#   my_label2 = gsub(".csv","_input_file_2.tsv",metadata_file)
#   write.table(my_df1,file=paste("prep_voe_input_files/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#   write.table(my_df2,file=paste("prep_voe_input_files/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#   return(my_df)
# })

```

#Do a third version where I do 3 fold CV

```{r}
# setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3")
# 
# abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern=".csv",full.names = TRUE)
# abundance_files = abundance_files[-grep("healthy",abundance_files)]
# abundance_files = abundance_files[-grep("total_genes_left_each_timepoint.csv",abundance_files)]
# mapping_files = list.files(pattern = ".mapping.csv")
# metadata_files = gsub(".mapping.csv",".csv",mapping_files)
# training_data_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66.train_subjects.txt",mapping_files)
# training_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.train_subjects.txt",mapping_files)
# testing_data_files_two_third_train = gsub(".mapping.csv","_proportion_train_0.66.test_subjects.txt",mapping_files)
# testing_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.test_subjects.txt",mapping_files)
# 
# mapping_metadata_two_third_train = data.frame(metadata_files,mapping_files,training_data_files_two_thirds_train,testing_data_files_two_third_train)
# 
# mapping_metadata_fifty_fity_train = data.frame(metadata_files,mapping_files,training_data_files_fifty_train,testing_data_files_fifty_train)
# 
# write_output_files = function(mapping_metadata) {
#   df_lists = apply(mapping_metadata, 1, function(myrow) {
#     metadata_file = myrow[1]
#     mapping_file = myrow[2]
#     training_file = myrow[3]
#     testing_file = myrow[4]
#     metadata_files_rep = rep(metadata_file,length(abundance_files))
#     mapping_files_rep = rep(mapping_file,length(abundance_files))
#     training_files_rep = rep(training_file,length(abundance_files))
#     testing_files_rep = rep(testing_file,length(abundance_files))
#     my_df = data.frame(abundance_files,metadata_files_rep,mapping_files_rep,training_files_rep,testing_files_rep)
#     # split into 2 pieces
#     split_point = floor(nrow(my_df)/2)
#     my_df1 = my_df[1:split_point,]
#     my_df2 = my_df[(split_point +1):nrow(my_df),]
#     # this extra column is just so I don't mess with the metadata since I am doing things in parallel
#     my_df1$metadata_suffix = 1
#     my_df2$metadata_suffix = 2
#     my_label1 = gsub(".train_subjects.txt","_input_file_1.tsv",training_file)
#     my_label2 = gsub(".train_subjects.txt","_input_file_2.tsv",training_file)
#     write.table(my_df1,file=paste("prep_voe_input_files/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#     write.table(my_df2,file=paste("prep_voe_input_files/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#     return(my_df)
#   })
#   return(df_lists)
# }
# output_files_two_thirds_train = write_output_files(mapping_metadata=mapping_metadata_two_third_train)
# output_files_fifty_train = write_output_files(mapping_metadata=mapping_metadata_fifty_fity_train)


#df_lists = apply(mapping_metadata,1, function(myrow) {
#  metadata_file = myrow[1]
#  mapping_file = myrow[2]
#  training_file = myrow[3]
#  testing_file = myrow[4]
#  metadata_files_rep = rep(metadata_file,length(abundance_files))
#  mapping_files_rep = rep(mapping_file,length(abundance_files))
#  training_files_rep = rep(training_file,length(abundance_files))
#  testing_files_rep = rep(testing_file,length(abundance_files))
#  my_df = data.frame(abundance_files,metadata_files_rep,mapping_files_rep,training_files_rep,testing_files_rep)
#  # split into 2 pieces
#  split_point = floor(nrow(my_df)/2)
#  my_df1 = my_df[1:split_point,]
#  my_df2 = my_df[(split_point +1):nrow(my_df),]
#  # this extra column is just so I don't mess with the metadata since I am doing things in parallel
#  my_df1$metadata_suffix = 1
#  my_df2$metadata_suffix = 2
#  my_label1 = gsub(".csv","_input_file_1.tsv",metadata_file)
#  my_label2 = gsub(".csv","_input_file_2.tsv",metadata_file)
#  write.table(my_df1,file=paste("prep_voe_input_files/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#  write.table(my_df2,file=paste("prep_voe_input_files/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
#  return(my_df)
#})

```

#Do a 4th version. 

```{r}
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4")

abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern=".csv",full.names = TRUE)
abundance_files = abundance_files[-grep("healthy",abundance_files)]
abundance_files = abundance_files[-grep("total_genes_left_each_timepoint.csv",abundance_files)]
mapping_files = list.files(pattern = ".mapping.csv")
#train_metadata_files = gsub(".mapping.csv","_train_metadata.csv",mapping_files)
#test_metadata_files = gsub(".mapping.csv","_test_metadata.csv",mapping_files)

training_metadata_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66_train_metadata.csv",mapping_files)
training_metadata_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5_train_metadata.csv",mapping_files)
testing_metadata_files_two_third_train = gsub(".mapping.csv","_proportion_train_0.66_test_metadata.csv",mapping_files)
testing_metadata_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5_test_metadata.csv",mapping_files)


training_data_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66.train_subjects.txt",mapping_files)
training_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.train_subjects.txt",mapping_files)
testing_data_files_two_third_train = gsub(".mapping.csv","_proportion_train_0.66.test_subjects.txt",mapping_files)
testing_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.test_subjects.txt",mapping_files)

mapping_metadata_two_third_train = data.frame(training_metadata_files_two_thirds_train,testing_metadata_files_two_third_train,mapping_files,training_data_files_two_thirds_train,testing_data_files_two_third_train)

mapping_metadata_fifty_fity_train = data.frame(training_metadata_files_fifty_train,testing_metadata_files_fifty_train,mapping_files,training_data_files_fifty_train,testing_data_files_fifty_train)

write_output_files = function(mapping_metadata) {
  df_lists = apply(mapping_metadata, 1, function(myrow) {
    train_metadata_file = myrow[1]
    test_metadata_file = myrow[2]
    mapping_file = myrow[3]
    training_file = myrow[4]
    testing_file = myrow[5]
    train_metadata_file_rep = rep(train_metadata_file,length(abundance_files))
    test_metadata_file_rep = rep(test_metadata_file,length(abundance_files))
    mapping_files_rep = rep(mapping_file,length(abundance_files))
    training_files_rep = rep(training_file,length(abundance_files))
    testing_files_rep = rep(testing_file,length(abundance_files))
    my_df = data.frame(abundance_files,train_metadata_file_rep,test_metadata_file_rep,mapping_files_rep,training_files_rep,testing_files_rep)
    # split into 2 pieces
    split_point = floor(nrow(my_df)/2)
    my_df1 = my_df[1:split_point,]
    my_df2 = my_df[(split_point +1):nrow(my_df),]
    # this extra column is just so I don't mess with the metadata since I am doing things in parallel
    my_df1$metadata_suffix = 1
    my_df2$metadata_suffix = 2
    my_label1 = gsub(".train_subjects.txt","_input_file_1.tsv",training_file)
    my_label2 = gsub(".train_subjects.txt","_input_file_2.tsv",training_file)
    write.table(my_df1,file=paste("prep_voe_input_files/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
    write.table(my_df2,file=paste("prep_voe_input_files/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
    return(my_df)
  })
  return(df_lists)
}
output_files_two_thirds_train = write_output_files(mapping_metadata=mapping_metadata_two_third_train)
output_files_fifty_train = write_output_files(mapping_metadata=mapping_metadata_fifty_fity_train)

```

#Create input files for when I compare ctrl to case right before onset of T1D

```{r}
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4")

abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern=".csv",full.names = TRUE)
abundance_files = abundance_files[-grep("healthy",abundance_files)]
abundance_files = abundance_files[-grep("total_genes_left_each_timepoint.csv",abundance_files)]

mapping_files = list.files(pattern = ".mapping.csv")
mapping_files = mapping_files[grep("before_condition_case_vs_control",mapping_files)]
#train_metadata_files = gsub(".mapping.csv","_train_metadata.csv",mapping_files)
#test_metadata_files = gsub(".mapping.csv","_test_metadata.csv",mapping_files)

training_metadata_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66_train_metadata.csv",mapping_files)
training_metadata_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5_train_metadata.csv",mapping_files)
testing_metadata_files_two_third_train = gsub(".mapping.csv","_proportion_train_0.66_test_metadata.csv",mapping_files)
testing_metadata_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5_test_metadata.csv",mapping_files)


training_data_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66.train_subjects.txt",mapping_files)
training_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.train_subjects.txt",mapping_files)
testing_data_files_two_third_train = gsub(".mapping.csv","_proportion_train_0.66.test_subjects.txt",mapping_files)
testing_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.test_subjects.txt",mapping_files)

mapping_metadata_two_third_train = data.frame(training_metadata_files_two_thirds_train,testing_metadata_files_two_third_train,mapping_files,training_data_files_two_thirds_train,testing_data_files_two_third_train)

mapping_metadata_fifty_fity_train = data.frame(training_metadata_files_fifty_train,testing_metadata_files_fifty_train,mapping_files,training_data_files_fifty_train,testing_data_files_fifty_train)

write_output_files = function(mapping_metadata) {
  df_lists = apply(mapping_metadata, 1, function(myrow) {
    train_metadata_file = myrow[1]
    test_metadata_file = myrow[2]
    mapping_file = myrow[3]
    training_file = myrow[4]
    testing_file = myrow[5]
    train_metadata_file_rep = rep(train_metadata_file,length(abundance_files))
    test_metadata_file_rep = rep(test_metadata_file,length(abundance_files))
    mapping_files_rep = rep(mapping_file,length(abundance_files))
    training_files_rep = rep(training_file,length(abundance_files))
    testing_files_rep = rep(testing_file,length(abundance_files))
    my_df = data.frame(abundance_files,train_metadata_file_rep,test_metadata_file_rep,mapping_files_rep,training_files_rep,testing_files_rep)
    # split into 2 pieces
    split_point = floor(nrow(my_df)/2)
    my_df1 = my_df[1:split_point,]
    my_df2 = my_df[(split_point +1):nrow(my_df),]
    # this extra column is just so I don't mess with the metadata since I am doing things in parallel
    my_df1$metadata_suffix = 1
    my_df2$metadata_suffix = 2
    my_label1 = gsub(".train_subjects.txt","_input_file_1.tsv",training_file)
    my_label2 = gsub(".train_subjects.txt","_input_file_2.tsv",training_file)
    write.table(my_df1,file=paste("prep_voe_input_files_before_case_vs_ctrl/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
    write.table(my_df2,file=paste("prep_voe_input_files_before_case_vs_ctrl/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
    return(my_df)
  })
  return(df_lists)
}
output_files_two_thirds_train = write_output_files(mapping_metadata=mapping_metadata_two_third_train)
output_files_fifty_train = write_output_files(mapping_metadata=mapping_metadata_fifty_fity_train)

```


#Now run to average samples together that are apart of the same subjects

```{bash}
#cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data
#for x in prep_voe_input_files/*tsv
#do
#  sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk.bash ${x}
#done

#sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk.bash prep_voe_input_files/healthy_pre-t1d-all_HLA_input_file_1.tsv
```

#Run for antibody associations

```{bash}
#cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data
#for x in prep_voe_input_files_antibody_associations/*tsv
#do
#  sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk.bash ${x}
#done
```

#average samples together that are apart of the same subjects for training data

```{bash}
#cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2

#for x in prep_voe_input_files/*tsv
#do
#  sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data.bash ${x}
#done

#cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3

#for x in prep_voe_input_files/*tsv
#do
#  sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data.bash ${x}
#done # 3403522 still running. check this guy later

# redo timed out ones
#sacct -S 2022-02-18 | grep "prep_abun" | grep -v "COMPLETED" | grep "TIMEOUT" | awk '{print $1}' | while read line; do head -n 4 slurm-$line.out | tail -n 1 ; done |  while read line; do grep -w -l $line prep_voe_input_files/*.tsv; done > voe_input_files_to_redo.txt

#while read line
#do
#  sbatch -c 1 -t 1-00:00 -p medium --mem=50G prep_abundance_for_voe_bulk_training_data_2.bash ${line}
#done < voe_input_files_to_redo.txt


# we are going to do another version

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4

for x in prep_voe_input_files/*tsv
do
  sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data_v2.bash ${x}
done 

# do version where we compare case vs control but cases are only x months prior to onset

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4

for x in prep_voe_input_files_before_case_vs_ctrl/*tsv
do
  sbatch -c 1 -t 0-11:59 -p short --mem=10G prep_abundance_for_voe_bulk_training_data_v2.bash ${x}
done  # this should have been like 5 GBs. I'm gonna get in trouble!!


```

#see which still have to run

```{r}
input_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/prep_voe_input_files",pattern="tsv",full.names = TRUE)

# get all output folders that should exist

output_folders = gsub("_input_file_2.tsv$","",basename(input_files))
output_folders = gsub("_input_file_1.tsv$","",output_folders)
# output files
output_files_mc = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4","/",output_folders,"/",basename(output_folders),"_test_mc.rds",sep="")
output_files_ga = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4","/",output_folders,"/",basename(output_folders),"_test_ga.rds",sep="")


mydf = data.frame(input_files,output_folders,output_files_mc,output_files_ga)

mydf_1= mydf[grep("_input_file_1.tsv",mydf[,1]),]
mydf_2= mydf[grep("_input_file_2.tsv",mydf[,1]),]

# get files we have

output_files_mc = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="_test_mc.rds",recursive = TRUE,full.names = TRUE)

output_files_ga = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="_test_ga.rds",recursive = TRUE,full.names = TRUE)

hello = mydf_1[!mydf_1$output_files_mc%in%output_files_mc,]

goodbye = mydf_2[!mydf_2$output_files_ga%in%output_files_ga,]


to_do = c(hello$input_files,goodbye$input_files)

write.table(to_do,file = "prep_voe_input_files/files_to_do.txt",quote=FALSE,col.names=FALSE,row.names=FALSE)
```

```{bash}
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4

while read line
do
sbatch -c 1 -t 0-06:00 -p short --mem=30G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < prep_voe_input_files/files_to_do.txt
```

```{r}
input_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/prep_voe_input_files",pattern="tsv",full.names = TRUE)

# get all output folders that should exist

output_folders = gsub("_input_file_2.tsv$","",basename(input_files))
output_folders = gsub("_input_file_1.tsv$","",output_folders)
# output files
output_files_mc = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4","/",output_folders,"/",basename(output_folders),"_test_mc.rds",sep="")
output_files_ga = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4","/",output_folders,"/",basename(output_folders),"_test_ga.rds",sep="")


mydf = data.frame(input_files,output_folders,output_files_mc,output_files_ga)

mydf_1= mydf[grep("_input_file_1.tsv",mydf[,1]),]
mydf_2= mydf[grep("_input_file_2.tsv",mydf[,1]),]

# get files we have

output_files_mc = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="_test_mc.rds",recursive = TRUE,full.names = TRUE)

output_files_ga = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="_test_ga.rds",recursive = TRUE,full.names = TRUE)

hello = mydf_1[!mydf_1$output_files_mc%in%output_files_mc,]

goodbye = mydf_2[!mydf_2$output_files_ga%in%output_files_ga,]


to_do = c(hello$input_files,goodbye$input_files)

write.table(to_do,file = "prep_voe_input_files/files_to_do_v2.txt",quote=FALSE,col.names=FALSE,row.names=FALSE)
```

```{bash}
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < prep_voe_input_files/files_to_do_v2.txt

```

#We have to redo the 0.66 ones

```{bash}
ls /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/prep_voe_input_files/*_proportion_train_0.66* > prop_0.66_prep_voe_input_files.txt

split -l 557 prop_0.66_prep_voe_input_files.txt prop_0.66_prep_voe_input_files_

# done on sez10
while read line
do
sbatch -c 1 -t 0-06:00 -p short --mem=30G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < prop_0.66_prep_voe_input_files_aa
 
# done on ldp9
while read line
do
sbatch -c 1 -t 0-06:00 -p short --mem=30G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < prop_0.66_prep_voe_input_files_ab

```

#Redo timed out ones

```{r}
input_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/prep_voe_input_files",pattern="tsv",full.names = TRUE)

# get all output folders that should exist

output_folders = gsub("_input_file_2.tsv$","",basename(input_files))
output_folders = gsub("_input_file_1.tsv$","",output_folders)
# output files
output_files_mc = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4","/",output_folders,"/",basename(output_folders),"_test_mc.rds",sep="")
output_files_ga = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4","/",output_folders,"/",basename(output_folders),"_test_ga.rds",sep="")


mydf = data.frame(input_files,output_folders,output_files_mc,output_files_ga)

mydf_1= mydf[grep("_input_file_1.tsv",mydf[,1]),]
mydf_2= mydf[grep("_input_file_2.tsv",mydf[,1]),]

# get files we have

output_files_mc = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="_test_mc.rds",recursive = TRUE,full.names = TRUE)

output_files_ga = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="_test_ga.rds",recursive = TRUE,full.names = TRUE)

hello = mydf_1[!mydf_1$output_files_mc%in%output_files_mc,]

goodbye = mydf_2[!mydf_2$output_files_ga%in%output_files_ga,]


to_do = c(hello$input_files,goodbye$input_files)

write.table(to_do,file = "prep_voe_input_files/files_to_do_v3.txt",quote=FALSE,col.names=FALSE,row.names=FALSE)

```

```{bash}

split -l 97 prep_voe_input_files/files_to_do_v3.txt prep_voe_input_files/files_to_do_v3_

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < prep_voe_input_files/files_to_do_v3_aa

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < prep_voe_input_files/files_to_do_v3_ab

# I also want to remove folders that do not have a _proportion_train_0.5_train_metadata.csv or _proportion_train_0.66_train_metadata.csv. I will make one huge bash script single line to do this!

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/*/ | grep -v "prep_voe_input_files" | rev | cut -c2- | rev | while read line; do echo ${line}_train_metadata.csv; done | while read line; do if [ ! -f "$line" ]; then echo $line; fi; done | while read line; do echo ${line%_train_metadata.csv}; done | while read line; do rm -r $line; done```



#make a file that tells you where each gene is located

```{bash}
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data
ls *.csv | grep -v "healthy" | while read line; do awk -F ',' '{print $2,FILENAME}' ${line} | grep "genename"; done > gene_locs.txt
gzip gene_locs.txt
```

#Next filter to only include genes prevalent in 90% of subjects and do inverse normal transformation

```{bash}

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

#ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/*/ | grep -v "prep_voe_input_files" | rev | cut -c2- | rev > input_folder_list_parsed_3.txt

#while read line
#do
#  sbatch -n 1 -c 5 --mem=50G -p short -t 0-04:00 scripts/filter_normalize_abundance_metadata_data_v2.bash ${line} 0.9 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv 5 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3
#done < input_folder_list_parsed_3.txt

# did a bug that only affects non landmark analysis so redo those

#grep -v "month" input_folder_list_parsed_3.txt > input_folder_list_parsed_3_notlandmark.txt

#while read line
#do
#  sbatch -n 1 -c 5 --mem=50G -p short -t 0-04:00 scripts/filter_normalize_abundance_metadata_data_v2.bash ${line} 0.9 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv 5 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3
#done < input_folder_list_parsed_3_notlandmark.txt

##get jobs that ran out of memory

#sacct -S 2022-02-23 | grep "OUT_OF_ME" | grep "filter_no" | awk '{print $1}' | while read line; do grep "filtered_transformed_abundance_train.csv" slurm-${line}.out; done | while read line; do dirname $line; done > input_folder_list_parsed_3_outOfMem.txt

#while read line
#do
#  sbatch -n 1 -c 5 --mem=100G -p short -t 0-04:00 scripts/filter_normalize_abundance_metadata_data_v2.bash ${line} 0.9 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv 5 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3
#done < input_folder_list_parsed_3_outOfMem.txt





cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/*/ | grep -v "prep_voe_input_files" | rev | cut -c2- | rev > input_folder_list_parsed_4.txt

split -l 533 input_folder_list_parsed_4.txt input_folder_list_parsed_4_

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_aa

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_ab


sacct -S 2023-04-12 | grep "filter_no" | grep "OUT_OF_ME" | grep -v "6686663" | awk '{print $1}' | while read line; do grep -m 1 "_proportion_train_" slurm-${line}.out; done | while read line; do dirname $line; done | tr -d '"' > input_folder_list_parsed_4_outOfMem_a.txt

sacct -S 2023-04-12 | grep "filter_no" | grep "OUT_OF_ME" | grep -v "6684440" | awk '{print $1}' | while read line; do grep -m 1 "_proportion_train_" slurm-${line}.out; done | while read line; do dirname $line; done | tr -d '"' > input_folder_list_parsed_4_outOfMem_b.txt

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_outOfMem_a.txt

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_outOfMem_b.txt

sacct -S 2023-04-12 | grep "filter_no" | grep -A 1000 "6703998" | grep "OUT_OF_ME" |  awk '{print $1}' | while read line; do grep -m 1 "_proportion_train_" slurm-${line}.out; done | while read line; do dirname $line; done | tr -d '"' > input_folder_list_parsed_4_outOfMem2_a.txt


sacct -S 2023-04-12 | grep "filter_no" | grep -A 1000 "6704025" | grep "OUT_OF_ME" |  awk '{print $1}' | while read line; do grep -m 1 "_proportion_train_" slurm-${line}.out; done | while read line; do dirname $line; done | tr -d '"' > input_folder_list_parsed_4_outOfMem2_b.txt

while read line
do
  sbatch -n 1 -c 1 --mem=200G -p short -t 0-04:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_outOfMem2_a.txt

while read line
do
  sbatch -n 1 -c 1 --mem=200G -p short -t 0-04:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_outOfMem2_b.txt

```

```{r}
library(data.table)
#seems to be an issue with some abundance files. lets find out which ones
folder_list = read.table("input_folder_list_parsed_4.txt")
folder_list = folder_list[,1]
abundance_test_list = paste(folder_list,"/",basename(folder_list),"filtered_transformed_abundance_test.csv",sep="")
abundance_train_list = paste(folder_list,"/",basename(folder_list),"filtered_transformed_abundance_test.csv",sep="")

abundance_list = data.frame(abundance_test_list,abundance_train_list)

bool_list = rep("FALSE",nrow(abundance_list))
for(counter in 1:nrow(abundance_list)) {
  x = unlist(abundance_list[counter,])
  test_data = x[1]
  train_data = x[2]
  test_df = fread(cmd = paste("head -n 1",test_data, "| cut -d ',' -f1"),nrows=2)
  train_df = fread(cmd = paste("head -n 1",train_data, "| cut -d ',' -f1"),nrows=2)
  if(colnames(test_df) == "test_data_filtered_transformed") {
    bool_list[counter] = TRUE
  }
  if(colnames(train_df) == "train_data_filtered_transformed") {
    bool_list[counter] = TRUE
  }
}

to_redo_processing = folder_list[which(bool_list=="TRUE")]

write.table(to_redo_processing,file="to_redo_processing.txt",quote=FALSE,col.names=FALSE,row.names=FALSE)

```

```{bash}
while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < to_redo_processing.txt

```

# Filter and normalize for case right before onset vs control

```{bash}

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/*before_condition_case_vs_control*/ | rev | cut -c2- | rev > input_folder_list_parsed_4_before_condition_case_vs_ctrl.txt

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_before_condition_case_vs_ctrl.txt


```





#create input files for running models

```{r}
all_dirs = list.dirs("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3",recursive=FALSE)
all_dirs = all_dirs[!grepl("prep_voe_input_files",all_dirs)]

transformed_abundance_file_list = lapply(all_dirs, function(x) list.files(x,pattern="transform",full.names = TRUE))
names(transformed_abundance_file_list) = all_dirs

transformed_abundance_file_list = transformed_abundance_file_list[sapply(transformed_abundance_file_list, length)==2]

test_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_test.csv",x)])
train_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_train.csv",x)])

input_info = data.frame(test_abundance_data,train_abundance_data)

# now get metadata

metadata_test = sapply(rownames(input_info), function(x) list.files(x,pattern="_test_1_metadata_filtered_baseline_",full.names = TRUE))
metadata_train = sapply(rownames(input_info), function(x) list.files(x,pattern="_train_1_metadata_filtered_baseline_",full.names = TRUE))

train_fold1 = paste(rownames(input_info),".train_subjects_1.txt",sep="")
train_fold2 = paste(rownames(input_info),".train_subjects_2.txt",sep="")
train_fold3 = paste(rownames(input_info),".train_subjects_3.txt",sep="")
test_subjects = paste(rownames(input_info),".test_subjects.txt",sep="")


input_info$metadata_train = metadata_train
input_info$metadata_test = metadata_test
input_info$train_fold1 = train_fold1
input_info$train_fold2 = train_fold2
input_info$train_fold3 = train_fold3
input_info$test_subjects = test_subjects

write.csv(input_info,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v3.csv",row.names = FALSE,quote=FALSE)
#write.table(input_info,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v3.tsv",row.names = FALSE,col.names=FALSE,sep="\t",quote=FALSE)
```


#create input files for running models parse_data_4

```{r}
all_dirs = list.dirs("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",recursive=FALSE)
all_dirs = all_dirs[!grepl("prep_voe_input_files",all_dirs)]

transformed_abundance_file_list = lapply(all_dirs, function(x) list.files(x,pattern="transform",full.names = TRUE))
names(transformed_abundance_file_list) = all_dirs

transformed_abundance_file_list = transformed_abundance_file_list[sapply(transformed_abundance_file_list, length)==2]

test_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_test.csv",x)])
train_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_train.csv",x)])

input_info = data.frame(test_abundance_data,train_abundance_data)

# now get metadata

metadata_train = paste(rownames(input_info),"_train_metadata.csv",sep="")
metadata_test = paste(rownames(input_info),"_test_metadata.csv",sep="")

train_fold1 = paste(rownames(input_info),".train_subjects_1.txt",sep="")
train_fold2 = paste(rownames(input_info),".train_subjects_2.txt",sep="")
train_fold3 = paste(rownames(input_info),".train_subjects_3.txt",sep="")
test_subjects = paste(rownames(input_info),".test_subjects.txt",sep="")


input_info$metadata_train = metadata_train
input_info$metadata_test = metadata_test
input_info$train_fold1 = train_fold1
input_info$train_fold2 = train_fold2
input_info$train_fold3 = train_fold3
input_info$test_subjects = test_subjects

write.csv(input_info,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4.csv",row.names = FALSE,quote=FALSE)
#write.table(input_info,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v3.tsv",row.names = FALSE,col.names=FALSE,sep="\t",quote=FALSE)
```

#Create inputs for running the models where cases are right before onset and controls are aged matched

```{r}
all_dirs = list.dirs("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",recursive=FALSE)
all_dirs = all_dirs[!grepl("prep_voe_input_files",all_dirs)]
all_dirs = all_dirs[grep("before_condition_case_vs_control",all_dirs)]

transformed_abundance_file_list = lapply(all_dirs, function(x) list.files(x,pattern="transform",full.names = TRUE))
names(transformed_abundance_file_list) = all_dirs

transformed_abundance_file_list = transformed_abundance_file_list[sapply(transformed_abundance_file_list, length)==2]

test_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_test.csv",x)])
train_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_train.csv",x)])

input_info = data.frame(test_abundance_data,train_abundance_data)

# now get metadata

metadata_train = paste(rownames(input_info),"_train_metadata.csv",sep="")
metadata_test = paste(rownames(input_info),"_test_metadata.csv",sep="")

train_fold1 = paste(rownames(input_info),".train_subjects_1.txt",sep="")
train_fold2 = paste(rownames(input_info),".train_subjects_2.txt",sep="")
train_fold3 = paste(rownames(input_info),".train_subjects_3.txt",sep="")
test_subjects = paste(rownames(input_info),".test_subjects.txt",sep="")


input_info$metadata_train = metadata_train
input_info$metadata_test = metadata_test
input_info$train_fold1 = train_fold1
input_info$train_fold2 = train_fold2
input_info$train_fold3 = train_fold3
input_info$test_subjects = test_subjects

write.table(input_info,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_condition_case_vs_control.csv",row.names = FALSE,quote=FALSE,col.names=FALSE,sep=",")

```



#Run random forest on models

```{bash}

# remove header
tail -n +2 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4.csv > /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header.csv
# remove before_condition cause that is only for binary classification

grep -v "before_condition" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header.csv > /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv
#while read line
#do
#  sbatch /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_RF.bash ${line} microbiome,number_autoantibodies,fdr,grs2 all
#done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v3_noheader.csv

# run lasso

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/test.csv

split -l 358 models_input_files_parsed_v4_no_header_no_before_condition.csv models_input_files_parsed_v4_no_header_no_before_condition_

while read line
do
  sbatch -n 1 -c 1 --mem=70G -p short -t 0-03:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition_aa

while read line
do
  sbatch -n 1 -c 1 --mem=70G -p short -t 0-03:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition_ab

# now redo out of memory or timeout 

sacct -S 2023-04-13 | grep "run_lasso" | grep -A 1000 "6742575" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4b.txt

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-04:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_v4b.txt


sacct -S 2023-04-13 | grep "run_lasso" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4a.txt

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-04:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_v4a.txt

# more jobs still failed. see what I need to redo

sacct -S 2023-04-14 | grep "run_lasso" | grep -A 1000 "6792650" | grep -v "COMPLETED" |grep -v "FAILED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4aa.txt

sacct -S 2023-04-14 | grep "run_lasso" | grep -A 1000 "6792379" | grep -v "COMPLETED" |grep -v "FAILED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4ab.txt

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-08:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_v4aa.txt

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-08:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_v4ab.txt


sacct | grep "run_lasso" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4aaa.txt


sacct | grep "run_lasso" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4aab.txt


while read line
do
  sbatch -n 1 -c 1 --mem=200G -p medium -t 1-00:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_v4aaa.txt

while read line
do
  sbatch -n 1 -c 1 --mem=100G -p medium -t 1-00:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_v4aab.txt

#locs account
sacct -S 2023-04-15 | grep -A 1000 6824859 | grep "run_lasso" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4aaab.txt

while read line
do
  sbatch -n 1 -c 1 --mem=150G -p medium -t 1-12:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_v4aaab.txt

# my account
sacct -S 2023-04-15 | grep -A 1000 6824848 | grep "run_lasso" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv; done > out_of_mem_jobs_v4aaab.txt

# make sure I got everything 
Rscript scripts/find_failed_job_folders.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv output_lasso_time_to_event_C_loss_microbiome_selection_method_all_feature_list_microbiome.rds failed_lasso_survival.txt

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-03:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/failed_lasso_survival.txt



# run ttest

head -n 1 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header.csv > test.txt

split -l 533 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header.csv models_input_files_parsed_v4_no_header_


while read line
do
  sbatch -n 1 -c 1 --mem=30G -p short -t 0-01:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_ttests_2.bash ${line}
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_aa

while read line
do
  sbatch -n 1 -c 1 --mem=30G -p short -t 0-01:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_ttests_2.bash ${line}
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_ab

# get jobs that ran out of memory on locs account
sacct -S 2023-05-09 | grep "run_ttest" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_train.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header.csv; done > out_of_mem_jobs_ttest_a.txt

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-01:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_ttests_2.bash ${line}
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_ttest_a.txt

# get jobs that ran out of memory on my account
sacct -S 2023-05-09 | grep "run_ttest" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_train.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header.csv; done > out_of_mem_jobs_ttest_b.txt

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-01:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_ttests_2.bash ${line}
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs_ttest_b.txt


```

#Lets just see how many files actually have sig hits

```{r}
all_dirs = list.dirs("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",recursive=FALSE)
all_dirs = all_dirs[!grepl("prep_voe_input_files",all_dirs)]

ttest_files = lapply(all_dirs, function(x) list.files(x,pattern="_ttest_results.rds",full.names = TRUE))
ttest_files = unlist(ttest_files)

results = lapply(ttest_files, function(x) readRDS(x))

names(results) = ttest_files


BH_results = lapply(results, function(x) x[[1]])
BY_results = lapply(results, function(x) x[[2]])
names(BH_results) = ttest_files
names(BY_results) = ttest_files

BH_results = BH_results[!sapply(BH_results,is.null)]
BH_results = BH_results[sapply(BH_results,function(x) length(x)>0)]

BY_results = BY_results[!sapply(BY_results,is.null)]
BY_results = BY_results[sapply(BY_results,function(x) length(x)>0)]


length(BY_results) # 139
length(BH_results) # 194

mean(sapply(BY_results,length)) # 84151.42
median(sapply(BY_results,length)) # 56640

mean(sapply(BH_results,length)) # 196,441.4
median(sapply(BH_results,length)) # 141,993.5

folders_with_sig_genes_BY = dirname(names(BY_results))
folders_with_sig_genes_BH = dirname(names(BH_results))

#now get input file

all_files_input = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header.csv",header=FALSE)

all_files_input_ttest_sig = all_files_input[sapply(folders_with_sig_genes_BY,function(x) grep(x,all_files_input[,1])),]

write.table(all_files_input_ttest_sig,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_v4.csv",row.names = FALSE,quote=FALSE,col.names=FALSE,sep=",")

# also make output where we only include those that have baseline months to do survival analysis


folders_with_sig_genes_for_survival = folders_with_sig_genes_BY[!grepl("before_condition",folders_with_sig_genes_BY)]

all_files_input_ttest_sig_for_survival = all_files_input[sapply(folders_with_sig_genes_for_survival,function(x) grep(x,all_files_input[,1])),]


write.table(all_files_input_ttest_sig_for_survival,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv",row.names = FALSE,quote=FALSE,col.names=FALSE,sep=",")
```

#Run lasso with sig genes

```{bash}
while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv

sacct -S 2023-05-01 | grep "run_lasso" | grep -A 1000 "7699324" | grep -v "COMPLETED" |grep -v "FAILED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv; done > input_files_ttest_sig_for_survival_v4_moretime.csv

while read line
do
  sbatch -n 1 -c 3 --mem=50G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA ttest_sig 3 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4_moretime.csv

sacct -S 2023-05-03 | grep "run_lasso" | grep "TIMEOUT" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv; done > input_files_ttest_sig_for_survival_v5_moretime.csv

while read line
do
  sbatch -n 1 -c 3 --mem=50G -p medium -t 1-00:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA ttest_sig 3 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v5_moretime.csv

Rscript scripts/find_failed_job_folders.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv output_lasso_time_to_event_C_loss_microbiome_selection_method_ttest_sig_feature_list_microbiome.rds failed_lasso_survival_ttest.txt

while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome NA ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/failed_lasso_survival_ttest.txt


# now run random forest

while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_rf_survival.bash ${line} microbiome NA ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv

sacct -S 2023-05-11 | grep "run_rf_su" | grep -v "COMPLETED" | grep -v "FAILED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv; done > input_files_ttest_sig_for_survival_v4_moretime_rf.csv


while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-11:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_rf_survival.bash ${line} microbiome NA ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4_moretime_rf.csv

```

#Now run models but combine autoantibodies and such with microbiome genes

```{bash}

# we only want to use number of autoantibodies for models that are healthy_pre-T1D
grep "healthy_pre-t1d" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv > health_pre_t1d_input_files.csv

grep -v "healthy_pre-t1d" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_no_header_no_before_condition.csv > no_health_pre_t1d_input_files.csv

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-03:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,number_autoantibodies,fdr,grs2 number_autoantibodies,fdr,grs2 all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/health_pre_t1d_input_files.csv

sacct -S 2023-05-29 | grep "run_lasso" | grep -A 10000 "9857812" | grep -v "COMPLETED" | grep -v "FAILED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/health_pre_t1d_input_files.csv; done > out_of_mem_jobs2.txt


while read line
do
  sbatch -n 1 -c 1 --mem=400G -p highmem -t 1-00:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,number_autoantibodies,fdr,grs2 number_autoantibodies,fdr,grs2 all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/out_of_mem_jobs2.txt # 125GB and 3 hours would have worked


while read line 
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-03:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files.csv

sacct -S 2023-04-29 | grep "run_lasso" | grep -A 10000 "7564824" | grep -v "COMPLETED" | grep -v "FAILED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files.csv; done > no_health_pre_t1d_input_files_timed_out_jobs.txt

while read line 
do
  sbatch -n 1 -c 1 --mem=100G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files_timed_out_jobs.txt

sacct -S 2023-04-30 | grep "run_lasso" | grep -v "highmem" | grep -v "COMPLETED" | grep -v "FAILED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files_timed_out_jobs.txt; done > no_health_pre_t1d_input_files_timed_out_jobs_v2.txt

while read line 
do
  sbatch -n 1 -c 1 --mem=400G -p highmem -t 1-00:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files_timed_out_jobs_v2.txt

Rscript scripts/find_failed_job_folders.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files.csv output_lasso_time_to_event_C_loss_microbiome_selection_method_all_feature_list_microbiome,fdr,grs2.rds failed_lasso_survival_no_T1D_only_microbiome_clinical.txt

while read line 
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-03:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 all 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/failed_lasso_survival_no_T1D_only_microbiome_clinical.txt



# now do again but with ttest sig

grep "healthy_pre-t1d" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv > input_files_ttest_sig_for_survival_healthy_T1D.csv

grep -v "healthy_pre-t1d" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_v4.csv > input_files_ttest_sig_for_survival_not_healthy_T1D.csv

while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,number_autoantibodies,fdr,grs2 number_autoantibodies,fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_healthy_T1D.csv

Rscript scripts/find_failed_job_folders.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_healthy_T1D.csv output_lasso_time_to_event_C_loss_microbiome_selection_method_ttest_sig_feature_list_microbiome,number_autoantibodies,fdr,grs2.rds failed_lasso_survival_T1D_only_ttest_microbiome_clinical.txt

while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,number_autoantibodies,fdr,grs2 number_autoantibodies,fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/failed_lasso_survival_T1D_only_ttest_microbiome_clinical.txt


while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D.csv

sacct -S 2023-05-04 | grep "run_lasso" | grep -A 1000 "7907395" | grep -v "FAILED" | grep -v "COMPLETED"  | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D.csv; done > input_files_ttest_sig_for_survival_not_healthy_T1D_v2.csv

while read line
do
  sbatch -n 1 -c 1 --mem=250G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D_v2.csv

sacct -S 2023-05-05 | grep "run_lasso" | grep -v "FAILED" | grep -v "COMPLETED"  | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D_v2.csv; done > input_files_ttest_sig_for_survival_not_healthy_T1D_v3.csv

while read line
do
  sbatch -n 1 -c 1 --mem=50G -p medium -t 0-18:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D_v3.csv

Rscript scripts/find_failed_job_folders.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D.csv output_lasso_time_to_event_C_loss_microbiome_selection_method_ttest_sig_feature_list_microbiome,fdr,grs2.rds failed_lasso_survival_not_T1D_ttest_microbiome_clinical.txt

while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_lasso.bash ${line} microbiome,fdr,grs2 fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/failed_lasso_survival_not_T1D_ttest_microbiome_clinical.txt



while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_rf_survival.bash ${line} microbiome,number_autoantibodies,fdr,grs2 number_autoantibodies,fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_healthy_T1D.csv


while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_rf_survival.bash ${line} microbiome,fdr,grs2 fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D.csv

sacct | grep "run_rf_su"  | grep -v "COMPLETED" | grep "short" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D.csv; done > /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D_2.csv

while read line
do
  sbatch -n 1 -c 1 --mem=50G -p short -t 0-11:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_rf_survival.bash ${line} microbiome,fdr,grs2 fdr,grs2 ttest_sig 1 C
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/input_files_ttest_sig_for_survival_not_healthy_T1D_2.csv

```

#Now lets do a normal cox regression without microbiome data.

```{bash}
#while read line
#do
#  sbatch -n 1 -c 1 --mem=10G -p short -t 0-02:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_cox_regression.bash ${line} number_autoantibodies,fdr,grs2 1 C
#done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v3_no_header_no_before_condition.csv

split -l 10 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/health_pre_t1d_input_files.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/health_pre_t1d_input_files_clinical_only

split -l 10 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files_clinical_only

for x in health_pre_t1d_input_files_clinical_only*
do
sbatch -n 1 -c 1 --mem=5G -p short -t 0-2:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_cox_regression_batch.bash ${x} number_autoantibodies,fdr,grs2
done

sbatch -n 1 -c 1 --mem=5G -p short -t 0-2:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_cox_regression_batch.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/no_health_pre_t1d_input_files.csv fdr,grs2


#for x in no_health_pre_t1d_input_files_clinical_only*
#do
#sbatch -n 1 -c 1 --mem=5G -p short -t 0-2:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_cox_regression_batch.bash ${x} fdr,grs2
#done
```



#extract output into nice table format

```{r}
# NOTE: RUN THIS AFTER ALL SPECIFICATIONS ARE DONE INCLUDING SPECIES LEVEL and BINARY CLASSIFICATION

output_lasso_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern = "lasso",full.names = TRUE,recursive=TRUE)
random_forest_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="output_random_forest_",full.names = TRUE,recursive = TRUE)
cox_regression_files= list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="output_lasso_",full.names = TRUE,recursive = TRUE)


#output_lasso_files_species_pathways = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/humann_analysis",pattern = "lasso",full.names = TRUE,recursive=TRUE)
#random_forest_files_species_pathways = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/humann_analysis",pattern="output_random_forest_time_to_event_",full.names = TRUE,recursive = TRUE)

all_output_files_with_microbiome_genelevel = c(output_lasso_files,random_forest_files)
all_output_files_with_microbiome_genelevel = all_output_files_with_microbiome_genelevel[grepl(".rds",all_output_files_with_microbiome_genelevel)]

library(data.table)
prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=TRUE)
setkey(prokka_annotations,V1)

library(locStra)

prokka_annotations_each_model = lapply(all_output_files_with_microbiome_genelevel, function(myfile) {
  print(myfile)
  rds_output = readRDS(myfile)
  if(length(rds_output) == 1) {
    if(is.na(rds_output)) {
      return(NA)
    }
  }
  if(grepl("time_to_event",myfile)) {
    coefs = rds_output[[5]]
  } else {
    coefs = rds_output[[3]]
  }
  if(as.character(class(coefs)[1]) == "dgCMatrix" | class(coefs)[1] == "matrix") {
    coefs = as.matrix(coefs)
    coefs = coefs[,1,drop=FALSE]
    colnames(coefs) = "coefs"
  } else if(class(coefs)[1] == "numeric") {
    coefs = as.data.frame(coefs)
  } else if(is.na(coefs)) {
    return(NA)
  }
  coefs = coefs[abs(coefs[,1])>0,,drop=FALSE]
  fdr_index = match("fdr",rownames(coefs))
  fdr_index = fdr_index[!is.na(fdr_index)]
  grs2_index = match("grs2",rownames(coefs))
  grs2_index= grs2_index[!is.na(grs2_index)]
  number_auto_indexes = grep("number_autoantibodies",rownames(coefs))
  clinical_indexes = c(fdr_index,grs2_index,number_auto_indexes)
  
  if(length(clinical_indexes) < nrow(coefs)) {
    prokka_annotations_temp = prokka_annotations[rownames(coefs)]
    prokka_annotations_temp = as.data.frame(prokka_annotations_temp)
    prokka_annotations_temp = cbind(prokka_annotations_temp,coefs=coefs)
    rownames(prokka_annotations_temp) = rownames(coefs)
    prokka_annotations_temp = prokka_annotations_temp[order(abs(prokka_annotations_temp$coefs)),]
  } else {
    return(NA)
  }
})
names(prokka_annotations_each_model) = basename(all_output_files_with_microbiome_genelevel)

notNull = sapply(prokka_annotations_each_model, function(x) {
  if(length(x) == 1 & sum(is.na(x))==1) {
    return(FALSE)
  } else {
    return(TRUE)
  }
})

prokka_annotations_each_model = prokka_annotations_each_model[notNull]

fumarate_hydratase_indexes = which(sapply(prokka_annotations_each_model, function(x) sum(grepl("Fumarate hydratase class II",x$V7))) >0)
prokka_annotations_each_model_fumarate_hydratase = prokka_annotations_each_model[fumarate_hydratase_indexes]

phophogluconate_dehydrogenase_indexes = which(sapply(prokka_annotations_each_model, function(x) sum(grepl("6-phosphogluconate dehydrogenase",x$V7))) >0)

vitmainB_indexes = which(sapply(prokka_annotations_each_model, function(x) sum(grepl("Vitamin B12 import ATP-binding protein BtuD",x$V7))) >0)

Vitamin12_coefs = do.call("rbind",lapply(prokka_annotations_each_model, function(x) {
  x[grep("Vitamin B12 import ATP-binding protein BtuD",x),]
}))


top_protein_function_predictive = sort(table(unlist(lapply(prokka_annotations_each_model,function(x) x$V7))),decreasing = TRUE)
top_protein_function_predictive = as.data.frame(top_protein_function_predictive)


top_protein_function_predictive_permodel = sort(table(unlist(lapply(prokka_annotations_each_model,function(x) unique(x$V7)))),decreasing = TRUE)
top_protein_function_predictive_permodel = as.data.frame(top_protein_function_predictive_permodel)
top_protein_function_predictive_permodel = top_protein_function_predictive_permodel[-match("hypothetical protein",top_protein_function_predictive_permodel$Var1),]
top_protein_function_predictive_permodel$percent = top_protein_function_predictive_permodel$Freq/length(prokka_annotations_each_model)
top_protein_function_predictive_permodel$percent = top_protein_function_predictive_permodel$percent*100

library(ggplot2)
pdf("top_protein_funcs_predictive_perc.pdf")
ggplot(top_protein_function_predictive_permodel[1:50,],aes(x=Var1,y=percent)) + geom_col() + coord_flip()
dev.off()

pdf("all_protein_funcs_predictive_perc_hist.pdf")
hist(top_protein_function_predictive_permodel$percent)
dev.off()

num_genes_each_model = sapply(prokka_annotations_each_model,nrow)

pdf("num_genes_per_model_hist.pdf")
hist(num_genes_each_model) # 23.86971 mean
dev.off()

# get similarity between models of genes chosen


all_genes = unique(unlist(lapply(prokka_annotations_each_model,function(x) unique(x$V7))))

gene_presence_mat = lapply(prokka_annotations_each_model,function(x) as.numeric(all_genes%in%x$V7))
gene_presence_mat = do.call("rbind",gene_presence_mat)
colnames(gene_presence_mat) = all_genes

jaccard_dist = jaccardMatrix(gene_presence_mat, useCpp = TRUE, sparse = FALSE)

jaccard_dist_upper = upper.tri(jaccard_dist, diag = FALSE)

jaccard_dist_upper = jaccard_dist[jaccard_dist_upper]

mean(jaccard_dist_upper)
pdf("jaccard_similarity_hist.pdf")
hist(jaccard_dist_upper)


top_protein_function_predictive[top_protein_function_predictive>2]

grep("Fumarate hydratase",names(top_protein_function_predictive))
grep("6-phosphogluconate dehydrogenase",names(top_protein_function_predictive))

all_output_files = c(output_lasso_files,random_forest_files,output_lasso_files_species_pathways,random_forest_files_species_pathways,cox_regression_files)
all_output_files = all_output_files[grepl(".rds",all_output_files)]

confint.ipcwsurvivalROC <- function(object,parm=NULL,level=0.95,n.sim=2000,...){
  if(object$iid==FALSE){  
    stop(paste("Confidence intervals cannot be computed because you have chosen iid=FALSE (default) in the input of timeROC(). \n",sep=""))
  }
 ## browser()
  # {{{ remove NA if there is NA
  AUC <-  object$AUC[!is.na(object$AUC)]
  se <- object$inference$vect_sd_1[!is.na(object$AUC)]
  mat.iid <- object$inference$mat_iid_rep_1[,!is.na(object$AUC),drop=FALSE]
  # }}}
  # {{{ Pointwise confidence intervals
  lower <- AUC*100-se*100*qnorm(1-(1-level)/2)
  upper <- AUC*100+se*100*qnorm(1-(1-level)/2)
  tab_AUC_1<-round(cbind(lower,upper),2)
  colnames(tab_AUC_1)<-c(paste(((1-level)/2)*100,"%",sep=""),paste((1-(1-level)/2)*100,"%",sep=""))
  return(tab_AUC_1)
}
  
get_AUCs = function(myfile) {
  # first get baseline
  if(grepl("18month-24month",basename(myfile))) {
    basline = "18month-24month"
  } else if(grepl("12month-18month",basename(myfile))) {
    basline = "12month-18month"
  } else if(grepl("6month-12month",basename(myfile))) {
    basline = "6month-12month"
  } else if(grepl("24month",basename(myfile))) {
    basline = "24month"
  } else if(grepl("18month",basename(myfile))) {
    basline = "18month"
  } else if(grepl("12month",basename(myfile))) {
    basline = "12month"
  } else if(grepl("6month",basename(myfile))) {
    basline = "6month"
  } else if(grepl("3month",basename(myfile))) {
    basline = "3month"
  } else if(grepl("1month-2month",basename(myfile))) {
    basline = "1month-2month"
  } else if(grepl("2month-3month",basename(myfile))) {
    basline = "2month-3month"
  } else if(grepl("2month",basename(myfile))) {
    basline = "2month"
  } else if(grepl("1month",basename(myfile))) {
    basline = "1month"
  } else {
    basline = "NA"
  }
  # now get condition
  if(grepl("healthy_pre-t1d",basename(myfile))) {
    condition = "healthy_pre-t1d"
  } else if (grepl("healthy_pre-MIAA",basename(myfile))) {
    condition = "healthy_pre-MIAA"
  } else if (grepl("healthy_pre-GAD",basename(myfile))) {
    condition = "healthy_pre-GAD"
  } else if (grepl("healthy_pre-IA2A",basename(myfile))) {
    condition = "healthy_pre-IA2A"
  } else if (grepl("healthy_pre-sero",basename(myfile))) {
    condition = "healthy_pre-sero"
  } else if (grepl("triple_converters_vs_T1D",basename(myfile))) {
    condition = "triple_converters_vs_T1D" 
  } else if (grepl("serconverters_or_T1D",basename(myfile))) {
    condition = "serconverters_or_T1D"
  } else {
    condition = "NA" # there should be no NA for this category
  }
  if(grepl("_before_condition",basename(myfile))) {
    time_before_event = "before_condition"
  } else {
    time_before_event = "age_at_collection"
  }
  # now get HLA
  if(grepl("DR3_DR4_only",basename(myfile))) {
    HLA = "DR3_DR4_only"
  } else if(grepl("DR4_DR4_only",basename(myfile))) {
    HLA = "DR4_DR4_only"
  } else if(grepl("DR4_DR8_only",basename(myfile))) { 
    HLA = "DR4_DR8_only"
  } else if(grepl("DR3_DR3_only",basename(myfile))) { 
    HLA = "DR3_DR3_only"
  } else if(grepl("DR4_DR1_only",basename(myfile))) { 
    HLA = "DR4_DR1_only"
  } else if(grepl("DR4_DR13",basename(myfile))) { 
    HLA = "DR4_DR13_only"
  } else if(grepl("all_HLA",basename(myfile))) { 
    HLA = "all_HLA"
  } else {
    HLA = "NA" # should not be NA
  }
  
  # now get proportion train
  if(grepl("_proportion_train_0.66",basename(myfile))) {
    proportion_train="2-3rds"
  } else if(grepl("_proportion_train_0.5",basename(myfile))) {
    proportion_train="50-50"
  } else {
    proportion_train = "NA" # never should be NA
  }
  # now get model
  
  if(grepl("lasso_time_to_event",basename(myfile))) {
    model = "lasso_cox-regression"
  } else if(grepl("output_random_forest_time_to_event_",basename(myfile))) {
    model = "random_survival_forest"
  } else if(grepl("lasso_logistic_regression",basename(myfile))) {
    model = "lasso_logistic_regression"
  } else {
    model = "cox_regression"
  }
  # get loss function used. 
  if(grepl("C_loss",basename(myfile))) {
    loss_function = "C-index"
  } else {
    loss_function = "NA"
  }
  
  if(grepl("ttest",basename(myfile))) {
    feature_selection_method="ttest"
  } else {
    feature_selection_method="NA"
  }


  rds_output = readRDS(myfile)
  #train_AUC = rds_output[[1]]$AUC
  if(model == "lasso_cox-regression") {
    test_AUC = rds_output[[2]]$AUC
    confidence_intervals = confint.ipcwsurvivalROC(rds_output[[2]])
    confidence_intervals = confidence_intervals[match(names(test_AUC),rownames(confidence_intervals)),]
    confidence_intervals = confidence_intervals/100
    rownames(confidence_intervals) = names(test_AUC)
    lower_CIs = confidence_intervals[,1]
    upper_CIs = confidence_intervals[,2]
    coefs = rds_output[[3]]
    # basically if coefs aren't in dgCMatrix class then switch model from lasso_cox-regression to regular cox regression
    if(class(coefs) != "dgCMatrix" & model == "lasso_cox-regression") {
      model = "cox_regression"
    }
    cases_time_t = rds_output[[2]]$Stats[,1]
    survivors_time_t = rds_output[[2]]$Stats[,2]
  } else if(model == "random_survival_forest") {
    test_data = as.data.frame(rds_output[[2]])
    test_AUC = test_data$AUC
    cases_time_t = test_data$Cases
    survivors_time_t = test_data$Survivors
    lower_CIs = test_data[,5]
    upper_CIs = test_data[,6]
  } else if(model == "cox_regression") {
    test_AUC = rds_output[[2]]$AUC
    confidence_intervals = confint.ipcwsurvivalROC(rds_output[[2]])
    confidence_intervals = confidence_intervals[match(names(test_AUC),rownames(confidence_intervals)),]
    rownames(confidence_intervals) = names(test_AUC)
    confidence_intervals = confidence_intervals/100
    lower_CIs = confidence_intervals[,1]
    upper_CIs = confidence_intervals[,2]
    cases_time_t = rds_output[[2]]$Stats[,1]
    survivors_time_t = rds_output[[2]]$Stats[,2]
  } else if(model == "lasso_logistic_regression") {
    test_AUC = rds_output[[2]]["AUC"]
    lower_CIs = rds_output[[2]]["lower_ci"]
    upper_CIs = rds_output[[2]]["upper_ci"]
    cases_time_t = rds_output[[4]]["test_case"]
    survivors_time_t = rds_output[[4]]["test_ctrl"]
  }
  if(model == "random_survival_forest" | model == "cox_regression" | model == "lasso_cox-regression") {
    cases_time_1 = cases_time_t[1]
    cases_time_3 = cases_time_t[2]
    cases_time_5 = cases_time_t[3]
  
    survivors_time_1 = survivors_time_t[1]
    survivors_time_3 = survivors_time_t[2]
    survivors_time_5 = survivors_time_t[3]
  
    oneyear_AUC = test_AUC[1]
    threeyear_AUC = test_AUC[2]
    fiveyear_AUC = test_AUC[3]
  
    one_year_upper_CI = upper_CIs[1]
    three_year_upper_CI = upper_CIs[2]
    five_year_upper_CI = upper_CIs[3]

    one_year_lower_CI = lower_CIs[1]
    three_year_lower_CI = lower_CIs[2]
    five_year_lower_CI = lower_CIs[3]
  }
  sample_counts = rds_output[[4]]
  total_test_samples = sample_counts["total_test"]
  features_to_examine = rds_output[[5]]
  if(sum(grepl("number_autoantibodies",features_to_examine)>0)) {
    features_to_examine = features_to_examine[-grep("number_autoantibodies",features_to_examine)]
    features_to_examine = c(features_to_examine,"number_autoantibodies")
  }

  feature_vec = paste(features_to_examine,collapse=",")
  
  
  if(grepl("species",basename(myfile))) {
    microbiome_type = "species"
  } else if(grepl("pathway",basename(myfile))) {
    microbiome_type = "pathway"
  } else if(grepl("microbiome",feature_vec)) {
    microbiome_type = "gene"
  } else {
    microbiome_type = "NA"
  }

  info_from_name = c(condition,basline,time_before_event,HLA,proportion_train,model,loss_function,feature_selection_method,microbiome_type)
  
  if(model == "random_survival_forest" | model == "cox_regression" | model == "lasso_cox-regression") {
    info_from_name1year = c(info_from_name,total_test_samples,feature_vec,cases_time_1,survivors_time_1,"one_year_horizon",one_year_lower_CI,oneyear_AUC,one_year_upper_CI)
    info_from_name3year = c(info_from_name,total_test_samples,feature_vec,cases_time_3,survivors_time_3,"three_year_horizon",three_year_lower_CI,threeyear_AUC,three_year_upper_CI)
    info_from_name5year = c(info_from_name,total_test_samples,feature_vec,cases_time_5,survivors_time_5,"five_year_horizon",five_year_lower_CI,fiveyear_AUC,five_year_upper_CI)
  
    mydf = as.data.frame(rbind(info_from_name1year,info_from_name3year,info_from_name5year))
    colnames(mydf) = c("condition","baseline","time_before_event","HLA","proportion_train","model","loss_function","feature_selection_method","microbiome_type","sample_number","features","cases_time_horizon","survivors_time_horizon","horizon_time","lower_CI_AUC","AUC","upper_CI_AUC")
    return(mydf) 
  } else {
    info = c(info_from_name,total_test_samples,feature_vec,cases_time_t,survivors_time_t,"NA",lower_CIs,test_AUC,upper_CIs)
    names(info) = c("condition","baseline","time_before_event","HLA","proportion_train","model","loss_function","feature_selection_method","microbiome_type","sample_number","features","cases_time_horizon","survivors_time_horizon","horizon_time","lower_CI_AUC","AUC","upper_CI_AUC")
    return(info)
  }
}
output_df = do.call("rbind",lapply(all_output_files, function(myfile) {
  print(myfile)
  return(get_AUCs(myfile))
}))


output_df$AUC = as.numeric(output_df$AUC)
output_df$upper_CI_AUC = as.numeric(output_df$upper_CI_AUC)
output_df$lower_CI_AUC = as.numeric(output_df$lower_CI_AUC)

write.csv(output_df,"lasso_rf_regression_output_df_with_AUCs.csv")


# lets see how 

output_df[output_df$condition == "healthy_pre-t1d" & output_df$baseline == "12month" & output_df$HLA == "DR3_DR4_only" & output_df$proportion_train == "50-50" & (output_df$model == "lasso_cox-regression" | output_df$model == "cox-regression") & output_df$microbiome_type == "gene",]


# I also need to compare the microbiome+AA+GRS+FDR to AA+GRS+FDR

output_df$group_cat = paste(output_df$condition,output_df$baseline,output_df$time_before_event,output_df$HLA,output_df$proportion_train,output_df$horizon_time,sep="_")

# we only want to do this for healthy-preT1D cause thats the only one we include AA for

output_df_healthy_pret1d = output_df[output_df$condition == "healthy_pre-t1d",]

output_df_healthy_pret1d = output_df_healthy_pret1d[output_df_healthy_pret1d$features == "microbiome,fdr,grs2,number_autoantibodies" | output_df_healthy_pret1d$features == "fdr,grs2,number_autoantibodies",]

get_diff_AUCs = function(df, abundance_type,feature_selection_method,model_type) {
  if(feature_selection_method == "ttest") {
    ttest_index = df$feature_selection_method == "ttest" & df$microbiome_type != "NA"
    NA_index = df$microbiome_type == "NA"
    df = df[ttest_index | NA_index,]
  } else {
    df = df[df$feature_selection_method != "ttest",]
  }
  
  if(abundance_type == "gene") {
    df_gene_or_no_microbiome = df[-which(df$microbiome_type == "pathway" | df$microbiome_type == "species"),]
  } else if(abundance_type == "pathway") {
    df_gene_or_no_microbiome = df[-which(df$microbiome_type == "gene" | df$microbiome_type == "species"),]
  } else if(abundance_type == "species") {
    df_gene_or_no_microbiome = df[-which(output_df_healthy_pret1d$microbiome_type == "gene" | output_df_healthy_pret1d$microbiome_type == "pathway"),]
  }
  if(model_type == "lasso_cox-regression") {
    df_gene_or_no_microbiome = df_gene_or_no_microbiome[df_gene_or_no_microbiome$model == "lasso_cox-regression" | df_gene_or_no_microbiome$model == "cox_regression",]
  } else if(model_type == "random_survival_forest") {
    df_gene_or_no_microbiome = df_gene_or_no_microbiome[df_gene_or_no_microbiome$model == "random_survival_forest" | df_gene_or_no_microbiome$model == "cox_regression",]
  }
  
  
  output_df_gene_split = split(df_gene_or_no_microbiome,df_gene_or_no_microbiome$group_cat)
  output_df_split_to_compare = output_df_gene_split[sapply(output_df_gene_split,nrow)>1]
  nonNAs = sapply(output_df_split_to_compare, function(x) {
    AUC_temp_vals = unique(x$AUC)
    if(length(AUC_temp_vals) >1) {
      return(TRUE)
    } else if(length(AUC_temp_vals) ==1 & is.na(AUC_temp_vals)) {
      return(FALSE)
    } else {
      return(TRUE)
    }
  })
  output_df_split_to_compare = output_df_split_to_compare[nonNAs]
  gene_vs_clinical_AUC_diff = do.call("rbind",lapply(output_df_split_to_compare, function(x) {
    microbiome_row = x[grep("microbiome,fdr,grs2,number_autoantibodies",x$features),]
    non_microbiome = x[match("fdr,grs2,number_autoantibodies",x$features),]
    AUC_diff = microbiome_row$AUC - non_microbiome$AUC
    vec_to_return = c(microbiome_row$condition,microbiome_row$baseline,microbiome_row$time_before_event,microbiome_row$HLA,microbiome_row$proportion_train,microbiome_row$feature_selection_method,microbiome_row$cases_time_horizon,microbiome_row$survivors_time_horizon,microbiome_row$horizon_time,AUC_diff)
    return(vec_to_return)
  }))
  colnames(gene_vs_clinical_AUC_diff) = c("condition","baseline","time_before_event","HLA","proportion_train","feature_selection_method","cases","survivors","horizon_time","AUC_difference")
  gene_vs_clinical_AUC_diff = as.data.frame(gene_vs_clinical_AUC_diff)
  gene_vs_clinical_AUC_diff$abundance_type = abundance_type
  gene_vs_clinical_AUC_diff$feature_selection_method = feature_selection_method
  gene_vs_clinical_AUC_diff = gene_vs_clinical_AUC_diff[order(gene_vs_clinical_AUC_diff$AUC),]
  gene_vs_clinical_AUC_diff$rank = 1:nrow(gene_vs_clinical_AUC_diff)
  write.csv(gene_vs_clinical_AUC_diff,file=paste("diff_AUC_plots","_",abundance_type,"_",feature_selection_method,"_",model_type,".csv",sep=""))
  return(gene_vs_clinical_AUC_diff)
}

gene_nottest_diff_AUCs = get_diff_AUCs(output_df_healthy_pret1d,"gene","none","lasso_cox-regression")
gene_nottest_diff_AUCs = get_diff_AUCs(output_df_healthy_pret1d,"gene","ttest","lasso_cox-regression")
gene_nottest_diff_AUCs = get_diff_AUCs(output_df_healthy_pret1d,"pathway","none","lasso_cox-regression")
gene_nottest_diff_AUCs = get_diff_AUCs(output_df_healthy_pret1d,"pathway","ttest","lasso_cox-regression")
gene_nottest_diff_AUCs = get_diff_AUCs(output_df_healthy_pret1d,"species","none","lasso_cox-regression")

gene_nottest_diff_AUCs = get_diff_AUCs(output_df_healthy_pret1d,"gene","ttest","random_survival_forest")
gene_nottest_diff_AUCs = get_diff_AUCs(output_df_healthy_pret1d,"pathway","ttest","random_survival_forest")

```

#Now locally visualize

```{r}
# first get number of samples to have power to do predictions
cox_models = list.files("~/Dropbox (HMS)/TEDDY/teddy_niddk_clinical_072721/TEDDY_DATA/Analysis_Datasets/M_68_LFerrat_NIDDK_Submission/MP68_wc/models/",pattern="Cox_model__",full.names = TRUE)
library(ldatools)

library(survMisc)
library(survival)

coxsnell_rsquared = function(fit) {
  l0 <- fit$loglik[1]
  l1 <- fit$loglik[2]
  n1 <- fit$n
  logtest <- -2 * (fit$loglik[1] - fit$loglik[2])
  rsnell = (1 - exp(-logtest/fit$n))
  return(rsnell)
}

r2_vals = sapply(cox_models, function(myfile) {
  load(myfile)
  #coxsnell = royston(fit)["R.N"]
  coxsnell = coxsnell_rsquared(res.cox)
  return(coxsnell)
})

mean_r2 = mean(r2_vals)
library(pmsampsize)
pmsampsize(type="s",rsquared=mean_r2,parameters=20,shrinkage=0.9,rate=0.1,timepoint=2,meanfup=0.5)


pmsampsize(type="b",rsquared=mean_r2,parameters=20,shrinkage=0.9,prevalence=0.10)



### check power we have given sample size!!
pmsampsize(type="s",rsquared=mean_r2,parameters=20,shrinkage=0.9,rate=0.1,timepoint=2,meanfup=0.5)

pmsampsize(type="s",rsquared=0.329393818652566,parameters=20,shrinkage=0.9,rate=0.1,timepoint=2,meanfup=0.5)

pmsampsize(type="b",rsquared=0.329393818652566,parameters=20,shrinkage=0.9,prevalence=0.10)


## for different powers, what r2 are we powered to detect!!!! 

## use power calculation to choose training cohort size. power > 50-60% 

## compare differences across outcomes. antibody conversion vs diagnosis. 

## differences in genes and pathways found in different modeling approaches. 

#Fig 1. Specification curve for all models. Specification for how many new novel genes are you finding. novel genes as a function of models 
```


```{r}
#setwd("/Users/samuelzimmerman/Dropbox (HMS)/Kostic_Lab/datasets/TEDDY/TEDDY_analysis_v2_march_2022")

#losso_cox_res = read.csv("lasso_rf_regression_output_df_with_AUCs.csv")
# remove AUC NAs
#losso_cox_res = losso_cox_res[!is.na(losso_cox_res$AUC),] # 4148 models
# remove 

#losso_cox_res = losso_cox_res[order(losso_cox_res$AUC),]

#losso_cox_res$rank = 1:nrow(losso_cox_res)
#losso_cox_res$sample_number2 = losso_cox_res$cases_time_horizon + losso_cox_res$survivors_time_horizon

#losso_cox_res$sample_number_log = log(losso_cox_res$sample_number)/5

#ggplot(losso_cox_res,aes(x=rank,y=AUC,color=sample_number_log)) + geom_errorbar(aes(ymin=lower_CI_AUC,ymax=upper_CI_AUC),width=.1) + geom_point()

#losso_cox_res_high_case = losso_cox_res[losso_cox_res$cases_time_horizon>10 & losso_cox_res$survivors_time_horizon > 10,]
#losso_cox_res_high_case[is.na(losso_cox_res_high_case)] = "NA"
#losso_cox_res_high_case$everything = apply(losso_cox_res_high_case,1, function(x) paste(x,collapse="_"))

#losso_cox_res_high_case$lower_CI_AUC[which(losso_cox_res_high_case$AUC<0.5)] = NA
#losso_cox_res_high_case$upper_CI_AUC[which(losso_cox_res_high_case$AUC<0.5)] = NA
#losso_cox_res_high_case$AUC[which(losso_cox_res_high_case$AUC<0.5)] = 0.5
#losso_cox_res_high_case$lower_CI_AUC[which(losso_cox_res_high_case$lower_CI_AUC<0.5)] = 0.5

#losso_cox_res_high_case_noAUC = losso_cox_res_high_case[,-c(1,11,13,14,15,16,17,18,19,20,21)]
#losso_cox_res_high_case_noAUC = as.data.frame(losso_cox_res_high_case_noAUC)
#losso_cox_res_high_case_noAUC[is.na(losso_cox_res_high_case_noAUC)] = "NA"
#losso_cox_res_high_case_noAUC = apply(losso_cox_res_high_case_noAUC,2,as.factor)
#losso_cox_res_high_case_noAUC = as.data.frame(losso_cox_res_high_case_noAUC)

#library(caret)

#mydum_vars = dummyVars(~+ condition + baseline + HLA + proportion_train + model + loss_function + feature_selection_method + microbiome_type + features, data=losso_cox_res_high_case)

#losso_cox_res_high_case_mat = predict(mydum_vars,losso_cox_res_high_case)

#colnames(losso_cox_res_high_case_mat) = gsub("^condition","condition:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^baseline","baseline:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^HLA","HLA:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^proportion_train","proportion_train:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^model","model:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^loss_function","loss_function:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^feature_selection_method","feature_selection_method:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^microbiome_type","microbiome_type:",colnames(losso_cox_res_high_case_mat))
#colnames(losso_cox_res_high_case_mat) = gsub("^features","features:",colnames(losso_cox_res_high_case_mat))

#losso_cox_res_high_case_mat = as.data.frame(losso_cox_res_high_case_mat)
#losso_cox_res_high_case_mat = t(losso_cox_res_high_case_mat)

#library(ComplexHeatmap)
#library(circlize)
#pdf("AUC_condition_heatmap.pdf")
#ht = Heatmap(losso_cox_res_high_case_mat,cluster_rows=FALSE,cluster_columns=FALSE,column_labels=rep("",ncol(losso_cox_res_high_case_mat)),top_annotation = HeatmapAnnotation(scatterplot=anno_points(losso_cox_res_high_case$AUC),annotation_height=unit(3, "cm")))
#print(ht)
#dev.off()

# lets make another version so we can put in confidence intervals

#anno1 = AnnotationFunction(
#    fun = function(index, k, n) {
#        n = length(index)
#        pushViewport(viewport(xscale = c(0.5, n + 0.5), yscale = c(0.5, 1)))
#        grid.rect()
#        grid.segments(1:n, x$lower_CI_AUC, 1:n, x$upper_CI_AUC[index], default.units = "native")
#        grid.points(1:n, x$AUC[index], default.units = "native",size=unit(1, "mm"),gp =gpar(col="red"))
#        # the below will show the point with the max sample size
#        grid.points(1107, 0.9, default.units = "native",size=unit(1, "mm"))
#        if(k == 1) grid.yaxis()
#        popViewport()
#    },
#    var_import = list(x = losso_cox_res_high_case),
#    n = nrow(losso_cox_res_high_case),
#    subsetable = TRUE,
#    height = unit(3, "cm")
#)


#col_fun = colorRamp2(c(min(losso_cox_res_high_case$sample_number2), max(losso_cox_res_high_case$sample_number2)), c("blue", "red"))

#pdf("AUC_condition_heatmap_with_CIs.pdf")
#ht = Heatmap(losso_cox_res_high_case_mat,cluster_rows=FALSE,cluster_columns=FALSE,column_labels=rep("",ncol(losso_cox_res_high_case_mat)),top_annotation = HeatmapAnnotation(AUC=anno1,samples=losso_cox_res_high_case$sample_number2,col=list(samples=col_fun)))
#draw(ht,padding = unit(c(0, 10, 2, 2), "mm"))
#dev.off()

# get max sample size when AUC greater than 0.5
#max(losso_cox_res_high_case$sample_number2[losso_cox_res_high_case$AUC>0.5]) # 376
#losso_cox_res_high_case[losso_cox_res_high_case$sample_number2 == 376,]

# 

#healthy_pre_MIAA_all_HLA_proportion_train_0.5_data = c(AUC=0.5659181,specificity=1,sensitivity=0)
#healthy_pre_MIAA_all_HLA_proportion_train_0.5_data = as.data.frame(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data)
#healthy_pre_MIAA_all_HLA_proportion_train_0.5_data$stat = rownames(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data)
#colnames(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data)[1] = c("value")

#pdf("~/Downloads/highest_non_0.5_auc.pdf")
#ggplot(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data,aes(x=stat,y=value)) + geom_col() + theme_classic() + geom_text(aes(label=value),vjust = -0.5) + theme(axis.text.x = element_text(size=20))
#dev.off()

#healthy_pre_MIAA_all_HLA_proportion_train_0.5_data_beta_coefs = c(MEEMKGCL_64500=-0.004437999,EAJCKFOI_20755=-0.006206360)
#healthy_pre_MIAA_all_HLA_proportion_train_0.5_data_beta_coefs = data.frame(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data_beta_coefs)
#healthy_pre_MIAA_all_HLA_proportion_train_0.5_data_beta_coefs$gene = rownames(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data_beta_coefs)
#colnames(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data_beta_coefs)[1] = c("coef")

#pdf("~/Downloads/highest_non_0.5_auc_coefs.pdf")
#ggplot(healthy_pre_MIAA_all_HLA_proportion_train_0.5_data_beta_coefs,aes(x=gene,y=coef)) + geom_col() + theme_classic() + theme(axis.text.x = element_text(size=20))


#losso_cox_res_high_case_sampSize_order = losso_cox_res_high_case[order(losso_cox_res_high_case$sample_number2,decreasing = TRUE),]
#losso_cox_res_high_case_sampSize_order = losso_cox_res_high_case_sampSize_order[which(losso_cox_res_high_case_sampSize_order$AUC > 0.5),]

#max(losso_cox_res_high_case$sample_number2) # 379
#library(ggalluvial)
#library(dplyr)
#losso_cox_res_high_case_alluvial = losso_cox_res_high_case[,c("condition","baseline","time_before_event","HLA","proportion_train","model","loss_function","feature_selection_method","microbiome_type","features","horizon_time")]
#losso_cox_res_high_case_alluvial = unique(losso_cox_res_high_case_alluvial)

#losso_cox_res_high_case_alluvial = losso_cox_res_high_case_alluvial %>% group_by(condition,baseline,time_before_event,HLA,proportion_train,model,loss_function,feature_selection_method,microbiome_type,features,horizon_time) %>% tally()
#colnames(losso_cox_res_high_case_alluvial)[12] = "Freq"

#losso_cox_res_high_case_alluvial = losso_cox_res_high_case_alluvial[-which(losso_cox_res_high_case_alluvial$time_before_event=="before_condition"),]

#pdf("alluvial_plot.pdf",width=20)
#ggplot(losso_cox_res_high_case_alluvial,aes(y = Freq, axis1 = time_before_event,axis2=baseline,axis3=HLA,axis4=proportion_train,axis5=feature_selection_method,axis6=model,axis7=microbiome_type,axis8=features)) + geom_alluvium(aes(fill=condition)) + geom_stratum(width = 1/12, fill = "black", color = "grey") + geom_label(stat = "stratum", aes(label = after_stat(stratum))) + theme_classic()
#dev.off()

#losso_cox_res_high_case[losso_cox_res_high_case$microbiome_type == "gene" & losso_cox_res_high_case$feature_selection_method=="ttest" & losso_cox_res_high_case$model == "random_survival_forest" & losso_cox_res_high_case$proportion_train == "2-3rds" & losso_cox_res_high_case$HLA=="DR3_DR4_only" & losso_cox_res_high_case$baseline=="18month-24month" & losso_cox_res_high_case$condition == "healthy_pre-t1d",]

#best_model_coefs = c(AIM24=-0.034001569,fdr=0.058676264,grs2=0.007052737,number_autoantibodies1=0.147032426,number_autoantibodies2=0.517607980,number_autoantibodies3=0.190592914)
#best_model_coefs = as.data.frame(best_model_coefs)
#best_model_coefs$names = rownames(best_model_coefs)

#pdf("coefs_best_model.pdf")
#ggplot(best_model_coefs,aes(reorder(x=names,abs(best_model_coefs)),y=best_model_coefs)) + geom_col() + theme_classic()
#dev.off()







#losso_cox_res_high_case_noAUC_melt = melt(losso_cox_res_high_case_noAUC_mat,measure.vars=colnames(losso_cox_res_high_case_noAUC_mat)[-37])

#ggplot(losso_cox_res_high_case_noAUC_melt, aes(everything, variable)) +geom_point(aes(color=value))


#losso_cox_res_high_case_noAUC_mat_t = t(losso_cox_res_high_case_noAUC_mat)

#ggplot(losso_cox_res_high_case_noAUC_melt,aes(x=everything,y=value)) + geom

#ggplot(subset(losso_cox_res_high_case,features!="microbiome"),aes(x=1:nrow(subset(losso_cox_res_high_case,features!="microbiome")),y=AUC,color=sample_number2)) + geom_point()


#ggplot(subset(losso_cox_res_high_case,features=="microbiome"),aes(x=1:nrow(subset(losso_cox_res_high_case,features=="microbiome")),y=AUC,color=sample_number2)) + geom_point()


#ggplot(subset(losso_cox_res_high_case,features=="microbiome" & condition=="healthy_pre-t1d"),aes(x=1:nrow(subset(losso_cox_res_high_case,features=="microbiome" & condition=="healthy_pre-t1d")),y=AUC,color=sample_number2)) + geom_point()

#ggplot(subset(losso_cox_res_high_case,features=="microbiome" & condition=="healthy_pre-MIAA"),aes(x=1:nrow(subset(losso_cox_res_high_case,features=="microbiome" & condition=="healthy_pre-MIAA")),y=AUC,color=sample_number2)) + geom_point()

#ggplot(subset(losso_cox_res_high_case,features=="microbiome" & condition=="serconverters_or_T1D"),aes(x=1:nrow(subset(losso_cox_res_high_case,features=="microbiome" & condition=="serconverters_or_T1D")),y=AUC,color=sample_number2)) + geom_point()






#losso_cox_res_high_case = losso_cox_res[losso_cox_res$cases_time_horizon>10 & #losso_cox_res$survivors_time_horizon > 10,]
#losso_cox_res_high_case$rank = 1:nrow(losso_cox_res_high_case)

#library(plotly)

#plot_ly(data = losso_cox_res, x = ~rank, y = ~AUC,color=~condition,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>total_samples:',sample_number, '<br>CI:',paste(round(lower_CI_AUC,4),round(upper_CI_AUC,4),sep="-")))


#plot_ly(data = losso_cox_res_high_case, x = ~rank, y = ~AUC,color=~condition,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>features:',features,'<br>microbiome_type:',microbiome_type,'<br>CI:',paste(round(lower_CI_AUC,4),round(upper_CI_AUC,4),sep="-")))

#plot_ly(data = losso_cox_res_high_case[losso_cox_res_high_case$features == "microbiome",], x = ~rank, y = ~AUC,color=~microbiome_type,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>features:',features,'<br>microbiome_type:',microbiome_type,'<br>CI:',paste(round(lower_CI_AUC,4),round(upper_CI_AUC,4),sep="-")))

#plot_ly(data = losso_cox_res[losso_cox_res$features == "microbiome",], x = ~rank, y = ~AUC,color=~microbiome_type,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>features:',features,'<br>microbiome_type:',microbiome_type,'<br>CI:',paste(round(lower_CI_AUC,4),round(upper_CI_AUC,4),sep="-")))



#plot_diff_AUC = function(fileName) {
#  mycsv = read.csv(fileName)
#  mycsv = mycsv[mycsv$cases >= 10 & mycsv$survivors >= 10,]
#  mycsv = mycsv[!is.na(mycsv$AUC_difference),]
#  mycsv = mycsv[order(mycsv$AUC_difference),]
#  mycsv$rank = 1:nrow(mycsv)
#  myplot = ggplot(mycsv,aes(x=rank,y=AUC_difference,color=HLA)) + geom_point()
#  print(myplot)
#  return(myplot)
#}


#plot_diff_AUC(fileName="diff_AUC_plots_gene_none_lasso_cox-regression.csv")
#plot_diff_AUC(fileName="diff_AUC_plots_gene_ttest_lasso_cox-regression.csv")
#plot_diff_AUC(fileName="diff_AUC_plots_gene_ttest_random_survival_forest.csv")
#plot_diff_AUC(fileName="diff_AUC_plots_pathway_none_lasso_cox-regression.csv")
#plot_diff_AUC(fileName="diff_AUC_plots_pathway_ttest_lasso_cox-regression.csv")
#plot_diff_AUC(fileName="diff_AUC_plots_pathway_ttest_random_survival_forest.csv")
#plot_diff_AUC(fileName="diff_AUC_plots_species_none_lasso_cox-regression.csv")













#plot_ly(data = losso_cox_res_high_case, x = ~rank, y = ~AUC,color=~features,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>features:',features,'<br>microbiome_type:',microbiome_type))


#plot_ly(data = losso_cox_res_high_case[losso_cox_res_high_case$features == "microbiome",], x = ~rank, y = ~AUC,color=~condition,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>features:',features,'<br>microbiome_type:',microbiome_type))



#plot_ly(data = losso_cox_res[losso_cox_res$condition == "healthy_pre-t1d",], x = ~rank, y = ~AUC,color=~HLA,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>total_samples:',sample_number))

#plot_ly(data = losso_cox_res[losso_cox_res$condition == "healthy_pre-MIAA",], x = ~rank, y = ~AUC,color=~HLA,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time,'<br>total_samples:',sample_number))



#plot_ly(data = losso_cox_res_high_case[losso_cox_res_high_case$condition == "healthy_pre-MIAA",], x = ~rank, y = ~AUC,color=~HLA,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time))

#plot_ly(data = losso_cox_res[losso_cox_res$feature_selection_method == "ttest",], x = ~rank, y = ~AUC,color=~condition,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time))

#plot_ly(data = losso_cox_res_high_case[losso_cox_res_high_case$feature_selection_method == "ttest" & losso_cox_res_high_case$model == "random_survival_forest",], x = ~rank, y = ~AUC,color=~condition,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time))

#plot_ly(data = losso_cox_res[losso_cox_res$feature_selection_method == "ttest" & losso_cox_res$model == "lasso_cox-regression",], x = ~rank, y = ~AUC,color=~condition,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time))


#plot_ly(data = losso_cox_res[which(losso_cox_res$feature_selection_method == "ttest" & losso_cox_res$condition == "healthy_pre-t1d" & losso_cox_res$horizon_time == "one_year_horizon") ,], x = ~rank, y = ~AUC,color=~condition,type="scatter",
#        text = ~paste("HLA: ", HLA, '<br>cases_horizon:', cases_time_horizon,'<br>survivors_horizon:',survivors_time_horizon,'<br>condition:',condition,'<br>baseline',baseline,'<br>horizon',horizon_time))



#losso_cox_res[which(losso_cox_res$feature_selection_method == "ttest" & losso_cox_res$condition == "healthy_pre-t1d"),]
```



#Also lets run maaslin!

```{bash}
# strategy for maaslin is we are going to use the ones that are not involved with month 

/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/aa.csv

/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv


ls prep_voe_input_files | grep -v "month"



ls /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/*.csv | grep -v "healthy" | grep -v "total_genes_left_each_timepoint.csv" | while read line
do 
sbatch -c 1 -t 0-10:00 -p short --mem=50G  scripts/run_maaslin.bash ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv DR3_DR4_only,DR4_DR4_only,DR4_DR8_only,DR3_DR3_only,DR4_DR1_only,DR4_DR13,all MIAA,GAD,IA2A,seroconverters,triple_converters_vs_T1D,T1D,serconverters_or_T1D /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut
done

# 2 jobs timed out. lets finish. fb and hx. for fb IA2A_all was last written. for hx T1D_all was last

sbatch -c 1 -t 0-04:00 -p short --mem=50G  scripts/run_maaslin.bash /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/fb.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv all IA2A,seroconverters,triple_converters_vs_T1D,T1D,serconverters_or_T1D /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut

sbatch -c 1 -t 0-04:00 -p short --mem=50G  scripts/run_maaslin.bash /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/hx.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv all T1D,serconverters_or_T1D /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut

```

#Now we are going to calculate which genes are significant

```{r}
library(dplyr)
library(data.table)
maaslin_folders = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut",pattern="all_results.tsv",recursive = TRUE,full.names = TRUE)

maaslin_out_split = strsplit(basename(dirname(maaslin_folders)),split="_")

condition_hla_info = sapply(maaslin_out_split, function(x) paste(x[seq(1,length(x)-3)],collapse="_"))

maaslin_folders_df = data.frame(output_files=maaslin_folders,condition_hla_info=condition_hla_info)
maaslin_folders_df_split = split(maaslin_folders_df,maaslin_folders_df$condition_hla_info)

sig_genes = do.call("rbind",lapply(maaslin_folders_df_split, function(mydf) {
  print(unique(mydf[,2]))
  all_results = bind_rows(apply(mydf, 1, function(myrow) {
    myfile = myrow[1]
    temp_df = read.table(myfile,sep="\t",header=TRUE)
    # remove age_at_collection
    temp_df = temp_df[temp_df$value!= "age_at_collection",]
    return(temp_df)
  }))
  all_results$BY = p.adjust(all_results$pval,method="BY")
  all_results$BH = p.adjust(all_results$pval,method="BH")
  all_results$condition_HLA = unique(mydf[,2])
  all_results_sig = all_results[all_results$BH < 0.05,]
  all_results_sig = all_results_sig[order(all_results_sig$pval),]
  return(all_results_sig)
}))

# lets exclude triple_converters_vs_T1D cause I am not really confident about that comparison

sig_genes_no_triple = sig_genes[-grep("triple_converters_vs_T1D",sig_genes$condition),]

# lets get the function

prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=FALSE)

annotations = prokka_annotations[match(sig_genes_no_triple$feature,prokka_annotations$V1),]

sig_genes_no_triple$protein_function = annotations$V7

write.csv(sig_genes_no_triple,"sig_genes_mixed_effect_moedles_no_triple.csv")

# see if glucose-6 phosphate a known mimic is sig different
dim(sig_genes_no_triple[grep("Glucose-6-phosphate",sig_genes_no_triple$protein_function),]) # 376 genes




sig_genes_no_triple_BY = sig_genes_no_triple[sig_genes_no_triple$BY < 0.05,]

table(sig_genes_no_triple_BY$condition)

table(sig_genes_no_triple$condition)

# make scatter plots

gene_to_file_mapping = fread("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE)

locations_of_genes = unique(gene_to_file_mapping[match(sig_genes_no_triple_BY$feature,gene_to_file_mapping$V1),"V2"])
locations_of_genes = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/",locations_of_genes$V2,sep="")
# get abundance of all the genes

abundance_sig_BY_genes = do.call("rbind",lapply(locations_of_genes, function(myfile) {
  d_small = fread(myfile,sep=",",header=TRUE,data.table=FALSE,nrow=1)
  d_small = d_small[,-1]
  ## aa.csv will have an extra row for the gene names so lets remove that
  has_extra_row = d_small[1,1]=="genename"
  if(has_extra_row == TRUE) {
    d = fread(myfile,sep=",",header=TRUE,data.table=FALSE,skip=1)
    d = d[,-1]
  } else {
    d = fread(myfile,sep=",",header=TRUE,data.table=FALSE)
    d = d[,-1]
  }
  rownames(d) = d$genename
  d = d[,-1]
  filtered_d = d[rownames(d)%in%sig_genes_no_triple_BY$feature,,drop=FALSE]
  return(filtered_d)
}))
abundance_sig_BY_genes_t = t(abundance_sig_BY_genes)
sig_genes_no_triple_BY$condition = gsub("_only","",sig_genes_no_triple_BY$condition)

library(ggplot2)
metadata = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv")
make_scatter_plot = function(geneName,metadata) {
  abundance_vals = abundance_sig_BY_genes_t[,geneName,drop=FALSE]
  geneStats = sig_genes_no_triple_BY[match(geneName,sig_genes_no_triple_BY$feature),]
  condition_split = strsplit(geneStats$condition,split="_")[[1]]
  if(grepl("_all",geneStats$condition)) {
    HLA_temp = paste(condition_split[length(condition_split)],collapse="_")
    condition_temp = paste(condition_split[seq(1,length(condition_split)-1)],collapse="_")
  } else {
    HLA_temp = paste(condition_split[seq(length(condition_split)-1,length(condition_split))],collapse="_")
    condition_temp = paste(condition_split[seq(1,length(condition_split)-2)],collapse="_")
  }
  if(condition_temp == "MIAA") {
    controls = metadata %>% dplyr::filter(is.na(age_first_MIAA)) %>% mutate(condition = 0)
    cases = metadata %>% dplyr::filter(age_at_collection<age_first_MIAA,!is.na(age_first_MIAA)) %>% mutate(condition = 1)
  } else if (condition_temp == "GAD") {
    controls = metadata %>% dplyr::filter(is.na(age_first_GAD)) %>% mutate(condition = 0)
    cases = metadata %>% dplyr::filter(age_at_collection<age_first_GAD,!is.na(age_first_GAD)) %>% mutate(condition = 1)
  } else if (condition_temp == "IA2A"){
    controls = metadata %>% dplyr::filter(is.na(age_first_IA2A)) %>% mutate(condition = 0)
    cases = metadata %>% dplyr::filter(age_at_collection<age_first_IA2A,!is.na(age_first_IA2A)) %>% mutate(condition = 1)
  } else if (condition_temp == "seroconverters"){
    controls = metadata %>% dplyr::filter(t1d_sero_control == 'control') %>% mutate(condition = 0)
    cases = metadata %>% dplyr::filter(age_at_collection<age_mult_persist,!is.na(age_mult_persist)) %>% mutate(condition = 1)
  } else if (condition_temp == "triple_converters_vs_T1D") {
    controls = metadata %>% dplyr::filter(t1d_sero_control == 'seroconverted') %>% filter(three_persist_conf == TRUE) %>% mutate(condition = 0)
    cases = metadata %>% dplyr::filter(t1d == TRUE, T1D_Outcome=='Before') %>% mutate(condition = 1)
  } else if (condition_temp == "T1D") {
    controls = metadata %>% dplyr::filter(t1d == FALSE) %>% mutate(condition = 0)
    cases = metadata %>% dplyr::filter(t1d == TRUE, T1D_Outcome=='Before') %>% mutate(condition = 1)
  } else if (condition_temp == "serconverters_or_T1D") {
    controls = metadata %>% dplyr::filter(t1d_sero_control == 'control') %>% mutate(condition = 0)
    cases_T1D = metadata %>% dplyr::filter(t1d == TRUE, T1D_Outcome=='Before') %>% mutate(condition = 1)
    cases_sero = metadata %>% dplyr::filter(age_at_collection<age_mult_persist,!is.na(age_mult_persist)) %>% mutate(condition = 1)
    cases_run = unique(c(cases_T1D$Run,cases_sero$Run))
    cases = metadata[match(cases_run,metadata$Run),]  %>% mutate(condition = 1)
  }
  metadata = rbind(controls,cases)

    if(HLA_temp == "DR3_DR4") {
    # remove subjects that are not eligable. only 68 samples so shouldn't be a huge deal to remove
    metadata = metadata %>% filter(HLA_Category.x!= "Not*Eligible")
    metadata = metadata %>% filter(HLA_Category.x== "DR4*030X/0302*DR3*0501/0201")
  } else if (HLA_temp == "DR4_DR4") {
    metadata = metadata %>% filter(HLA_Category.x!= "Not*Eligible")
    metadata = metadata %>% filter(HLA_Category.x== "DR4*030X/0302*DR4*030X/0302")
  } else if (HLA_temp == "DR4_DR8") {
    metadata = metadata %>% filter(HLA_Category.x!= "Not*Eligible")
    metadata = metadata %>% filter(HLA_Category.x== "DR4*030X/0302*DR8*0401/0402")
  } else if (HLA_temp == "DR3_DR3") {
    metadata = metadata %>% filter(HLA_Category.x!= "Not*Eligible")
    metadata = metadata %>% filter(HLA_Category.x== "DR3*0501/0201*DR3*0501/0201")
  } else if (HLA_temp == "DR4_DR1") {
    metadata = metadata %>% filter(HLA_Category.x!= "Not*Eligible")
    metadata = metadata %>% filter(HLA_Category.x== "DR4*030X/0302*DR1*0101/0501")
  } else if (HLA_temp == "DR4_DR13") {
    metadata = metadata %>% filter(HLA_Category.x!= "Not*Eligible")
    metadata = metadata %>% filter(HLA_Category.x== "DR4*030X/0302*DR13*0102/0604")
  }
  metadata_abundance_samples = intersect(metadata$Run,rownames(abundance_vals))
  
  abundance_vals = abundance_vals[match(metadata_abundance_samples,rownames(abundance_vals)),]
  
  abundance_vals <- replace(abundance_vals, abundance_vals == 0, min(abundance_vals[abundance_vals>0]) / 2)
  abundance_vals = scale(abundance_vals)
  
  metadata = metadata[match(metadata_abundance_samples,metadata$Run),]
  metadata$condition[metadata$condition == 1] = condition_temp
  metadata$condition[metadata$condition == 0] = "ctrl"
  metadata$condition = as.factor(metadata$condition)
  metadata$abundance_vals = abundance_vals[,1]
  title_prefix = paste(geneStats$feature,"\n",geneStats$condition_HLA,"\n",geneStats$protein_function,"\n","BY:",geneStats$BY,sep="")
  pdf(paste(geneStats$feature,"_",geneStats$condition_HLA,".pdf",sep=""))
  plot1 = ggplot(metadata,aes(x=age_at_collection,y=abundance_vals,fill=condition,color=condition)) + geom_point() + geom_smooth() + theme_classic() + ggtitle(title_prefix)
  plot2 = ggplot(metadata,aes(x=condition,y=abundance_vals)) + geom_boxplot(outlier.shape = NA) +  geom_jitter() + theme_classic() + ggtitle(title_prefix)
  print(plot1)
  print(plot2)
  dev.off()
}

scatter_plots_all = sapply(sig_genes_no_triple_BY$feature, function(geneName) {
  scatter_plots = make_scatter_plot(geneName,metadata)
})
```

#Use maaslin to see if I can identify genes differentially abundant X months before T1D vs earlier

```{bash}

sbatch -c 1 -t 0-06:00 -p short --mem=50G  scripts/run_maaslin_before_after_condition_cases_only.bash /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/aa.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv all MIAA,GAD,IA2A,seroconverters,T1D,serconverters_or_T1D 30,30-60,60,60-90,90 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut_before_condition

ls /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/*.csv | grep -v "healthy" | grep -v "total_genes_left_each_timepoint.csv" | grep -v "aa.csv" | while read line
do 
sbatch -c 1 -t 0-06:00 -p short --mem=40G  scripts/run_maaslin_before_after_condition_cases_only.bash ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv all_HLA MIAA,GAD,IA2A,seroconverters,T1D,serconverters_or_T1D 30,30-60,60,60-90,90 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut_before_condition
done



```

```{r}
library(dplyr)
library(data.table)
maaslin_folders = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut_before_condition",pattern="all_results.tsv",recursive = TRUE,full.names = TRUE)

maaslin_out_split = strsplit(basename(dirname(maaslin_folders)),split="_")

condition_hla_info = sapply(maaslin_out_split, function(x) paste(x[seq(1,length(x)-5)],collapse="_"))

maaslin_folders_df = data.frame(output_files=maaslin_folders,condition_hla_info=condition_hla_info)
maaslin_folders_df_split = split(maaslin_folders_df,maaslin_folders_df$condition_hla_info)

sig_genes = do.call("rbind",lapply(maaslin_folders_df_split, function(mydf) {
  print(unique(mydf[,2]))
  all_results = bind_rows(apply(mydf, 1, function(myrow) {
    myfile = myrow[1]
    temp_df = read.table(myfile,sep="\t",header=TRUE)
    # remove age_at_collection
    temp_df = temp_df[temp_df$value!= "age_at_collection",]
    return(temp_df)
  }))
  all_results$BY = p.adjust(all_results$pval,method="BY")
  all_results$BH = p.adjust(all_results$pval,method="BH")
  all_results$condition_HLA = unique(mydf[,2])
  all_results_sig = all_results[all_results$BH < 0.05,]
  all_results_sig = all_results_sig[order(all_results_sig$pval),]
  return(all_results_sig)
}))

# lets get the function

prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=TRUE)
setkey(prokka_annotations,V1)
annotations = prokka_annotations[sig_genes$feature,]

sig_genes$protein_function = annotations$V7

sig_genes_BYsig = sig_genes[sig_genes$BY < 0.05,]

write.csv(sig_genes,"sig_genes_mixed_effect_model_before_condition.csv")

```

#Do it for training only now so I can test on other data

```{bash}


ls /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/*.csv | grep -v "healthy" | grep -v "total_genes_left_each_timepoint.csv" | while read line
do 
sbatch -c 1 -t 0-01:00 -p short --mem=20G scripts/run_maaslin_before_after_condition_cases_only_prediction.bash ${line} all_HLA MIAA,GAD,IA2A,seroconverters,T1D,serconverters_or_T1D 90 0.66 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut_before_condition_train_only_2
done

#ls /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/*.csv | grep -v "healthy" | grep -v "total_genes_left_each_timepoint.csv" | grep -v "aa.csv" | while read line
#do 
#sbatch -c 1 -t 0-06:00 -p short --mem=40G scripts/run_maaslin_before_after_condition_cases_only_prediction.bash /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/aa.csv all_HLA MIAA,GAD,IA2A,seroconverters,T1D,serconverters_or_T1D 90 0.66 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut_before_condition_train_only
#done

```

#Lets check number of significant genes

```{r}
library(dplyr)
library(data.table)
maaslin_folders = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut_before_condition_train_only_2",pattern="all_results.tsv",recursive = TRUE,full.names = TRUE)

maaslin_out_split = strsplit(basename(dirname(maaslin_folders)),split="_")

condition_hla_info = sapply(maaslin_out_split, function(x) paste(x[seq(1,length(x)-1)],collapse="_"))

maaslin_folders_df = data.frame(output_files=maaslin_folders,condition_hla_info=condition_hla_info)
maaslin_folders_df_split = split(maaslin_folders_df,maaslin_folders_df$condition_hla_info)

sig_genes = lapply(maaslin_folders_df_split, function(mydf) {
  print(unique(mydf[,2]))
  all_results = apply(mydf, 1, function(myrow) {
    myfile = myrow[1]
    temp_df = fread(myfile,sep="\t",header=TRUE)
    # remove age_at_collection
    temp_df = temp_df[temp_df$value!= "age_at_collection",]
    return(temp_df)
  })
  all_results = bind_rows(all_results)
  all_results$BY = p.adjust(all_results$pval,method="BY")
  all_results$BH = p.adjust(all_results$pval,method="BH")
  all_results$condition_HLA = unique(mydf[,2])
  all_results_sig = all_results[all_results$BH < 0.05,]
  all_results_sig = all_results_sig[order(all_results_sig$pval),]
  return(all_results_sig)
})
sig_genes = do.call("rbind",sig_genes)

# lets get the function

prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=TRUE)
setkey(prokka_annotations,V1)
annotations = prokka_annotations[sig_genes$feature,]

sig_genes$protein_function = annotations$V7






gene_to_file_mapping = fread("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE)

locations_of_genes = unique(gene_to_file_mapping[match(sig_genes_no_triple_BY$feature,gene_to_file_mapping$V1),"V2"])
locations_of_genes = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/",locations_of_genes$V2,sep="")
# get abundance of all the genes

abundance_sig_BY_genes = do.call("rbind",lapply(locations_of_genes, function(myfile) {
  d_small = fread(myfile,sep=",",header=TRUE,data.table=FALSE,nrow=1)
  d_small = d_small[,-1]
  ## aa.csv will have an extra row for the gene names so lets remove that
  has_extra_row = d_small[1,1]=="genename"
  if(has_extra_row == TRUE) {
    d = fread(myfile,sep=",",header=TRUE,data.table=FALSE,skip=1)
    d = d[,-1]
  } else {
    d = fread(myfile,sep=",",header=TRUE,data.table=FALSE)
    d = d[,-1]
  }
  rownames(d) = d$genename
  d = d[,-1]
  filtered_d = d[rownames(d)%in%sig_genes_no_triple_BY$feature,,drop=FALSE]
  return(filtered_d)
}))
abundance_sig_BY_genes_t = t(abundance_sig_BY_genes)

```

#Next step is to make the abundance files

```{bash}
ls maaslin_ouptut_before_condition_train_only_2 | rev | cut -d"_" -f2- | rev | uniq > maaslin_ouptut_before_condition_train_only_2_folders.txt

folder_prefix = "healthy_pre-sero-3month_before_vs_far_from_condition-all_HLA_proportion_train_0.66_maaslin_output"
maaslin_folders = "maaslin_ouptut_before_condition_train_only_2"


while read line
do
sbatch -c 1 -t 0-06:00 -p short --mem=40G scripts/make_abundance_tables.bash maaslin_ouptut_before_condition_train_only_2 ${line}
done < maaslin_ouptut_before_condition_train_only_2_folders.txt


sbatch -c 1 -t 0-11:00 -p short --mem=100G scripts/make_abundance_tables.bash maaslin_ouptut_before_condition_train_only_2 healthy_pre-IA2A-3month_before_vs_far_from_condition-all_HLA_proportion_train_0.66_maaslin_output

# run logistic regression

```

#Cretae input file

```{r}
test_abundance_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/maaslin_ouptut_before_condition_train_only_2",pattern="_test_abundance.csv",full.names = TRUE)
train_abundance_files = gsub("_test_abundance.csv","_train_abundance.csv",test_abundance_files)
train_metadata_files = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",gsub("_maaslin_output_train_abundance.csv","_train_metadata.csv",basename(train_abundance_files)),sep="")
test_metadata_files = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",gsub("_maaslin_output_train_abundance.csv","_test_metadata.csv",basename(train_abundance_files)),sep="")
train_sujbects1_files = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",gsub("_maaslin_output_train_abundance.csv",".train_subjects_1.txt",basename(train_abundance_files)),sep="")
train_sujbects2_files = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",gsub("_maaslin_output_train_abundance.csv",".train_subjects_2.txt",basename(train_abundance_files)),sep="")
train_sujbects3_files = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",gsub("_maaslin_output_train_abundance.csv",".train_subjects_3.txt",basename(train_abundance_files)),sep="")
test_subjects_files = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",gsub("_maaslin_output_train_abundance.csv",".test_subjects.txt",basename(train_abundance_files)),sep="")

mydf = data.frame(test_abundance_files,train_abundance_files,train_metadata_files,test_metadata_files,train_sujbects1_files,train_sujbects2_files,train_sujbects3_files,test_subjects_files)

write.table(mydf,file="maaslin_ouptut_before_condition_train_only_2_input_table.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)
```

```{bash}

while read line
do
sbatch -n 1 -c 1 --mem=30G -p short -t 0-00:30 scripts/run_lasso_binary_all_clinical_sample_level.bash ${line} microbiome NA 1 NA
done < maaslin_ouptut_before_condition_train_only_2_input_table.csv

```

#OK lets try another method. We are going to create paired data using cases only. First thing to do is average samples together

```{r}
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4")

abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern=".csv",full.names = TRUE)
abundance_files = abundance_files[-grep("healthy",abundance_files)]
abundance_files = abundance_files[-grep("total_genes_left_each_timepoint.csv",abundance_files)]
mapping_files = list.files(pattern = ".mapping.csv")
mapping_files = mapping_files[grep("_before_vs_far_from_condition_subject_level",mapping_files)]

training_metadata_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66_train_metadata.csv",mapping_files)
training_metadata_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5_train_metadata.csv",mapping_files)
testing_metadata_files_two_third_train = gsub(".mapping.csv","_proportion_train_0.66_test_metadata.csv",mapping_files)
testing_metadata_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5_test_metadata.csv",mapping_files)


training_data_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66.train_subjects.txt",mapping_files)
training_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.train_subjects.txt",mapping_files)
testing_data_files_two_third_train = gsub(".mapping.csv","_proportion_train_0.66.test_subjects.txt",mapping_files)
testing_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.test_subjects.txt",mapping_files)

mapping_metadata_two_third_train = data.frame(training_metadata_files_two_thirds_train,testing_metadata_files_two_third_train,mapping_files,training_data_files_two_thirds_train,testing_data_files_two_third_train)

mapping_metadata_fifty_fity_train = data.frame(training_metadata_files_fifty_train,testing_metadata_files_fifty_train,mapping_files,training_data_files_fifty_train,testing_data_files_fifty_train)

write_output_files = function(mapping_metadata) {
  df_lists = apply(mapping_metadata, 1, function(myrow) {
    train_metadata_file = myrow[1]
    test_metadata_file = myrow[2]
    mapping_file = myrow[3]
    training_file = myrow[4]
    testing_file = myrow[5]
    train_metadata_file_rep = rep(train_metadata_file,length(abundance_files))
    test_metadata_file_rep = rep(test_metadata_file,length(abundance_files))
    mapping_files_rep = rep(mapping_file,length(abundance_files))
    training_files_rep = rep(training_file,length(abundance_files))
    testing_files_rep = rep(testing_file,length(abundance_files))
    my_df = data.frame(abundance_files,train_metadata_file_rep,test_metadata_file_rep,mapping_files_rep,training_files_rep,testing_files_rep)
    # split into 2 pieces
    split_point = floor(nrow(my_df)/2)
    my_df1 = my_df[1:split_point,]
    my_df2 = my_df[(split_point +1):nrow(my_df),]
    # this extra column is just so I don't mess with the metadata since I am doing things in parallel
    my_df1$metadata_suffix = 1
    my_df2$metadata_suffix = 2
    my_label1 = gsub(".train_subjects.txt","_input_file_1.tsv",training_file)
    my_label2 = gsub(".train_subjects.txt","_input_file_2.tsv",training_file)
    write.table(my_df1,file=paste("prep_voe_input_files_before_vs_far_from_condition_subject_level/",my_label1,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
    write.table(my_df2,file=paste("prep_voe_input_files_before_vs_far_from_condition_subject_level/",my_label2,sep=""),sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
    return(my_df)
  })
  return(df_lists)
}
output_files_two_thirds_train = write_output_files(mapping_metadata=mapping_metadata_two_third_train)
output_files_fifty_train = write_output_files(mapping_metadata=mapping_metadata_fifty_fity_train)

```

```{bash}
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4

# lets just do the ones we think are most likely to succeed to make this quick. we can do more later

ls prep_voe_input_files_before_vs_far_from_condition_subject_level/*-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66_input_file_*.tsv | grep -v "2month-3month" > files_to_do_for_quickness.txt



while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < files_to_do_for_quickness.txt

# do rest

ls prep_voe_input_files_before_vs_far_from_condition_subject_level/* | grep -v -f files_to_do_for_quickness.txt > rest_of_before_vs_far_from_files.txt

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=50G prep_abundance_for_voe_bulk_training_data_v2.bash ${line}
done < rest_of_before_vs_far_from_files.txt

# transform abundances and filter


cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/*/ | grep "_before_vs_far_from_condition_subject_level" | grep -v "prep_voe_input" | rev | cut -c2- | rev > input_folder_list_parsed_4_before_vs_far_from_condition_subject_level.txt

while read line
do
  sbatch -n 1 -c 1 --mem=60G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_before_vs_far_from_condition_subject_level.txt

# do rest

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/*/ | grep "_before_vs_far_from_condition_subject_level" | grep -v "prep_voe_input" | rev | cut -c2- | rev > input_folder_list_parsed_4_before_vs_far_from_condition_subject_level.txt

while read line
do
  sbatch -n 1 -c 1 --mem=20G -p short -t 0-01:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_before_vs_far_from_condition_subject_level.txt

sacct | grep "filter_no" | grep "OUT_OF_ME" | awk '{print $1}' | while read line; do grep -A 1 -m 1 "abundance_train" slurm-$line.out | tail -n 1 | tr -d '"'; done | while read line; do dirname $line; done > input_folder_list_parsed_4_before_vs_far_from_condition_subject_level_outmem.txt

while read line
do
  sbatch -n 1 -c 1 --mem=30G -p short -t 0-01:00 scripts/filter_normalize_abundance_metadata_data_v3.bash ${line} 0.9 1
done < input_folder_list_parsed_4_before_vs_far_from_condition_subject_level_outmem.txt

```

```{r}
all_dirs = list.dirs("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",recursive=FALSE)
all_dirs = all_dirs[!grepl("prep_voe_input_files",all_dirs)]
all_dirs = all_dirs[grepl("_before_vs_far_from_condition_subject_level",all_dirs)]

transformed_abundance_file_list = lapply(all_dirs, function(x) list.files(x,pattern="transform",full.names = TRUE))
names(transformed_abundance_file_list) = all_dirs

transformed_abundance_file_list = transformed_abundance_file_list[sapply(transformed_abundance_file_list, length)==2]

test_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_test.csv",x)])
train_abundance_data = sapply(transformed_abundance_file_list, function(x) x[grep("filtered_transformed_abundance_train.csv",x)])

input_info = data.frame(test_abundance_data,train_abundance_data)

# now get metadata

metadata_train = paste(rownames(input_info),"_train_metadata.csv",sep="")
metadata_test = paste(rownames(input_info),"_test_metadata.csv",sep="")

train_fold1 = paste(rownames(input_info),".train_subjects_1.txt",sep="")
train_fold2 = paste(rownames(input_info),".train_subjects_2.txt",sep="")
train_fold3 = paste(rownames(input_info),".train_subjects_3.txt",sep="")
test_subjects = paste(rownames(input_info),".test_subjects.txt",sep="")


input_info$metadata_train = metadata_train
input_info$metadata_test = metadata_test
input_info$train_fold1 = train_fold1
input_info$train_fold2 = train_fold2
input_info$train_fold3 = train_fold3
input_info$test_subjects = test_subjects

write.csv(input_info,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level.csv",row.names = FALSE,quote=FALSE)

```

```{bash}
# remove header
tail -n +2 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level.csv > /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

head -n 2 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv | tail -n 1 > test.csv

# now do univariate feature selection

#while read line
#do
#  sbatch -n 1 -c 5 --mem=12G -p short -t 0-06:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection.bash ${line} 5
#done < test.csv

while read line
do
  sbatch -n 1 -c 5 --mem=12G -p short -t 0-06:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection.bash ${line} 5
done < models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

#while read line
#do
#  sbatch -n 1 -c 10 --mem=60G -p medium -t 1-00:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_v2.bash ${line} 10
#done < test.csv

sacct -S 2023-07-24 | grep "run_paire" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do head -n 1 slurm-$line.out; done | while read line; do grep $line models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv; done > FAILED_jobs_to_redo.txt

while read line
do
  sbatch -n 1 -c 5 --mem=30G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection.bash ${line} 5
done < FAILED_jobs_to_redo.txt

sacct -S 2023-07-25 | grep "run_paire" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do head -n 1 slurm-$line.out; done | while read line; do grep $line FAILED_jobs_to_redo.txt; done > "FAILED_jobs_to_redo2.txt"

while read line
do
  sbatch -n 1 -c 5 --mem=60G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection.bash ${line} 5
done < FAILED_jobs_to_redo2.txt

sacct -S 2023-07-26 | grep "run_paire" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do head -n 1 slurm-$line.out; done | while read line; do grep $line FAILED_jobs_to_redo.txt; done > "FAILED_jobs_to_redo3.txt"

while read line
do
  sbatch -n 1 -c 5 --mem=60G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection.bash ${line} 5
done < FAILED_jobs_to_redo3.txt

grep "all_HLA" models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv | grep -v "serconverters_or_T1D" > models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA.csv


while read line
do
  sbatch -n 1 -c 5 --mem=12G -p short -t 0-06:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_cutoff.bash ${line} 5 0.9 test
done < models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA.csv

sacct -S 2023-08-24 | grep -A 10000 "15673260" | grep "run_paire" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do head -n 1 slurm-$line.out; done | while read line; do grep $line models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA.csv; done > failed_test_jobs.csv

while read line
do
  sbatch -n 1 -c 5 --mem=30G -p short -t 0-06:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_cutoff.bash ${line} 5 0.9 test
done < failed_test_jobs.csv

```

# look at significant genes

```{r}

my_genes = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern="output_ttest_results.rds",full.names = TRUE,recursive = TRUE)

my_genes_before_vs_far = my_genes[grep("before_vs_far_from_condition_subject_level",my_genes)]
my_genes_before_vs_far_list = lapply(my_genes_before_vs_far,function(x) readRDS(x))

my_genes_before_vs_far_list_BY = lapply(my_genes_before_vs_far_list, function(x) x[[2]])
names(my_genes_before_vs_far_list_BY) = my_genes_before_vs_far

mean(sapply(my_genes_before_vs_far_list_BY,length))

my_genes_before_vs_far_list_BY_gt0 = my_genes_before_vs_far_list_BY[sapply(my_genes_before_vs_far_list_BY,length)>0] # 186

# we want to do lmer for only files that have more than 0 significant genes
input_csv = read.table("models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv",sep=",",header=FALSE)

dirs_to_get = basename(dirname(names(my_genes_before_vs_far_list_BY_gt0)))

input_csv_for_lmer = input_csv[sapply(dirs_to_get, function(x) grep(x,input_csv[,1])),]
write.table(input_csv_for_lmer,file="models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)



# now do this for test data

my_genes = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern="0.9testoutput_ttest_results.rds",full.names = TRUE,recursive = TRUE)
my_genes = my_genes[-grep("species",my_genes)]
my_genes = my_genes[-grep("pathway",my_genes)]

my_genes_before_vs_far_list = lapply(my_genes,function(x) readRDS(x))

my_genes_before_vs_far_list_BY = lapply(my_genes_before_vs_far_list, function(x) x[[2]])
names(my_genes_before_vs_far_list_BY) = my_genes
my_genes_before_vs_far_list_BY_gt0 = my_genes_before_vs_far_list_BY[sapply(my_genes_before_vs_far_list_BY,length)>0] # 49
input_csv = read.table("models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA.csv",sep=",",header=FALSE)
dirs_to_get = basename(dirname(names(my_genes_before_vs_far_list_BY_gt0)))
input_csv_for_lmer = input_csv[sapply(dirs_to_get, function(x) grep(x,input_csv[,1])),]
write.table(input_csv_for_lmer,file="models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA_for_lmer_test_data.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)

```

#For each metadata file get the number of samples averaged together

```{r}
input_file = read.csv("models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer.csv",header=FALSE)
metadata_train = input_file[,3]
new_metadata_file_names = sapply(metadata_train, function(metadatafile) {
  metadata_temp = read.csv(metadatafile)
  mapping_file = paste(strsplit(metadatafile,split="_proportion_train_")[[1]][1],".mapping.csv",sep="")
  mapping_df = read.csv(mapping_file)
  samples_per_subject = sapply(metadata_temp$SubjectID, function(subjectID) {
    sample_num_per_window = sum(mapping_df[,1]==subjectID)
    return(sample_num_per_window)
  })
  metadata_temp$samples_per_subject = samples_per_subject
  metadatafile_new = gsub("_metadata.csv","_metadata_for_lmer.csv",metadatafile)
  write.csv(metadata_temp,file=metadatafile_new)
  return(metadatafile_new)
})
input_file[,3] = new_metadata_file_names
write.table(input_file,file="models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)


# do for test
input_file = read.csv("models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA_for_lmer_test_data.csv",header=FALSE)
metadata_test = input_file[,4]
new_metadata_file_names = sapply(metadata_test, function(metadatafile) {
  metadata_temp = read.csv(metadatafile)
  mapping_file = paste(strsplit(metadatafile,split="_proportion_train_")[[1]][1],".mapping.csv",sep="")
  mapping_df = read.csv(mapping_file)
  samples_per_subject = sapply(metadata_temp$SubjectID, function(subjectID) {
    sample_num_per_window = sum(mapping_df[,1]==subjectID)
    return(sample_num_per_window)
  })
  metadata_temp$samples_per_subject = samples_per_subject
  metadatafile_new = gsub("_metadata.csv","_metadata_test_for_lmer.csv",metadatafile)
  write.csv(metadata_temp,file=metadatafile_new)
  return(metadatafile_new)
})
input_file[,4] = new_metadata_file_names
write.table(input_file,file="models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA_for_lmer_test_data.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)

```


#Now run lmer

```{bash}

grep "healthy_pre-IA2A-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66" models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer.csv > test.csv

# also only include models that have all HLA
grep "all_HLA" models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer.csv > models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA.csv

#while read line
#do
#  sbatch -n 1 -c 5 --mem=30G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_v3.bash ${line} 5
#done < test.csv


while read line
do
  sbatch -n 1 -c 5 --mem=25G -p short -t 0-06:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_v3.bash ${line} 5
done < models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA.csv

sacct -S 2023-07-30 | grep "run_paire" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do head -n 1 slurm-$line.out; done | while read line; do grep $line models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA.csv; done > "FAILED_jobs_to_redo_lmer2.txt"

while read line
do
  sbatch -n 1 -c 5 --mem=30G -p short -t 0-11:59 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_v3.bash ${line} 5
done < FAILED_jobs_to_redo_lmer2.txt

head -n 1 models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA_for_lmer_test_data.csv > blah.csv
# test data

while read line
do
  sbatch -n 1 -c 1 --mem=5G -p short -t 0-01:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_v3_test.bash ${line} 1 0.9 test
done < blah.csv


while read line
do
  sbatch -n 1 -c 5 --mem=25G -p short -t 0-06:00 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/run_paired_univariate_feature_selection_v3_test.bash ${line} 5 0.9 test
done < models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_all_HLA_for_lmer_test_data.csv

```

#Get genes significant after mixed effect model. also make input file for bootstrapping. First make input file


```{r}
my_genes = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern="output_lmer_feature_selection_results.rds",full.names = TRUE,recursive = TRUE)

my_genes_before_vs_far = my_genes[grep("before_vs_far_from_condition_subject_level",my_genes)]
my_genes_before_vs_far_list = lapply(my_genes_before_vs_far,function(x) readRDS(x))

my_genes_before_vs_far_list_sig = lapply(my_genes_before_vs_far_list, function(mydf) {
  mydf = mydf[mydf$singular == 0,]
  mydf = mydf[!is.na(mydf$pvalue),]
  mydf$BY = p.adjust(mydf$pvalue,method="BY")
  mydf = mydf[mydf$BY < 0.05,]
  mydf = mydf[order(mydf$pvalue),]
  return(mydf)
})

num_sig_genes_per_comparison = sapply(my_genes_before_vs_far_list_sig, nrow)
names(num_sig_genes_per_comparison) = basename(dirname(my_genes_before_vs_far))
num_sig_genes_per_comparison = sort(num_sig_genes_per_comparison)
num_sig_genes_per_comparison_gt0 = num_sig_genes_per_comparison[num_sig_genes_per_comparison>0]
mean(num_sig_genes_per_comparison_gt0)
median(num_sig_genes_per_comparison_gt0)


# get input file for only those that have sig genes
#input_csv = read.table("models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv",sep=",",header=FALSE)

dirs_to_get = names(num_sig_genes_per_comparison_gt0)

#input_csv_for_lmer = input_csv[sapply(dirs_to_get, function(x) grep(x,input_csv[,1])),]
#write.table(input_csv_for_lmer,file="models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_bootstrap.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)

#parsed_folder_dirs = list.dirs(path="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4")
#parsed_folder_dirs = parsed_folder_dirs[grep("prep_voe_input_files_before_vs_far_from_condition_subject_level",parsed_folder_dirs)]

setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4")

#abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern=".csv",full.names = TRUE)
#abundance_files = abundance_files[-grep("healthy",abundance_files)]
#abundance_files = abundance_files[-grep("total_genes_left_each_timepoint.csv",abundance_files)]


dirs_to_get_no_proportion = sapply(strsplit(dirs_to_get,split="_proportion_train_"), function(x) x[1])
dirs_to_get_no_proportion = unique(dirs_to_get_no_proportion)
dirs_to_get_no_proportion_mapping_files = paste(dirs_to_get_no_proportion,".mapping.csv",sep="")

mapping_files = list.files(pattern = ".mapping.csv")
mapping_files = mapping_files[grep("_before_vs_far_from_condition_subject_level",mapping_files)]
mapping_files = mapping_files[match(dirs_to_get_no_proportion_mapping_files,mapping_files)]

training_metadata_train_split=lapply(mapping_files, function(x) {
  prefix = gsub(".mapping.csv","",x)
  dirs_with_prefix = dirs_to_get[grep(prefix,dirs_to_get)]
  suffixes = sapply(strsplit(dirs_with_prefix,split="_"), function(y) y[length(y)])
  if("0.66"%in%suffixes) {
    training_metadata_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66_train_metadata.csv",x)
    training_data_files_two_thirds_train = gsub(".mapping.csv","_proportion_train_0.66.train_subjects.txt",x)
    training_files_two_thirds_train_all = c(x,training_metadata_files_two_thirds_train,training_data_files_two_thirds_train)
  } else {
    training_files_two_thirds_train_all = c(NA,NA,NA)
  }
  if("0.5"%in%suffixes) {
    training_metadata_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5_train_metadata.csv",x)
    training_data_files_fifty_train = gsub(".mapping.csv","_proportion_train_0.5.train_subjects.txt",x)
    training_files_fifty_train_all = c(x,training_metadata_files_fifty_train,training_data_files_fifty_train)

  } else {
    training_files_fifty_train_all = c(NA,NA,NA)
  }
  return(rbind(training_files_two_thirds_train_all,training_files_fifty_train_all))
})
training_metadata_train_split = do.call("rbind",training_metadata_train_split)
training_metadata_train_split = as.data.frame(training_metadata_train_split)
training_metadata_train_split = training_metadata_train_split[!is.na(training_metadata_train_split[,1]),]


write.table(training_metadata_train_split,file="before_vs_after_subject_level_bootstrapping_input_files.txt",sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)

```

#Get input for permutation 1 with test data

```{r}
get_permutation_input_with_cutoff_test = function(genetype,output,cutoff) {
  if(genetype=="species") {
    my_genes_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern=paste("_species",cutoff,test_or_train,"output_lmer_feature_selection_results_window_size_adjusted.rds",sep=""),full.names = TRUE,recursive = TRUE)
  } else if(genetype == "pathway") {
    my_genes_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern=paste("_pathway",cutoff,test_or_train,"output_lmer_feature_selection_results_window_size_adjusted.rds",sep=""),full.names = TRUE,recursive = TRUE)
  } else if(genetype == "gene") {
    my_genes_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern=paste(cutoff,"test","output_lmer_feature_selection_results_window_size_adjusted.rds",sep=""),full.names = TRUE,recursive = TRUE)
    my_genes_files = my_genes_files[-grep("pathway",my_genes_files)]
  }

  my_genes_before_vs_far = my_genes_files[grep("before_vs_far_from_condition_subject_level",my_genes_files)]
  my_genes_before_vs_far = my_genes_before_vs_far[grep("all_HLA",my_genes_before_vs_far)]
  my_genes_before_vs_far_list = lapply(my_genes_before_vs_far,function(x) readRDS(x))
  my_genes_before_vs_far_list_sig = lapply(1:length(my_genes_before_vs_far_list), function(ind) {
    mydf = my_genes_before_vs_far_list[[ind]]
    mydf = mydf[mydf$singular == 0,]
    mydf = mydf[!is.na(mydf$pvalue),]
    mydf$BY = p.adjust(mydf$pvalue,method="BY")
    mydf = mydf[mydf$BY < 0.05,]
    mydf = mydf[order(mydf$pvalue),]
    mapping_files = gsub("_proportion_train_0.66","",basename(dirname(my_genes_before_vs_far[[ind]])))
    mapping_files = gsub("_proportion_train_0.5","",mapping_files)
    mapping_files = paste(mapping_files,".mapping.csv",sep="")
    train_metadata = paste(basename(dirname(my_genes_before_vs_far[[ind]])),"_test_metadata.csv",sep="")
    train_subjects = paste(basename(dirname(my_genes_before_vs_far[[ind]])),".test_subjects.txt",sep="")

    if(length(rownames(mydf))>0) {
      sig_gene_names_list = split(rownames(mydf), ceiling(seq_along(rownames(mydf))/50000)) 
      outputname_list = rep("",length(sig_gene_names_list))
      num_sig_genes_list = rep(0,length(sig_gene_names_list))
      for (ind2 in 1:length(sig_gene_names_list)) {
        sig_genes_slice = sig_gene_names_list[[ind2]]
        outputname = gsub(".rds",paste("bootstrap_one_sig_genes_",ind2,".txt",sep=""),my_genes_before_vs_far[ind])
        outputname_list[ind2] = outputname
        num_sig_genes_list[ind2] = length(sig_genes_slice)
        write.table(sig_genes_slice,file=outputname,col.names=FALSE,row.names=FALSE,quote=FALSE) 
      }
      end_df = data.frame(rep(mapping_files,length(sig_gene_names_list)),rep(train_metadata,length(sig_gene_names_list)),rep(train_subjects,length(sig_gene_names_list)),outputname_list,num_sig_genes_list)
    } else {
      end_df = data.frame(mapping_files,train_metadata,train_subjects,NA,0)
    }
    colnames(end_df) = c("mapping_files","train_metadata","train_subjects","outputname","sig_gene_number")
    return(end_df)
  })
  my_genes_before_vs_far_list_sig = do.call("rbind",my_genes_before_vs_far_list_sig)
  my_genes_before_vs_far_list_sig = as.data.frame(my_genes_before_vs_far_list_sig)
  my_genes_before_vs_far_list_sig[,5] = as.numeric(my_genes_before_vs_far_list_sig[,5])
  my_genes_before_vs_far_list_sig = my_genes_before_vs_far_list_sig[my_genes_before_vs_far_list_sig[,5]>0,]
  my_genes_before_vs_far_list_sig = my_genes_before_vs_far_list_sig[,-5]
  write.table(my_genes_before_vs_far_list_sig,file=output,sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
  return(my_genes_before_vs_far_list_sig)
}

blah = get_permutation_input_with_cutoff_test(genetype="gene",output="before_vs_after_subject_level_bootstrapping_input_files_window_size_boot_gene_cutoff_0.9.test.txt",cutoff=0.9) 

```



#Get sig genes when doing lmer with window size adjusted

```{r}
library(data.table)
prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=TRUE)
setkey(prokka_annotations,V1)

my_genes = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern="output_lmer_feature_selection_results_window_size_adjusted.rds",full.names = TRUE,recursive = TRUE)

my_genes_before_vs_far = my_genes[grep("before_vs_far_from_condition_subject_level",my_genes)]
my_genes_before_vs_far = my_genes_before_vs_far[grep("all_HLA",my_genes_before_vs_far)]
my_genes_before_vs_far_list = lapply(my_genes_before_vs_far,function(x) readRDS(x))

my_genes_before_vs_far_list_sig = lapply(my_genes_before_vs_far_list, function(mydf) {
  mydf = mydf[mydf$singular == 0,]
  mydf = mydf[!is.na(mydf$pvalue),]
  mydf$BY = p.adjust(mydf$pvalue,method="BY")
  mydf = mydf[mydf$BY < 0.05,]
  mydf = mydf[order(mydf$pvalue),]
  prokka_annotations_temp = prokka_annotations[rownames(mydf)]
  mydf = cbind(mydf,prokka_annotations_temp)
  return(mydf)
})
names(my_genes_before_vs_far_list_sig) = my_genes_before_vs_far
my_genes_before_vs_far_list_sig = my_genes_before_vs_far_list_sig[sapply(my_genes_before_vs_far_list_sig, nrow)>0]

mapping_files = gsub("_proportion_train_0.66","",basename(dirname(names(my_genes_before_vs_far_list_sig))))
mapping_files = gsub("_proportion_train_0.5","",mapping_files)
mapping_files = paste(mapping_files,".mapping.csv",sep="")

train_metadata = paste(basename(dirname(names(my_genes_before_vs_far_list_sig))),"_train_metadata.csv",sep="")

train_subjects = paste(basename(dirname(names(my_genes_before_vs_far_list_sig))),".train_subjects.txt",sep="")

sig_file_for_bootsrap2 = data.frame(mapping_files,train_metadata,train_subjects)
write.table(sig_file_for_bootsrap2,file="before_vs_after_subject_level_bootstrapping_input_files_2.txt",sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)






my_genes_before_vs_far_list_sig = my_genes_before_vs_far_list_sig[order(sapply(my_genes_before_vs_far_list_sig, nrow),decreasing = TRUE)]

hello = table(unlist(lapply(my_genes_before_vs_far_list_sig,function(x) unique(x$V7))))
hello = hello[order(hello,decreasing = TRUE)]
hello = data.frame(hello)
hello$prop = hello$Freq / length(my_genes_before_vs_far_list_sig)
write.csv(hello,"top_hits.csv")

num_sig_genes_per_comparison = sapply(my_genes_before_vs_far_list_sig, nrow)
names(num_sig_genes_per_comparison) = basename(dirname(my_genes_before_vs_far))
num_sig_genes_per_comparison = sort(num_sig_genes_per_comparison)
num_sig_genes_per_comparison_gt0 = num_sig_genes_per_comparison[num_sig_genes_per_comparison>0]

mean(num_sig_genes_per_comparison_gt0)
median(num_sig_genes_per_comparison_gt0)

```


#Create HDF5file

```{bash}
#cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

#ls /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/*.csv | grep -v "healthy" | grep -v "total_genes_left_each_timepoint.csv" > #/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/abundance_file_list.txt
#
#sbatch -n 1 -c 1 --mem=40G -p medium -t 1-00:00 scripts/writeHDF5file.bash #/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/abundance_file_list.txt hdf5file.h5 13159 1
#
#```

# try using bigmemory. First concatenate all files

#```{bash}
#cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis
#sbatch -n 1 -c 1 --mem=5G -p short -t 0-11:00 create_giant_normalized_abundance_mat.bash

#sbatch -n 1 -c 1 --mem=40G -p medium -t 1-00:00 scripts/create_bigmemory_files.bash


```

```{r}
my_genes = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern="output_lmer_feature_selection_results_window_size_adjusted.rds",full.names = TRUE,recursive = TRUE)

my_genes_before_vs_far = my_genes[grep("before_vs_far_from_condition_subject_level",my_genes)]
my_genes_before_vs_far = my_genes_before_vs_far[grep("all_HLA",my_genes_before_vs_far)]
my_genes_before_vs_far_list = lapply(my_genes_before_vs_far,function(x) readRDS(x))
names(my_genes_before_vs_far_list) = my_genes_before_vs_far

my_genes_before_vs_far_list_sig = lapply(1:length(my_genes_before_vs_far_list), function(counter) {
  mydf = my_genes_before_vs_far_list[[counter]]
  input_temp_name = names(my_genes_before_vs_far_list)[counter]
  mydf = mydf[mydf$singular == 0,]
  mydf = mydf[!is.na(mydf$pvalue),]
  mydf$BY = p.adjust(mydf$pvalue,method="BY")
  mydf = mydf[mydf$BY < 0.05,]
  mydf = mydf[order(mydf$pvalue),]
  sig_gene_names = rownames(mydf)
  if(length(sig_gene_names)>0) {
    sig_gene_names_list = split(sig_gene_names, ceiling(seq_along(sig_gene_names)/50000)) 
    for (ind in 1:length(sig_gene_names_list)) {
      sig_genes_slice = sig_gene_names_list[[ind]]
      outputname = gsub(".rds",paste("_",ind,".txt",sep=""),input_temp_name)
      write.table(sig_genes_slice,file=outputname,quote=FALSE,col.names=FALSE,row.names=FALSE)
    }
  }
  return(mydf)
})

```


```{bash}

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4

# lets do test first

head -n 1 before_vs_after_subject_level_bootstrapping_input_files_window_size_boot_gene_cutoff_0.9.test.txt > test.txt

while read line
do
  sbatch -c 5 -t 0-11:59 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps_gene_cutoff_train_or_test.bash "${line}" 5 0.9 test
done < test.txt

while read line
do
  sbatch -c 5 -t 0-06:00 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps_gene_cutoff_train_or_test.bash "${line}" 5 0.9 test
done < before_vs_after_subject_level_bootstrapping_input_files_window_size_boot_gene_cutoff_0.9.test.txt

sacct -S 2023-08-27 | grep -A 100000 "16034325" | grep "prep_abun" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do head -n 5 slurm-${line}.out | tail -n 1; done | while read line; do grep $line before_vs_after_subject_level_bootstrapping_input_files_window_size_boot_gene_cutoff_0.9.test.txt; done > failed_test_0.9.txt

while read line
do
  sbatch -c 5 -t 0-11:59 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps_gene_cutoff_train_or_test.bash "${line}" 5 0.9 test
done < failed_test_0.9.txt


grep "healthy_pre-MIAA-2month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66" before_vs_after_subject_level_bootstrapping_input_files.txt > test.txt 

grep "all_HLA" before_vs_after_subject_level_bootstrapping_input_files.txt > before_vs_after_subject_level_bootstrapping_input_files_all_HLA_only.txt
#while read line
#do
#  mapping_data=$(echo ${line} | awk '{print $1}')
#  train_metadata=$(echo ${line} | awk '{print $2}')
#  training_subjects=$(echo ${line} | awk '{print $3}')
#  sbatch -c 5 -t 0-11:59 -p short --mem=100G prep_abundance_for_voe_bulk_training_data_v2_create_boostraps.bash ${mapping_data} ${train_metadata} ${training_subjects} 5 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/hdf5file.h5 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/hdf5_rownames.rds /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/hdf5_colnames.rds
#done < test.txt


while read line
do
  mapping_data=$(echo ${line} | awk '{print $1}')
  train_metadata=$(echo ${line} | awk '{print $2}')
  training_subjects=$(echo ${line} | awk '{print $3}')
  top_folder=${train_metadata%_train_metadata.csv}
  lmer_output_file=$(echo ${top_folder}"output_lmer_feature_selection_results_window_size_adjusted_")
  lmer_output_file2=$(echo ${top_folder}"/"${lmer_output_file})
  for x in ${lmer_output_file2}*.txt
  do 
  if [[ -f "$x" ]]
  then
    sbatch -c 5 -t 0-06:00 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps.bash ${mapping_data} ${train_metadata} ${training_subjects} 5 ${x}
  fi
  done
done < before_vs_after_subject_level_bootstrapping_input_files_2.txt

# redo timed out jobs

sacct -S 2023-08-06 | grep "prep_abun" | grep -A 10000 14427501 | grep -v "COMPLETED" | wc -l

```

#Get timed out jobs

```{r}
#mydf = read.table("before_vs_after_subject_level_bootstrapping_input_files_all_HLA_only.txt",header=FALSE,sep="\t")
mydf = read.table("before_vs_after_subject_level_bootstrapping_input_files_2.txt",header=FALSE,sep="\t")
output_folders = sapply(mydf[,2], function(x) gsub("_train_metadata.csv","",x))

to_redo_list = lapply(output_folders, function(x) {
  file_temp = list.files(x,pattern="output_lmer_feature_selection_results_window_size_adjusted.rds",full.names = TRUE)
  if(grepl("_proportion_train_0.66",dirname(file_temp))) {
    mapping_file = paste(gsub("_proportion_train_0.66","",dirname(file_temp)),".mapping.csv",sep="") 
  } else if (grepl("_proportion_train_0.5",dirname(file_temp))) {
    mapping_file = paste(gsub("_proportion_train_0.5","",dirname(file_temp)),".mapping.csv",sep="") 
  }
  train_metadata = paste(dirname(file_temp),"_train_metadata.csv",sep="")
  train_subjects = paste(dirname(file_temp),".train_subjects.txt",sep="")
  
  rds = readRDS(file_temp)
  rds = rds[rds$singular == 0,]
  rds = rds[!is.na(rds$pvalue),]
  rds$BY = p.adjust(rds$pvalue,method="BY")
  rds = rds[rds$BY < 0.05,]
  sig_gene_names = rownames(rds)
  if(length(sig_gene_names)>0) {
    sig_gene_names_list = split(sig_gene_names, ceiling(seq_along(sig_gene_names)/50000)) 
    
    output_files_to_write = sapply(1:length(sig_gene_names_list), function(ind) {
      sig_genes_slice = sig_gene_names_list[[ind]]
      outputname_orig = gsub(".rds",paste("_",ind,".txt",sep=""),file_temp)
      main_folder_name = x
      lmer_suffix_name = strsplit(outputname_orig,split="_")[[1]]
      lmer_suffix_name = lmer_suffix_name[length(lmer_suffix_name)]
      lmer_suffix_name = gsub(".txt","",lmer_suffix_name)
      main_folder_name_with_suffix = paste(x,"bootstrap_ttest_results",lmer_suffix_name,".rds",sep="")
      outputname = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",main_folder_name,"/",main_folder_name_with_suffix,sep="")
      if(!file.exists(outputname)) {
        return(outputname_orig)
      } else {
        return(NA)
      }
    })
    output_df = data.frame(rep(mapping_file,length(output_files_to_write)),
               rep(train_metadata,length(output_files_to_write)),
               rep(train_subjects,length(output_files_to_write)),output_files_to_write)
    output_df = output_df[!is.na(output_df[,4]),]
    return(output_df)
  } else {
    return(data.frame())
  }
})

to_redo_df = do.call("rbind",to_redo_list)
write.table(to_redo_df,file="to_redo_df_bootstrapping.tsv",sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)
```

#Redo timed out jobs
```{bash}

while read line
do
  mapping_data=$(echo ${line} | awk '{print $1}')
  train_metadata=$(echo ${line} | awk '{print $2}')
  training_subjects=$(echo ${line} | awk '{print $3}')
  lmer_sig_genes=$(echo ${line} | awk '{print $4}')
  sbatch -c 5 -t 0-11:59 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps.bash ${mapping_data} ${train_metadata} ${training_subjects} 5 ${lmer_sig_genes}
done < to_redo_df_bootstrapping.tsv

```



#organize RDS output

```{r}

initial_input = read.table("before_vs_after_subject_level_bootstrapping_input_files_2.txt",sep="\t",header=FALSE)
initial_input_folders = gsub("_train_metadata.csv","",initial_input[,2])
initial_input_folders = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",initial_input_folders,sep="/")
all_sig_gene_list = lapply(initial_input_folders, function(myfolder) {
  out_files = list.files(myfolder,pattern="bootstrap_ttest_results",full.names = TRUE)
  out_files = out_files[grep(".rds",out_files)]
  if(length(out_files) >0) {
    still_sig_gene_names = lapply(out_files, function(myoutfile) {
      myrds = readRDS(myoutfile)
      myrds_pvals = do.call("cbind",lapply(myrds, function(mydf) mydf[,1]))
      proportion_sig = rowSums(myrds_pvals<0.05)/ncol(myrds_pvals)
      proportion_still_sig = proportion_sig[proportion_sig>0.95]
      return(names(proportion_still_sig))
    })
    still_sig_gene_names = unlist(still_sig_gene_names)
    outname = paste(myfolder,"/",basename(myfolder),"bootstrap_ttest_results_all_genes.txt",sep="")
    write.table(still_sig_gene_names,file=outname,col.names=FALSE,row.names=FALSE,quote=FALSE)
    return(still_sig_gene_names)
  } else {
    return(NA)
  }
})
names(all_sig_gene_list) = initial_input_folders

sig_gene_folders = names(all_sig_gene_list)[sapply(all_sig_gene_list,length)>0]

# now also get lasso regression input files

lasso_reg_orig_input = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA.csv",sep=",",header=FALSE)

lasso_reg_orig_input_sig = lasso_reg_orig_input[sapply(basename(sig_gene_folders), function(x) grep(x,lasso_reg_orig_input[,1])),]

write.table(lasso_reg_orig_input_sig,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA_bootstrap_sig.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)
```

# I realized I want to do another perumutation test where instead of selecting the same amount of samples to average, I select samples within a random window

#First create input file for next round of bootstrapping

```{r}
library(data.table)

my_genes_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern="bootstrap_ttest_results_all_genes.txt",full.names = TRUE,recursive = TRUE)

mapping_files = gsub("_proportion_train_0.66","",basename(dirname(my_genes_files)))
mapping_files = gsub("_proportion_train_0.5","",mapping_files)
mapping_files = paste(mapping_files,".mapping.csv",sep="")

train_metadata = paste(basename(dirname(my_genes_files)),"_train_metadata.csv",sep="")

train_subjects = paste(basename(dirname(my_genes_files)),".train_subjects.txt",sep="")

sig_file_for_bootsrap3 = data.frame(mapping_files,train_metadata,train_subjects)
write.table(sig_file_for_bootsrap3,file="before_vs_after_subject_level_bootstrapping_input_files_3_window_size_boot.txt",sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)

# now split into groups of 50,000 
split_genes = lapply(my_genes_files, function(x) {
  sig_gene_names = read.table(x,header=FALSE)[,1]
  sig_gene_names_list = split(sig_gene_names, ceiling(seq_along(sig_gene_names)/50000)) 
  for (ind in 1:length(sig_gene_names_list)) {
    sig_genes_slice = sig_gene_names_list[[ind]]
    outputname = gsub(".txt",paste("_",ind,".txt",sep=""),x)
    print(outputname)
    write.table(sig_genes_slice,file=outputname,quote=FALSE,col.names=FALSE,row.names=FALSE)
  }
  return(sig_gene_names_list)
})


# now do test data

library(data.table)

bootstrap1_input_file = read.table("before_vs_after_subject_level_bootstrapping_input_files_window_size_boot_gene_cutoff_0.9.test.txt",sep="\t",header=FALSE)
unique_boostrap_input_files = unique(bootstrap1_input_file[,-4])
new_fun_input_list = apply(unique_boostrap_input_files, 1,function(myrow) {
  mapping_file = myrow[1]
  metadata_file = myrow[2]
  subjects_file = myrow[3]
  myfolder = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",gsub(".test_subjects.txt","",subjects_file),sep="")
  my_genes_files = list.files(myfolder,pattern="0.9testbootstrap_ttest_results_round1",full.names = TRUE,recursive = TRUE)
  all_sig_genes = lapply(my_genes_files, function(myrds) {
    myrds_data = readRDS(myrds) 
    myrds_pvals = do.call("cbind",lapply(myrds_data, function(mydf) mydf[,1]))
    rownames(myrds_pvals) = rownames(myrds_data[[1]])
    proportion_sig = rowSums(myrds_pvals<0.05,na.rm=TRUE)/ncol(myrds_pvals)
    proportion_still_sig = proportion_sig[proportion_sig>0.95]
    return(names(proportion_still_sig))
  })
  all_sig_genes = unlist(all_sig_genes)
  if(length(all_sig_genes)>0) {
    sig_gene_names_list = split(all_sig_genes, ceiling(seq_along(all_sig_genes)/50000)) 
    output_files_to_write = sapply(1:length(sig_gene_names_list), function(ind) {
      sig_genes_slice = sig_gene_names_list[[ind]]
      new_output_name = paste(myfolder,"/",basename(myfolder),"0.9test_bootstrap_ttest_results_round1_sig_names",ind,".txt",sep="")
      write.table(sig_genes_slice,file=new_output_name,col.names=FALSE,row.names=FALSE,quote=FALSE)
      return(new_output_name)
    })
  } else {
    return(NA)
  }
  end_df = data.frame(rep(mapping_file,length(sig_gene_names_list)),rep(metadata_file,length(sig_gene_names_list)),rep(subjects_file,length(sig_gene_names_list)),output_files_to_write)
  return(end_df)
})
new_fun_input__df = do.call("rbind",new_fun_input_list)

write.table(new_fun_input__df,file="before_vs_after_subject_level_bootstrapping_2_input_files_window_size_boot_gene_cutoff_0.9.test.txt",sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)


```




```{bash}
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4

# do test data first

head -n 1 before_vs_after_subject_level_bootstrapping_2_input_files_window_size_boot_gene_cutoff_0.9.test.txt > test.txt

while read line
do
  sbatch -c 5 -t 0-06:00 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps_round_2_gene_cutoff_train_or_test.bash "${line}" 5 0.9 test
done < before_vs_after_subject_level_bootstrapping_2_input_files_window_size_boot_gene_cutoff_0.9.test.txt

sacct -S 2023-08-28 | grep -A 100000 "16251538" | grep "prep_abun" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do head -n 5 slurm-${line}.out | tail -n 1; done | while read line; do grep $line before_vs_after_subject_level_bootstrapping_2_input_files_window_size_boot_gene_cutoff_0.9.test.txt; done > failed_test_0.9_2.txt

while read line
do
  sbatch -c 5 -t 0-11:59 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps_round_2_gene_cutoff_train_or_test.bash "${line}" 5 0.9 test
done < failed_test_0.9_2.txt



#head -n 1 /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_subject_level_bootstrapping_input_files_3_window_size_boot.txt > test.txt


while read line
do
  mapping_data=$(echo ${line} | awk '{print $1}')
  train_metadata=$(echo ${line} | awk '{print $2}')
  training_subjects=$(echo ${line} | awk '{print $3}')
  top_folder=${train_metadata%_train_metadata.csv}
  lmer_output_file=$(echo ${top_folder}"bootstrap_ttest_results_all_genes_")
  lmer_output_file2=$(echo ${top_folder}"/"${lmer_output_file})
  for x in ${lmer_output_file2}*.txt
  do 
  if [[ -f "$x" ]]
  then
    echo $x
    sbatch -c 5 -t 0-06:00 -p short --mem=75G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps_window_size.bash ${mapping_data} ${train_metadata} ${training_subjects} 5 ${x}
  fi
  done
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_subject_level_bootstrapping_input_files_3_window_size_boot.txt

```

#Find guys that failed or timed out

```{r}
library(data.table)

my_genes_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/",pattern="bootstrap_ttest_results_all_genes.txt",full.names = TRUE,recursive = TRUE)

# now split into groups of 50,000 
output_df_list = lapply(my_genes_files, function(x) {
  mapping_files = gsub("_proportion_train_0.66","",basename(dirname(x)))
  mapping_files = gsub("_proportion_train_0.5","",mapping_files)
  mapping_files = paste(mapping_files,".mapping.csv",sep="")
  train_metadata = paste(basename(dirname(x)),"_train_metadata.csv",sep="")
  train_subjects = paste(basename(dirname(x)),".train_subjects.txt",sep="")
  
  sig_gene_names = read.table(x,header=FALSE)[,1]
  sig_gene_names_list = split(sig_gene_names, ceiling(seq_along(sig_gene_names)/50000)) 
  output_files_to_write = sapply(1:length(sig_gene_names_list), function(ind) {
    sig_genes_slice = sig_gene_names_list[[ind]]
    outputname = gsub(".txt",paste("_",ind,".txt",sep=""),x)
    new_output_name = gsub("bootstrap_ttest_results_all_genes.txt",paste("bootstrap_ttest_results_window_size_adjust",ind,".rds",sep=""),x)
    print(new_output_name)
    if(!file.exists(new_output_name)) {
        return(outputname)
    } else {
        return(NA)
    }
  })
  output_df = data.frame(rep(mapping_files,length(output_files_to_write)),
            rep(train_metadata,length(output_files_to_write)),
            rep(train_subjects,length(output_files_to_write)),output_files_to_write)
  output_df = output_df[!is.na(output_df[,4]),]
  return(output_df)
})
output_df_list = do.call("rbind",output_df_list)
write.table(output_df_list,file="to_redo_df_bootstrapping_windo_size.tsv",sep="\t",col.names=FALSE,row.names=FALSE,quote=FALSE)

```

```{bash}
while read line
do
  mapping_data=$(echo ${line} | awk '{print $1}')
  train_metadata=$(echo ${line} | awk '{print $2}')
  training_subjects=$(echo ${line} | awk '{print $3}')
  lmer_output_file=$(echo ${line} | awk '{print $4}')
  sbatch -c 5 -t 0-11:59 -p short --mem=100G prep_abundance_for_voe_bulk_training_data_v3_create_boostraps_window_size.bash ${mapping_data} ${train_metadata} ${training_subjects} 5 ${lmer_output_file}
done < to_redo_df_bootstrapping_windo_size.tsv

```

#Ok one last bootstrap. I take genes that were sig in train and test sets and see if they are sig diff in random control samples as a negative control. Hopefully they are not! 

```{r}
library(data.table)
library(RNOmni)

# first get sig train genes

initial_input = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_subject_level_bootstrapping_input_files_3_window_size_boot.txt",sep="\t",header=FALSE)
initial_input_folders = gsub("_train_metadata.csv","",initial_input[,2])
initial_input_folders = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",initial_input_folders,sep="/")
all_sig_gene_list_train = lapply(initial_input_folders, function(myfolder) {
  out_files = list.files(myfolder,pattern="bootstrap_ttest_results_window_size_adjust",full.names = TRUE)
  out_files = out_files[grep(".rds",out_files)]
  if(length(out_files) >0) {
    still_sig_gene_names = lapply(out_files, function(myoutfile) {
      myrds = readRDS(myoutfile)
      myrds_pvals = do.call("cbind",lapply(myrds, function(mydf) mydf[,1]))
      proportion_sig = rowSums(myrds_pvals<0.05)/ncol(myrds_pvals)
      proportion_still_sig = proportion_sig[proportion_sig>0.95]
      return(names(proportion_still_sig))
    })
    still_sig_gene_names = unlist(still_sig_gene_names)
    return(still_sig_gene_names)
  } else {
    return(NA)
  }
})
names(all_sig_gene_list_train) = initial_input_folders
# get test sig genes

initial_test_input = read.table("before_vs_after_subject_level_bootstrapping_2_input_files_window_size_boot_gene_cutoff_0.9.test.txt",sep="\t",header=FALSE)
initial_test_input = gsub("_test_metadata.csv","",initial_test_input[,2])
initial_test_input = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",initial_test_input,sep="/")

all_sig_gene_list_test = lapply(initial_test_input, function(myfolder) {
  out_files = list.files(myfolder,pattern=".9testbootstrap_ttest_results_round2",full.names = TRUE)
  out_files = out_files[grep(".rds",out_files)]
  if(length(out_files) >0) {
    still_sig_gene_names = lapply(out_files, function(myoutfile) {
      myrds = readRDS(myoutfile)
      myrds_pvals = do.call("cbind",lapply(myrds, function(mydf) mydf[,1]))
      proportion_sig = rowSums(myrds_pvals<0.05)/ncol(myrds_pvals)
      proportion_still_sig = proportion_sig[proportion_sig>0.95]
      return(names(proportion_still_sig))
    })
    still_sig_gene_names = unlist(still_sig_gene_names)
    return(still_sig_gene_names)
  } else {
    return(NA)
  }
})
names(all_sig_gene_list_test) = initial_test_input

all_folder_names = unique(c(initial_test_input,initial_input_folders))

train_and_test_sig_genes = lapply(all_folder_names, function(x) {
  test_sig_genes = all_sig_gene_list_test[[x]]
  train_sig_genes = all_sig_gene_list_train[[x]]
  test_train_sig_genes = intersect(test_sig_genes,train_sig_genes)
  return(test_train_sig_genes)
})
names(train_and_test_sig_genes) = all_folder_names

output_names = lapply(1:length(train_and_test_sig_genes), function(x) {
  folder_name = names(train_and_test_sig_genes)[x]
  genes = train_and_test_sig_genes[[x]]
  
  if(length(genes)>0) {
    sig_gene_names_list = split(genes, ceiling(seq_along(genes)/50000)) 
    output_files_to_write = sapply(1:length(sig_gene_names_list), function(ind) {
      sig_genes_slice = sig_gene_names_list[[ind]]
      new_output_name = paste(folder_name,"/",basename(folder_name),"0.9test_bootstrap_ttest_results_round2_sig_names",ind,".txt",sep="")
      write.table(sig_genes_slice,file=new_output_name,col.names=FALSE,row.names=FALSE,quote=FALSE)
      return(new_output_name)
    })
  } else {
    return(NA)
  }
})
output_names = unlist(output_names)
output_names = output_names[!is.na(output_names)]
write.table(output_names,file="bootstrap_negative_control_input.txt",col.names=FALSE,row.names=FALSE,quote=FALSE)
```

```{bash}
while read line
do
  sbatch -c 5 -t 0-06:00 -p short --mem=75G bootstrap_negative_control.bash ${line} 5 0.9 test
done < bootstrap_negative_control_input.txt

sacct | grep "bootstrap" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep "0.9test_bootstrap_ttest_results_round2_sig_name" slurm-$line.out ; done | sed 's/[^ ]* //' | tr -d '"' | while read line; do grep $line bootstrap_negative_control_input.txt; done > failed_test_neg_ctrl_0.9_2.txt

while read line
do
  sbatch -c 1 -t 0-01:00 -p short --mem=30G bootstrap_negative_control.bash ${line} 1 0.9 test
done < failed_test_neg_ctrl_0.9_2.txt

```

```{r}
# Now lets see if any survived

output_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",pattern="negative_control_ttest_results_",full.names=TRUE,recursive = TRUE)

all_kept_genes = lapply(output_files, function(x) {
  myvec = readRDS(x)
  kept_genes = myvec[myvec<0.05]
  print(min(myvec))
  return(names(kept_genes))
})
```



#Now get significant genes

```{r}
initial_input = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_subject_level_bootstrapping_input_files_3_window_size_boot.txt",sep="\t",header=FALSE)
initial_input_folders = gsub("_train_metadata.csv","",initial_input[,2])
initial_input_folders = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4",initial_input_folders,sep="/")
all_sig_gene_list = lapply(initial_input_folders, function(myfolder) {
  out_files = list.files(myfolder,pattern="bootstrap_ttest_results_window_size_adjust",full.names = TRUE)
  out_files = out_files[grep(".rds",out_files)]
  if(length(out_files) >0) {
    still_sig_gene_names = lapply(out_files, function(myoutfile) {
      myrds = readRDS(myoutfile)
      myrds_pvals = do.call("cbind",lapply(myrds, function(mydf) mydf[,1]))
      proportion_sig = rowSums(myrds_pvals<0.05)/ncol(myrds_pvals)
      proportion_still_sig = proportion_sig[proportion_sig>0.95]
      return(names(proportion_still_sig))
    })
    still_sig_gene_names = unlist(still_sig_gene_names)
    outname = paste(myfolder,"/",basename(myfolder),"bootstrap_ttest_results_all_genes_window_size_adjusted.txt",sep="")
    write.table(still_sig_gene_names,file=outname,col.names=FALSE,row.names=FALSE,quote=FALSE)
    return(still_sig_gene_names)
  } else {
    return(NA)
  }
})
names(all_sig_gene_list) = initial_input_folders

sig_gene_folders = names(all_sig_gene_list)[sapply(all_sig_gene_list,length)>0]

# now also get lasso regression input files

lasso_reg_orig_input = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA.csv",sep=",",header=FALSE)

lasso_reg_orig_input_sig = lasso_reg_orig_input[sapply(basename(sig_gene_folders), function(x) grep(x,lasso_reg_orig_input[,1])),]

write.table(lasso_reg_orig_input_sig,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA_bootstrap_sig.csv",sep=",",col.names=FALSE,row.names=FALSE,quote=FALSE)

```








#Do lasso regression

```{bash}
#with bootstrapped significant genes only

while read line
do
sbatch -n 1 -c 1 --mem=25G -p short -t 0-01:00 scripts/run_lasso_binary_all_clinical_bootstrap_sig.bash ${line} microbiome NA ttest_sig 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA_bootstrap_sig.csv



while read line
do
sbatch -n 1 -c 1 --mem=25G -p short -t 0-01:00 scripts/run_lasso_binary_all_clinical.bash ${line} microbiome NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

# redo failed jobs

sacct  | grep "run_lasso" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv; done > failed_jobs_parsed_v4_before_vs_far_from_condition_subject_level.txt

while read line
do
sbatch -n 1 -c 1 --mem=25G -p short -t 0-01:00 scripts/run_lasso_binary_all_clinical.bash ${line} microbiome NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/failed_jobs_parsed_v4_before_vs_far_from_condition_subject_level.txt

# get out of memory jobs

sacct  | grep "run_lasso" | grep -A 1000 "12679014" | grep -v "COMPLETED" | awk '{print $1}' | while read line; do grep -m 1 "filtered_transformed_abundance_test.csv" slurm-${line}.out ; done | while read line; do grep ${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/failed_jobs_parsed_v4_before_vs_far_from_condition_subject_level.txt; done > outofmem_jobs_parsed_v4_before_vs_far_from_condition_subject_level.txt

while read line
do
sbatch -n 1 -c 1 --mem=35G -p short -t 0-01:00 scripts/run_lasso_binary_all_clinical.bash ${line} microbiome NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/outofmem_jobs_parsed_v4_before_vs_far_from_condition_subject_level.txt



while read line
do
sbatch -n 1 -c 1 --mem=20G -p short -t 0-00:30 scripts/run_lasso_binary_all_clinical.bash ${line} time NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

while read line
do
sbatch -n 1 -c 1 --mem=20G -p short -t 0-00:30 scripts/run_lasso_binary_all_clinical.bash ${line} Maternal_BMI,Maternal_WeightGain_Pregnancy,Birth_Weight,Gestational_Age,fdr,apgar_score,time_to_brstfed_stop,age_at_solid_start,age_at_cow_milk_start,age_at_gluten_start,age_at_cereals_start,age_at_meats_start,age_at_vegetables_start,age_at_fruits_start,time_to_abx,time_since_abx,time,Maternal_PreEclampsia_Toxemia,Maternal_Weight_Gain_Aagaard,Sex,Maternal_Medication,Preterm,brst_fed,Maternal_Antibiotics,Geographical_Location,delivery_simple,HLA_Category,Maternal_Diabetes,Maternal_BMI_Category,Maternal_Diabetes_Medication,Insulin,Metformin,Glyburide,antihypertensives,time NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

while read line
do
sbatch -n 1 -c 1 --mem=40G -p short -t 0-01:00 scripts/run_lasso_binary_all_clinical.bash ${line} Maternal_BMI,Maternal_WeightGain_Pregnancy,Birth_Weight,Gestational_Age,fdr,apgar_score,time_to_brstfed_stop,age_at_solid_start,age_at_cow_milk_start,age_at_gluten_start,age_at_cereals_start,age_at_meats_start,age_at_vegetables_start,age_at_fruits_start,time_to_abx,time_since_abx,time,Maternal_PreEclampsia_Toxemia,Maternal_Weight_Gain_Aagaard,Sex,Maternal_Medication,Preterm,brst_fed,Maternal_Antibiotics,Geographical_Location,delivery_simple,HLA_Category,Maternal_Diabetes,Maternal_BMI_Category,Maternal_Diabetes_Medication,Insulin,Metformin,Glyburide,antihypertensives,time,microbiome NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

while read line
do
sbatch -n 1 -c 1 --mem=50G -p short -t 0-00:30 scripts/run_lasso_binary_all_clinical.bash ${line} time,microbiome NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

while read line
do
sbatch -n 1 -c 1 --mem=40G -p short -t 0-01:00 scripts/run_lasso_binary_all_clinical.bash ${line} Maternal_BMI,Maternal_WeightGain_Pregnancy,Birth_Weight,Gestational_Age,fdr,apgar_score,time_to_brstfed_stop,age_at_solid_start,age_at_cow_milk_start,age_at_gluten_start,age_at_cereals_start,age_at_meats_start,age_at_vegetables_start,age_at_fruits_start,time_to_abx,time_since_abx,Maternal_PreEclampsia_Toxemia,Maternal_Weight_Gain_Aagaard,Sex,Maternal_Medication,Preterm,brst_fed,Maternal_Antibiotics,Geographical_Location,delivery_simple,HLA_Category,Maternal_Diabetes,Maternal_BMI_Category,Maternal_Diabetes_Medication,Insulin,Metformin,Glyburide,antihypertensives NA all 1 NA
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

```

#Get results

```{r}
library(data.table)
library(glmnet)
input = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header_for_lmer_all_HLA_bootstrap_sig.csv")
folders = dirname(input[,1])

microbiome_data = unlist(lapply(folders, function(x) list.files(x,pattern="output_lasso_logistic_regression_NA_loss_microbiome_selection_method_ttest_sig_feature_list_microbiome.rds",full.names = TRUE)))
# remove weighted data for now
#input_data = input_data[-grep("weighted",input_data)]
#microbiome_data = input_data[grep("_feature_list_microbiome",input_data)]

#microbiome_data = microbiome_data[sapply(c("healthy_pre-GAD-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66","healthy_pre-IA2A-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66","healthy_pre-MIAA-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66","healthy_pre-sero-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66","healthy_pre-t1d-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66","serconverters_or_T1D-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66"), function(x) grep(x,microbiome_data))]

#age_data = input_data[grep("_feature_list_time.rds",input_data)]
#age_plus_microbiome_data = input_data[grep("_feature_list_time,microbiome.rds",input_data)]
#all_clinical_data = input_data[grep("_feature_list_all_clinical.rds",input_data)]
#all_clinical_plus_microbiome = input_data[grep("_feature_list_all_clinical_plus_microbiome.rds",input_data)]

getAUCs = function(rds_files) {
  testAUCs = sapply(rds_files, function(myrds) {
    mylist = readRDS(myrds)
    AUC = mylist[[2]]["AUC-ROC_Score"]
    AUC_CIs = mylist[[2]]["AUC-ROC_CI"]
    AUC_CIs_lower = strsplit(AUC_CIs,split="-")[[1]][1]
    AUC_CIs_upper = strsplit(AUC_CIs,split="-")[[1]][2]
    return(c(AUC_CIs_lower,AUC,AUC_CIs_upper))
  })
  return(testAUCs)
}

#get_getAUCs_CI = function(rds_files) {
#  testAUCs = sapply(rds_files, function(myrds) {
 #   mylist = readRDS(myrds)
#    AUC = mylist[[2]]["AUC-ROC_CI"]
#    return(AUC)
#  })
#  return(testAUCs)
#}

#microbiome_only_AUcs = get_getAUCs_CI(microbiome_data)
#age_only_AUcs = get_getAUCs_CI(age_data)
#age_plus_microbiome_AUcs = get_getAUCs_CI(age_plus_microbiome_data)
#all_clinical_AUCs = get_getAUCs_CI(all_clinical_data)
#all_clinical_plus_microbiome_AUCs = get_getAUCs_CI(all_clinical_plus_microbiome)



microbiome_only_AUcs = getAUCs(microbiome_data)
#age_only_AUcs = getAUCs(age_data)
#age_plus_microbiome_AUcs = getAUCs(age_plus_microbiome_data)
#all_clinical_AUCs = getAUCs(all_clinical_data)
#all_clinical_plus_microbiome_AUCs = getAUCs(all_clinical_plus_microbiome)

#microbiome_only_AUcs_conditions = sapply(strsplit(basename(colnames(microbiome_only_AUcs)),split="-3month_before_vs_far_from_condition"), function(x) x[[1]])
microbiome_only_AUCs_conditions = sapply(strsplit(basename(colnames(microbiome_only_AUcs)),split="-"), function(my_temp_list) {
  beggin_remove = min(grep("month",my_temp_list))
  return(paste(my_temp_list[-seq(beggin_remove,length(my_temp_list))],collapse="-"))
})

microbiome_only_AUCs_ages = sapply(strsplit(basename(colnames(microbiome_only_AUcs)),split="-"), function(my_temp_list) {
  beggin_remove = grep("month",my_temp_list)
  month_parts = my_temp_list[beggin_remove]
  month_parts = gsub("_before_vs_far_from_condition_subject_level","",month_parts)
  return(paste(month_parts,collapse="-"))
})



#age_only_AUcs_conditions = sapply(strsplit(basename(colnames(age_only_AUcs)),split="-3month_before_vs_far_from_condition"), function(x) x[[1]])
#age_plus_microbiome_AUcs_conditions = sapply(strsplit(basename(colnames(age_plus_microbiome_AUcs)),split="-3month_before_vs_far_from_condition"), function(x) x[[1]])
#all_clinical_AUCs_conditions = sapply(strsplit(basename(colnames(all_clinical_AUCs)),split="-3month_before_vs_far_from_condition"), function(x) x[[1]])
#all_clinical_plus_microbiome_AUCs_conditions = sapply(strsplit(basename(colnames(all_clinical_plus_microbiome_AUCs)),split="-3month_before_vs_far_from_condition"), function(x) x[[1]])

#AUCs_all = cbind(microbiome_only_AUcs,age_only_AUcs,age_plus_microbiome_AUcs,all_clinical_AUCs,all_clinical_plus_microbiome_AUCs)
#AUCs_all = t(AUCs_all)
#version_all = c(rep("microbiome",5),rep("age",5),rep("age_plus_microbiome",5),rep("all_clinical",5),rep("all_clinical_plus_microbiome",5))
#conditions_all = c(microbiome_only_AUcs_conditions,age_only_AUcs_conditions,age_plus_microbiome_AUcs_conditions,all_clinical_AUCs_conditions,all_clinical_plus_microbiome_AUCs_conditions)

AUC_df = cbind(t(microbiome_only_AUcs),condition=microbiome_only_AUCs_conditions,age=microbiome_only_AUCs_ages)
colnames(AUC_df)[1] = "lower_AUC_CI"
colnames(AUC_df)[2] = "AUC"
colnames(AUC_df)[3] = "upper_AUC_CI"
AUC_df = as.data.frame(AUC_df)
AUC_df[,1] = as.numeric(AUC_df[,1])
AUC_df[,2] = as.numeric(AUC_df[,2])
AUC_df[,3] = as.numeric(AUC_df[,3])

AUC_df$condition = as.factor(AUC_df$condition)
AUC_df$age = factor(AUC_df$age,levels=c("1month","1month-2month","2month","2month-3month","3month"))
AUC_df = AUC_df[-grep("serconverters_or_T1D",rownames(AUC_df)),]
library(ggplot2)

# not to self. use ribbons instead of error bars
pdf("microbiome_AUCs_various_covariates_before_vs_early_T1D_bootsraped_ttest_res_0.66_train.pdf",width=10)
ggplot(AUC_df[grep("0.66",rownames(AUC_df)),], aes(x=condition, y=AUC, fill=age)) +geom_bar(stat="identity",position=position_dodge()) + geom_errorbar(aes(ymin=lower_AUC_CI, ymax=upper_AUC_CI),width=0.1,position=position_dodge(.9))
dev.off()

pdf("microbiome_AUCs_various_covariates_before_vs_early_T1D_bootsraped_ttest_res_0.5_train.pdf",width=10)
ggplot(AUC_df[grep("0.5",rownames(AUC_df)),], aes(x=condition, y=AUC, fill=age)) +geom_bar(stat="identity",position=position_dodge()) + geom_errorbar(aes(ymin=lower_AUC_CI, ymax=upper_AUC_CI),width=0.1,position=position_dodge(.9))
dev.off()


#pdf("microbiome_AUCs_various_covariates_before_vs_early_T1D.pdf",width=10)
#ggplot(AUC_df,aes(x=condition,y=AUC,fill=version)) + geom_bar(stat="identity",position=position_dodge()) + geom_errorbar( aes(x=condition, ymin=lower_AUC_CI, ymax=upper_AUC_CI),position=position_dodge(.9),width=.2) + coord_cartesian(ylim = c(0.5, 1)) + theme_classic()
#dev.off()

prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=TRUE)
setkey(prokka_annotations,V1)
gene_functions_func = function(microbiome_data_param,output_plot) {
  functions_list = lapply(microbiome_data_param, function(myrds) {
    mylist = readRDS(myrds)
    geneNames = mylist[[3]]
    if(class(geneNames)[1] == "numeric") {
      geneNames = geneNames[-grep("Intercept",names(geneNames))]
      geneNames = geneNames[order(abs(geneNames),decreasing = TRUE)]
      prokka_annotations_ordered = prokka_annotations[names(geneNames)]
      prokka_annotations_ordered = cbind(prokka_annotations_ordered,coef=geneNames)
      prokka_annotations_ordered$file = basename(myrds)
      return(prokka_annotations_ordered)
    } else {
      geneNames = geneNames[-grep("Intercept",rownames(geneNames)),,drop=FALSE]
      geneNames = geneNames[order(abs(geneNames[,1]),decreasing = TRUE),,drop=FALSE]
      prokka_annotations_ordered = prokka_annotations[rownames(geneNames)]
      prokka_annotations_ordered = cbind(prokka_annotations_ordered,coef=geneNames[,1])
      prokka_annotations_ordered$file = basename(myrds)
      return(prokka_annotations_ordered)
    }
  })
  frequency_table = data.frame(sort(table(unlist(lapply(functions_list, function(x) {
  unique(x$V7)
  }))),decreasing = TRUE))
  frequency_table$prop = frequency_table$Freq/length(functions_list)
  if("hypothetical protein"%in%frequency_table[,1]) {
    frequency_table = frequency_table[-match("hypothetical protein",frequency_table[,1]),]
  }
  myplot = ggplot(frequency_table[1:25,],aes(x=Var1,y=prop)) + geom_bar(stat="identity") + coord_flip() + theme_classic()
  pdf(output_plot)
  print(myplot)
  dev.off()
  return(frequency_table)
}

all_microbiome_functions = gene_functions_func(microbiome_data,"all_microbiome_functions_before_onset.pdf")
sero_microbiome_functions = gene_functions_func(microbiome_data[grep("healthy_pre-sero",microbiome_data)],"sero_microbiome_functions_before_onset.pdf")
MIAA_microbiome_functions = gene_functions_func(microbiome_data[grep("healthy_pre-MIAA",microbiome_data)],"MIAA_microbiome_functions_before_onset.pdf")
IA2A_microbiome_functions = gene_functions_func(microbiome_data[grep("healthy_pre-IA2A",microbiome_data)],"IA2A_microbiome_functions_before_onset.pdf")
GAD_microbiome_functions = gene_functions_func(microbiome_data[grep("healthy_pre-GAD",microbiome_data)],"GAD_microbiome_functions_before_onset.pdf")
T1D_microbiome_functions = gene_functions_func(microbiome_data[grep("healthy_pre-t1d",microbiome_data)],"T1D_microbiome_functions_before_onset.pdf")

  
gene_functions = lapply(microbiome_data, function(myrds) {
  mylist = readRDS(myrds)
  geneNames = mylist[[3]]
  if(class(geneNames)[1] == "numeric") {
    geneNames = geneNames[-grep("Intercept",names(geneNames))]
    geneNames = geneNames[order(abs(geneNames),decreasing = TRUE)]
    prokka_annotations_ordered = prokka_annotations[names(geneNames)]
    prokka_annotations_ordered = cbind(prokka_annotations_ordered,coef=geneNames)
    prokka_annotations_ordered$file = basename(myrds)
    return(prokka_annotations_ordered)
  } else {
    geneNames = geneNames[-grep("Intercept",rownames(geneNames)),,drop=FALSE]
    geneNames = geneNames[order(abs(geneNames[,1]),decreasing = TRUE),,drop=FALSE]
    prokka_annotations_ordered = prokka_annotations[rownames(geneNames)]
    prokka_annotations_ordered = cbind(prokka_annotations_ordered,coef=geneNames[,1])
    prokka_annotations_ordered$file = basename(myrds)
    return(prokka_annotations_ordered)
  }
})
names(gene_functions) = basename(microbiome_data)
gene_functions_df = do.call("rbind",gene_functions)
write.csv(gene_functions_df,"before_vs_after_sig_genes_table.csv")



sig_genes =unique(do.call("rbind",gene_functions)$V1)
#sig_genes = sig_genes[-match(c("time_since_abx","time"),sig_genes)]
write.table(sig_genes,"/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_sig_genes.txt",col.names=FALSE,row.names=FALSE,quote=FALSE)

## lets do some net reclassification

calc_microbiome_reclassification = function(microbiome_files,non_microbiome_files) {
  improvements_from_microbiome = lapply(1:length(microbiome_files), function(x) {
    all_clinical_plus_microbiome_temp = readRDS(microbiome_files[[x]])[[6]]
    all_clinical_data_temp = readRDS(non_microbiome_files[[x]])[[6]]
    calc_sum_stats = function(probability_df) {
      probability_df$condition = sapply(strsplit(rownames(probability_df),split="_"), function(x) x[[2]])
      TP_num = sum(probability_df$obs == "case" & probability_df$condition == 1)
      TN_num = sum(probability_df$obs == "ctrl" & probability_df$condition == 0)
      FP_num = sum(probability_df$obs == "case" & probability_df$condition == 0)
      FN_num = sum(probability_df$obs == "ctrl" & probability_df$condition == 1)
      temp_df = data.frame(c(TP_num,FN_num),c(FP_num,TN_num))
      rownames(temp_df) = c("Predicted_Case","Predicted_Ctrl")
      colnames(temp_df) = c("Actual_Case","Actual_Ctrl")
      return(temp_df)
    }
    all_clinical_plus_microbiome_table = calc_sum_stats(all_clinical_plus_microbiome_temp)
    all_clinical_data_table = calc_sum_stats(all_clinical_data_temp)
    microbiome_improvements = all_clinical_plus_microbiome_table-all_clinical_data_table
    return(microbiome_improvements)
  })
  return(improvements_from_microbiome)
}
clinical_vs_clincial_plus_microbiome = calc_microbiome_reclassification(microbiome_files=all_clinical_plus_microbiome,non_microbiome_files=all_clinical_data)

time_vs_time_plus_microbiome = calc_microbiome_reclassification(microbiome_files=age_plus_microbiome_data,non_microbiome_files=age_data)



#/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/models_input_files_parsed_v4_before_vs_far_from_condition_subject_level_no_header.csv

test_abundance_data = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/healthy_pre-t1d-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66/healthy_pre-t1d-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66filtered_transformed_abundance_test.csv"
T1D_test_abundances = fread(test_abundance_data,data.table=FALSE)
test_gene_names = read.table(paste(dirname(test_abundance_data),"/",basename(dirname(test_abundance_data)),"filtered_abundance_test_names.txt",sep=""),header=FALSE)[,1]
rownames(T1D_test_abundances) = test_gene_names
T1D_test_abundances$condition = as.numeric(sapply(strsplit(rownames(T1D_test_abundances),split="_"), function(x) x[2]))
T1D_test_abundances_sig = T1D_test_abundances[,c("CILKBADG_64185","condition")]
T1D_test_abundances_sig$condition = as.factor(T1D_test_abundances_sig$condition)
pdf("CILKBADG_64185_T1D_before_verse_farther_from_T1D.pdf")
ggplot(T1D_test_abundances_sig,aes(x=condition,y=CILKBADG_64185)) + geom_boxplot() + geom_jitter() + theme_classic()
dev.off()





#test_abundance_data = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/healthy_pre-sero-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66/healthy_pre-sero-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66filtered_transformed_abundance_test.csv"

test_abundance_data = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/healthy_pre-sero-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66filtered_transformed_abundance_test.csv"


T1D_test_abundances = fread(test_abundance_data,data.table=FALSE)
#test_gene_names = read.table(paste(dirname(test_abundance_data),"/",basename(dirname(test_abundance_data)),"filtered_abundance_test_names.txt",sep=""),header=FALSE)[,1]
test_gene_names = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_4/healthy_pre-sero-3month_before_vs_far_from_condition_subject_level-all_HLA_proportion_train_0.66filtered_abundance_test_names.txt")
rownames(T1D_test_abundances) = test_gene_names[,1]
T1D_test_abundances$condition = as.numeric(sapply(strsplit(rownames(T1D_test_abundances),split="_"), function(x) x[2]))
T1D_test_abundances_sig = T1D_test_abundances[,c("CIHABBKO_08417","condition")]
T1D_test_abundances_sig$condition = as.factor(T1D_test_abundances_sig$condition)
pdf("CIHABBKO_08417_sero_before_verse_farther_from_T1D.pdf")
ggplot(T1D_test_abundances_sig,aes(x=condition,y=CIHABBKO_08417)) + geom_boxplot() + geom_jitter() + theme_classic()
dev.off()


# read in mapping data

abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern = ".csv",full.names = TRUE)
abundance_files = abundance_files[-grep("healthy",abundance_files)]

gene_to_file_mapping = read.table("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE)

metadata_sample = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv")

genes_to_find = c("CIHABBKO_08417")

prefixes_to_search = gene_to_file_mapping[match(genes_to_find,gene_to_file_mapping[,1]),2]
prefixes_to_search = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",prefixes_to_search,sep="/")

abundance_sig_genes = do.call("rbind",lapply(prefixes_to_search, function(myfile) {
  d_small = fread(myfile,sep=",",header=TRUE,data.table=FALSE,nrow=1)
  d_small = d_small[,-1]
  ## aa.csv will have an extra row for the gene names so lets remove that
  has_extra_row = d_small[1,1]=="genename"
  if(has_extra_row == TRUE) {
    d = fread(myfile,sep=",",header=TRUE,data.table=FALSE,skip=1)
    d = d[,-1]
  } else {
    d = fread(myfile,sep=",",header=TRUE,data.table=FALSE)
    d = d[,-1]
  }
  rownames(d) = d$genename
  d = d[,-1]
  filtered_d = d[rownames(d)%in%genes_to_find,,drop=FALSE]
    return(filtered_d)
}))
abundance_sig_genes_t = t(abundance_sig_genes)

loged_data = lapply(colnames(abundance_sig_genes_t), function(mygene) {
    abundance_values_gene = abundance_sig_genes_t[,mygene]
    min_pos_abundance_values_gene = min(abundance_values_gene[abundance_values_gene>0])
    replace_vals = replace(abundance_values_gene, abundance_values_gene == 0, min_pos_abundance_values_gene / 2)
    train_replace = log2(replace_vals)
    return(train_replace)
  })
loged_data = do.call("cbind",loged_data)
colnames(loged_data) = colnames(abundance_sig_genes_t)

metadata_sample$time_to_seroconversion = metadata_sample$age_mult_persist - metadata_sample$age_at_collection
metadata_sample_ordered = metadata_sample[match(rownames(abundance_sig_genes_t),metadata_sample$Run),]
metadata_sample_ordered = cbind(metadata_sample_ordered,loged_data)
metadata_sample_ordered = metadata_sample_ordered[!is.na(metadata_sample_ordered$time_to_seroconversion),]
metadata_sample_ordered = metadata_sample_ordered[metadata_sample_ordered$time_to_seroconversion>0,]
pdf("CIHABBKO_08417_abundance_vs_sero_time.pdf")
ggplot(metadata_sample_ordered,aes(x=time_to_seroconversion,y=CIHABBKO_08417)) + geom_point() + theme_classic() + geom_vline(xintercept=90,color="red")
dev.off()

lm(CIHABBKO_08417~time_to_seroconversion,metadata_sample_ordered) # 0.936 pvalue...

cor.test(metadata_sample_ordered$CIHABBKO_08417,metadata_sample_ordered$time_to_seroconversion) # 0.001428094 
cor.test(metadata_sample_ordered$CIHABBKO_08417,metadata_sample_ordered$time_to_seroconversion,method="spearman")

# get subjects over and under 90 

metadata_sample_ordered$under90 = 0
metadata_sample_ordered$under90[metadata_sample_ordered$time_to_seroconversion <= 90] = 1

metadata_sample_ordered$bins <- cut(metadata_sample_ordered$time_to_seroconversion, breaks=seq(min(metadata_sample_ordered$time_to_seroconversion),max(metadata_sample_ordered$time_to_seroconversion),by=30), labels=c(1:74))


# take average gene abundance for each subject under and over 90

metadata_sample_ordered_dt = as.data.table(metadata_sample_ordered)

metadata_sample_ordered_dt_age_binned = metadata_sample_ordered_dt[,mean(CIHABBKO_08417),by=.(under90,maskid)]
metadata_sample_ordered_dt_age_binned$under90 = as.factor(metadata_sample_ordered_dt_age_binned$under90)
pdf("CIHABBKO_08417_abundance_3_month_avg_diff.pdf")
ggplot(metadata_sample_ordered_dt_age_binned,aes(x=under90,y=V1)) + geom_boxplot() + geom_jitter() + theme_classic()
dev.off()

metadata_sample_ordered_dt_age_binned2 = metadata_sample_ordered_dt[,mean(CIHABBKO_08417),by=.(bins,maskid)]
metadata_sample_ordered_dt_age_binned2$bins = as.factor(metadata_sample_ordered_dt_age_binned2$bins)
pdf("CIHABBKO_08417_abundance_3_month_avg_diff2.pdf")
ggplot(metadata_sample_ordered_dt_age_binned2,aes(x=bins,y=V1)) + geom_boxplot() + theme_classic()
dev.off()

```









```{bash}
sbatch -c 1 -t 0-01:00 --mem=50G -p short sig_gene_analysis/scripts/get_sequences.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_sig_genes.txt /n/scratch3/users/a/adk9/_RESTORE/adk9/TEDDY/all_seqs_rep_30_db_ted_merged_seqs.fasta /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_sig_genes.fasta

```

#Search for mimics

```{bash}
sbatch -c 1 -t 0-01:00 -p short --mem=30G scripts/search_for_mimics.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_sig_genes.fasta /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_sig_genes_blast_results.txt

awk '{print $2}' /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/before_vs_after_sig_genes_blast_results.txt | awk -F '|' '{print $7}' | sort | uniq


















#Get some stats about average expression, SD, and % of samples each gene is in

```{r}
library(dplyr)
dependent_vars_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only",pattern=".rds",full.names = TRUE)
dependent_vars_files = dependent_vars_files[-c(1,2)]
#independent_variables = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only/healthy_pre-t1d-DR3_DR4_only_1_metadata_filtered.rds")

#colnames(independent_variables)[1]='sampleID'
#independent_variables$sampleID = as.character(independent_variables$sampleID)
min_val_in_array = c()
for (dependent_varfile in dependent_vars_files) {
  dependent_variables = readRDS(dependent_varfile)
  temp = dependent_variables[,-match("SubjectID",colnames(dependent_variables))]
  temp = as.matrix(temp)
  min_nonzero_val = min(temp[temp>0])
  if(length(min_val_in_array) == 0) {
    min_val_in_array = min_nonzero_val
  } else if(min_val_in_array > min_nonzero_val) {
    min_val_in_array = min_nonzero_val
  }
}
min_log_val = log(min_val_in_array)

dependent_vars_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only",pattern=".rds",full.names = TRUE)
dependent_vars_files = dependent_vars_files[-c(1,2)]
independent_variables = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only/healthy_pre-t1d-DR3_DR4_only_1_metadata_filtered.rds")
colnames(independent_variables)[1]='sampleID'
independent_variables$sampleID = as.character(independent_variables$sampleID)
dataset_stats = lapply(dependent_vars_files[1:10], function(dependent_varfile) {
  dependent_variables = readRDS(dependent_varfile)
  dependent_variables$SubjectID = as.character(dependent_variables$SubjectID)
  
  independent_vars_temp = independent_variables[match(dependent_variables$SubjectID,independent_variables$sampleID),]
  my_condition = independent_vars_temp$condition
  
  temp = dependent_variables[,-match("SubjectID",colnames(dependent_variables))]
  temp = as.matrix(temp)
  genes_to_remove = which(colSums(temp) == 0)
  if(length(genes_to_remove) > 0) {
    temp = temp[,-genes_to_remove]
  }
  temp_sum = temp + min_val_in_array
  temp_logged = log(temp_sum_mat)
  # get average abundance
  avg_expression_each_gene = colMeans(temp_logged)
  # get proportion of 0s 
  num_0s_per_gene = apply(temp_logged,2, function(x) sum(x==min_log_val))
  prop_0s_per_gene = num_0s_per_gene/nrow(temp_logged)
  # calculate variance 
  variance_per_gene = apply(temp_logged,2, var)
  # calculate quantiles
  quantiles_per_gene = apply(temp_logged, 2, function(x) quantile(x,c(0.25,0.5,0.75)))
  
  # calculate proportion in T1D patients only
  temp_logged_t1d_subject = temp_logged[my_condition==1,]
  num_0s_per_gene_t1d_subjects = apply(temp_logged_t1d_subject,2, function(x) sum(x==min_log_val))
  prop_0s_per_gene_t1d_subjects = num_0s_per_gene_t1d_subjects/nrow(temp_logged_t1d_subject)
  
  temp_logged_healthy_subject = temp_logged[my_condition==0,]
  num_0s_per_gene_healthy_subjects = apply(temp_logged_healthy_subject,2, function(x) sum(x==min_log_val))
  prop_0s_per_gene_healthy_subjects = num_0s_per_gene_healthy_subjects/nrow(temp_logged_healthy_subject)

stats_temp_mat = rbind(avg_expression_each_gene,variance_per_gene,prop_0s_per_gene,prop_0s_per_gene_t1d_subjects,prop_0s_per_gene_healthy_subjects,quantiles_per_gene)
  
  return(stats_temp_mat)
})
all_dataset_stats = do.call("cbind",dataset_stats)
write.csv(all_dataset_stats,"DR3_4_all_gene_stats.tsv")



# sbatch -c 1 -t 0-05:00 -p short --mem=30G scripts/get_DR3_DR4_all_timepoint_gene_stats.bash

#...
library(data.table)
DR34_stats = fread("DR3_4_all_gene_stats.csv",header=TRUE,sep=",",data.table=FALSE)
rownames(DR34_stats) = DR34_stats[,1]
DR34_stats = DR34_stats[,-1]
# read output 
my_output = readRDS("initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_test/healthy_pre-t1d-DR3_DR4_only_output_test_full_association_output_adjusted.rds")
my_output = my_output[order(my_output$p.value),]

DR34_stats_sorted = DR34_stats[,my_output$feature]

proportion_0s_pergene = DR34_stats_sorted["prop_0s_per_gene",]
proportion_0s_pergene = unlist(proportion_0s_pergene)
jpeg("DRA3_DR4_prop0s_pval_sorted.jpeg",height=1000,width=1000)
dotchart(proportion_0s_pergene[1:10000])
dev.off()

jpeg("DRA3_DR4_prop0s_density.jpeg",height=1000,width=1000)
#plot(density(proportion_0s_pergene))
hist(proportion_0s_pergene)
dev.off()

proportion_0s_t1d = DR34_stats_sorted["prop_0s_per_gene_t1d_subjects",]
proportion_0s_t1d = unlist(proportion_0s_t1d)
jpeg("DRA3_DR4_prop0s_t1d_pval_sorted.jpeg",height=1000,width=1000)
dotchart(proportion_0s_t1d[1:10000])
dev.off()

jpeg("DRA3_DR4_prop0s_t1d_density.jpeg",height=1000,width=1000)
plot(density(proportion_0s_t1d))
dev.off()

proportion_0s_healthy_pergene = DR34_stats_sorted["prop_0s_per_gene_healthy_subjects",]
proportion_0s_healthy_pergene = unlist(proportion_0s_healthy_pergene)

jpeg("DRA3_DR4_prop0s_healthy_pval_sorted.jpeg",height=1000,width=1000)
dotchart(proportion_0s_healthy_pergene[1:10000])
dev.off()

jpeg("DRA3_DR4_prop0s_t1d_healthy.jpeg",height=1000,width=1000)
plot(density(proportion_0s_healthy_pergene))
dev.off()

median_pergene = DR34_stats_sorted["50%",]
median_pergene = unlist(median_pergene)

jpeg("DRA3_DR4_median_pval_sorted.jpeg",height=1000,width=1000)
dotchart(median_pergene[1:10000])
dev.off()

jpeg("DRA3_DR4_median_dist.jpeg",height=1000,width=1000)
hist(median_pergene)
dev.off()

variance_pergene = DR34_stats_sorted["variance_per_gene",]
variance_pergene = unlist(variance_pergene)

jpeg("DRA3_DR4_variance_dist.jpeg",height=1000,width=1000)
hist(variance_pergene)
abline(v=c(6))
dev.off()

jpeg("DRA3_DR4_variance_pval_sorted.jpeg",height=1000,width=1000)
dotchart(variance_pergene[1:10000])
dev.off()

mad_pergene = DR34_stats["mad_per_gene",]
mad_pergene = unlist(mad_pergene)

jpeg("DRA3_DR4_mad_distribution.jpeg",height=1000,width=1000)
hist(mad_pergene)
dev.off()

```

##Make boxplots for top genes by pvalue

```{r}
library(dplyr)
library(ggplot2)
dependent_vars_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only",pattern=".rds",full.names = TRUE)
metadata = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only/healthy_pre-t1d-DR3_DR4_only_1_metadata_filtered.rds")
dependent_vars_files = dependent_vars_files[-grep("_metadata_filtered.rds",dependent_vars_files)]

#min_value = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only/min_val")

my_output = readRDS("initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_test/healthy_pre-t1d-DR3_DR4_only_output_test_full_association_output_adjusted.rds")
my_output = my_output[order(my_output$p.value),]

top_genes_by_pval = my_output$feature[1:10]
old_top_sig_genes = "DPDLKFBF_28626"
top_genes_by_pval = c(top_genes_by_pval,old_top_sig_genes)
for (myfile in dependent_vars_files) {
  my_abundance_mat = readRDS(myfile)
  metadata_ordered = metadata[match(my_abundance_mat$SubjectID,metadata$SubjectID),]
  my_abundance_mat = my_abundance_mat[,-match("SubjectID",colnames(my_abundance_mat))]
  my_abundance_mat = as.matrix(my_abundance_mat)
  min_val = min(my_abundance_mat[my_abundance_mat>0])
  my_abundance_mat = my_abundance_mat+min_val
  my_abundance_mat = log(my_abundance_mat)
  are_any_topgenes_found = top_genes_by_pval%in%colnames(my_abundance_mat)
  if(sum(are_any_topgenes_found)>0) {
    topGenes_found = top_genes_by_pval[are_any_topgenes_found]
    print(topGenes_found)
    my_condition = metadata_ordered$condition
    my_condition = as.factor(my_condition)
    for(topgene in topGenes_found) {
      temp_df = data.frame(value=my_abundance_mat[,topgene],condition=my_condition)
      pdf(paste(topgene,"_DR3_DR4_output_test_boxplot.pdf",sep=""))
      myplot = ggplot(temp_df,aes(x=condition,y=value)) + geom_boxplot() + geom_jitter()
      print(myplot)
      dev.off()
    }
  }
}

```

##Now make the top boxplots when removing genes with 90% or more of samples have 0s

```{r}
library(dplyr)
library(ggplot2)
dependent_vars_files = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only",pattern=".rds",full.names = TRUE)
metadata = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only/healthy_pre-t1d-DR3_DR4_only_1_metadata_filtered.rds")
dependent_vars_files = dependent_vars_files[-grep("_metadata_filtered.rds",dependent_vars_files)]

min_value = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only/min_val")

my_output = readRDS("initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_ten_perc_cutoff/healthy_pre-t1d-DR3_DR4_only_output_ten_perc_cutoff_full_association_output_adjusted.rds")
my_output = my_output[order(my_output$p.value),]

top_genes_by_pval = my_output$feature[1:10]
old_top_sig_genes = "DPDLKFBF_28626"
top_genes_by_pval = c(top_genes_by_pval,old_top_sig_genes)
for (myfile in dependent_vars_files) {
  my_abundance_mat = readRDS(myfile)
  metadata_ordered = metadata[match(my_abundance_mat$SubjectID,metadata$SubjectID),]
  my_abundance_mat = my_abundance_mat[,-match("SubjectID",colnames(my_abundance_mat))]
  my_abundance_mat = as.matrix(my_abundance_mat)
  my_abundance_mat = my_abundance_mat+min_value
  my_abundance_mat = log(my_abundance_mat)
  are_any_topgenes_found = top_genes_by_pval%in%colnames(my_abundance_mat)
  if(sum(are_any_topgenes_found)>0) {
    topGenes_found = top_genes_by_pval[are_any_topgenes_found]
    print(topGenes_found)
    my_condition = metadata_ordered$condition
    my_condition = as.factor(my_condition)
    for(topgene in topGenes_found) {
      temp_df = data.frame(value=my_abundance_mat[,topgene],condition=my_condition)
      pdf(paste(topgene,"_DR3_DR4_output_ten_perc_boxplot.pdf",sep=""))
      myplot = ggplot(temp_df,aes(x=condition,y=value)) + geom_boxplot() + geom_jitter()
      print(myplot)
      dev.off()
    }
  }
}

```

#Make boxplot function

```{r}
library(dplyr)
library(ggplot2)
make_top_ten_boxplots = function(input_folder,output_file,output_folder) {
  dir.create(output_folder)
  dependent_vars_files = list.files(input_folder,pattern=".rds",full.names = TRUE)
  metadata = readRDS(dependent_vars_files[grep("_1_metadata_filtered.rds",dependent_vars_files)])
  dependent_vars_files = dependent_vars_files[-grep("_metadata_filtered.rds",dependent_vars_files)]
  min_value = readRDS(paste(input_folder,"/min_val",sep=""))
  my_output = readRDS(output_file)
  my_output = my_output[order(my_output$p.value),]
  top_genes_by_pval = my_output$feature[1:10]
  for (myfile in dependent_vars_files) {
    my_abundance_mat = readRDS(myfile)
    metadata_ordered = metadata[match(my_abundance_mat$SubjectID,metadata$SubjectID),]
    my_abundance_mat = my_abundance_mat[,-match("SubjectID",colnames(my_abundance_mat))]
    my_abundance_mat = as.matrix(my_abundance_mat)
    my_abundance_mat = my_abundance_mat+min_value
    my_abundance_mat = log(my_abundance_mat)
    are_any_topgenes_found = top_genes_by_pval%in%colnames(my_abundance_mat)
    if(sum(are_any_topgenes_found)>0) {
      topGenes_found = top_genes_by_pval[are_any_topgenes_found]
      print(topGenes_found)
      my_condition = metadata_ordered$condition
      my_condition = as.factor(my_condition)
      for(topgene in topGenes_found) {
        temp_df = data.frame(value=my_abundance_mat[,topgene],condition=my_condition)
        pdf(paste(output_folder,"/",topgene,"_boxplot.pdf",sep=""))
        myplot = ggplot(temp_df,aes(x=condition,y=value)) + geom_boxplot() + geom_jitter()
        print(myplot)
        dev.off()
      }
    }
  }
}

make_top_ten_boxplots(input_folder="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only",output_file="initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_same_median/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_same_median_full_association_output_adjusted.rds",output_folder="DR4_DR4_only_both_conditions_no_same_median_boxplots")

make_top_ten_boxplots(input_folder="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only",output_file="initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_variance_cutoff/healthy_pre-t1d-DR3_DR4_only_output_variance_cutoff_full_association_output_adjusted.rds",output_folder="DR4_DR4_variance_cutoff_boxplots")

make_top_ten_boxplots(input_folder="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only",output_file="initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_zero_median/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_zero_median_full_association_output_adjusted.rds",output_folder="DR4_DR4_no_zero_median_boxplots")
```

#Before we run associations lets check how many genes are abundant in X % of subjects

```{r}
library(dplyr)
library(data.table)
library(parallel)
abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern = ".csv")
abundance_files = abundance_files[-grep("healthy",abundance_files)]

metadata = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821.csv")

num_genes_remaining_mat = mclapply(abundance_files, function(x) {
  print(x)
  d_small = fread(x,sep=",",header=TRUE,data.table=FALSE,nrow=1) %>% select(-V1)
  ## aa.csv will have an extra row for the gene names so lets remove that
  has_extra_row = d_small[1,1]=="genename"
  if(has_extra_row == TRUE) {
    d = fread(x,sep=",",header=TRUE,data.table=FALSE,skip=1)
    d = d[,-1]
  } else {
    d = fread(x,sep=",",header=TRUE,data.table=FALSE) %>% select(-V1)
  }
  rownames(d) = d$genename
  d = d[,-1]
  
  samples_to_keep = intersect(colnames(d),metadata$Run) 
  d = d[,match(samples_to_keep,colnames(d))]
  metadata_temp = metadata[match(samples_to_keep,metadata$Run),]
  subjects = unique(metadata_temp$maskid)
  
  total_abundance_in_subjects = sapply(subjects, function(subj) {
    subject_index = metadata_temp$maskid%in%subj
    if(length(which(subject_index))>1) {
      return(rowSums(d[,subject_index]))
    } else {
      return(d[,subject_index])
    }
  })
  rm(d)
  percent_subjects_gene_in = rowSums(total_abundance_in_subjects>0) / ncol(total_abundance_in_subjects)
  
  
  num_genes_remaining = sapply(seq(0.1,1,0.05), function(p) {
    return(sum(percent_subjects_gene_in>=p))
  })
  names(num_genes_remaining) = seq(0.1,1,0.05)
  return(num_genes_remaining)
},mc.cores=9)

saveRDS(num_genes_remaining_mat,"num_genes_remaining_mat.rds")

# the below is not in the script below
combined_mat = do.call("cbind",num_genes_remaining_mat)

num_genes_in_x_perc = rowSums(combined_mat)
num_genes_in_x_perc_df = as.data.frame(num_genes_in_x_perc)
num_genes_in_x_perc_df$percentile = rownames(num_genes_in_x_perc_df)
num_genes_in_x_perc_df$percentile = as.factor(num_genes_in_x_perc_df$percentile)
colnames(num_genes_in_x_perc_df)[1] = "gene_number"
library(ggplot2)
pdf("num_genes_in_x_perc_of_subjects.pdf")
ggplot(num_genes_in_x_perc_df,aes(x=percentile,y=gene_number,group=1)) + geom_line() + geom_point() + geom_text(label=num_genes_in_x_perc_df$gene_number,nudge_x=-1.5, nudge_y=0.1, check_overlap=T)
dev.off()
```

#Run the above script

```{bash}
sbatch -c 1 -t 0-11:59 -p short --mem=100G get_genes_prevelent_diff_percent_filters.bash
```

#Lets do this for every time point

```{r}
library(parallel)
myfolders = list.files(pattern="healthy_pre-t1d-",full.names = TRUE)
myfolders = myfolders[-grep(".csv",myfolders)]

all_timepoints = mclapply(myfolders, function(folder) {
  print(folder)
  abundance_files = list.files(folder,pattern=".rds",full.names = TRUE)
  abundance_files = abundance_files[-grep("metadata",abundance_files)]
  num_genes_remaining_list = lapply(abundance_files, function(rds_file) {
    abundance_df = readRDS(rds_file)
    abundance_df = abundance_df[,-1]
    abundance_df = as.matrix(abundance_df)
    percent_subjects_gene_in = colSums(abundance_df>0) / nrow(abundance_df)
    num_genes_remaining = sapply(seq(0.1,1,0.05), function(p) {
      return(sum(percent_subjects_gene_in>=p))
    })
    names(num_genes_remaining) = seq(0.1,1,0.05)
    return(num_genes_remaining)
  })
},mc.cores=15)

total_genes_left_each_timepoint = lapply(all_timepoints, function(x) {
  return(rowSums(do.call("cbind",x)))
})
names(total_genes_left_each_timepoint) = myfolders

total_genes_left_each_timepoint_df = do.call("cbind",total_genes_left_each_timepoint)
saveRDS(total_genes_left_each_timepoint_df,"total_genes_left_each_timepoint_df.rds")

write.csv(total_genes_left_each_timepoint_df,"total_genes_left_each_timepoint.csv")

#ok these results are very weird. like at 100% only 92,000 genes are kept but in 12-18 months 245,223 genes are kept. Is this a bug or some sort of rounding error? Actually this may make sense. Cause all months will contain more subjects then the 12-18 months. Since we have more subjects, a lower number of genes will be in all subjects.  

# lets find genes that have 100% identity in 12-18 months but not all months and go from there
rds_files_12_18 = list.files("healthy_pre-t1d-12month-18month-all_HLA",pattern=".rds",full.names = TRUE)
rds_files_all = list.files("healthy_pre-t1d-all_HLA",pattern=".rds",full.names = TRUE)
rds_files_all = rds_files_all[-grep("metadata",rds_files_all)]
rds_files_12_18 = rds_files_12_18[-grep("metadata",rds_files_12_18)]

genes_in_100perc_12_18_list = lapply(rds_files_12_18, function(rds_file) {
  abundance_df = readRDS(rds_file)
  abundance_df = abundance_df[,-1]
  abundance_df = as.matrix(abundance_df)
  percent_subjects_gene_in = colSums(abundance_df>0) / nrow(abundance_df)
  one_hundred_perc_genes = percent_subjects_gene_in>=1
  one_hundred_perc_genes_names = names(which(one_hundred_perc_genes))
  return(one_hundred_perc_genes_names)
})
genes_in_100perc_12_18 = unlist(genes_in_100perc_12_18_list)

genes_in_100perc_all_times_list = lapply(rds_files_all, function(rds_file) {
  abundance_df = readRDS(rds_file)
  abundance_df = abundance_df[,-1]
  abundance_df = as.matrix(abundance_df)
  percent_subjects_gene_in = colSums(abundance_df>0) / nrow(abundance_df)
  one_hundred_perc_genes = percent_subjects_gene_in>=1
  one_hundred_perc_genes_names = names(which(one_hundred_perc_genes))
  return(one_hundred_perc_genes_names)
})
genes_in_100perc_all_times = unlist(genes_in_100perc_all_times_list)

in_12_18_not_all = setdiff(genes_in_100perc_12_18,genes_in_100perc_all_times) # length 193,383

#ok lets read in the mapping file so we can see where some of these genes are

# sample level abundance files
abundance_files = list.files("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",pattern = ".csv",full.names = TRUE)
abundance_files = abundance_files[-grep("healthy",abundance_files)]

gene_to_file_mapping = read.table("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE)

gene_to_file_mapping_order = gene_to_file_mapping[match(in_12_18_not_all,gene_to_file_mapping[,1]),2]

gene_map_only_in_12_18_df = data.frame(in_12_18_not_all,gene_to_file_mapping_order)

# read in aa.csv
library(data.table)
aa_abundance = fread("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/aa.csv",sep=",",header=TRUE)
aa_abundance = aa_abundance[,-1]
aa_abundance[,V1:=NULL]
# lets take AACDOEJM_02166 and AAMGICME_65964 as an example

PNLNDLFB_20979_abundance_sample_level = aa_abundance[genename == "PNLNDLFB_20979" | genename == "AAMGICME_65964"]
PNLNDLFB_20979_abundance_sample_level = as.data.frame(PNLNDLFB_20979_abundance_sample_level)
PNLNDLFB_20979_abundance_sample_level_dt = as.data.table(PNLNDLFB_20979_abundance_sample_level)
PNLNDLFB_20979_abundance_sample_level_melt = data.table::melt(PNLNDLFB_20979_abundance_sample_level_dt,id.vars="genename")
PNLNDLFB_20979_abundance_sample_level_melt$value = as.numeric(PNLNDLFB_20979_abundance_sample_level_melt$value)
# load in mapping files
all_HLA_mapping = read.csv("healthy_pre-t1d-all_HLA.mapping.csv")
samples_to_keep_all_HLA = intersect(all_HLA_mapping$V2,PNLNDLFB_20979_abundance_sample_level_melt$variable)

PNLNDLFB_20979_abundance_sample_level_melt_all_HLA = PNLNDLFB_20979_abundance_sample_level_melt[PNLNDLFB_20979_abundance_sample_level_melt$variable%in%samples_to_keep_all_HLA]
all_HLA_mapping = all_HLA_mapping[match(samples_to_keep_all_HLA,all_HLA_mapping$V2),]

PNLNDLFB_20979_abundance_sample_level_melt$subject = all_HLA_mapping[match(PNLNDLFB_20979_abundance_sample_level_melt$variable,all_HLA_mapping[,2]),"V1"]

PNLNDLFB_20979_abundance_subject_level_melt = PNLNDLFB_20979_abundance_sample_level_melt[,mean(value),by=.(subject,genename)]

PNLNDLFB_20979_abundance_subject_level_melt2 = PNLNDLFB_20979_abundance_sample_level_melt[,sum(value),by=.(subject,genename)]

```

#Step 8. run initial associations

```{bash}
#sbatch -c 1 -t 0-18:00 -p medium --mem=10G scripts/run_regressions_noVoE.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_ten_perc_cutoff 0.10

#sbatch -c 1 -t 0-10:00 -p short --mem=10G scripts/run_regressions_noVoE_variance_cutoff.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_variance_cutoff 6

#sbatch -c 1 -t 0-10:00 -p short --mem=10G scripts/run_regressions_noVoE_no_zero_median_both_conditions.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_zero_median

#sbatch -c 1 -t 0-18:00 -p medium --mem=10G scripts/run_regressions_noVoE_no_same_median_both_conditions.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-DR3_DR4_only /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_same_median

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-*DR3_DR4_only | while read line; do basename $line; done > input_folders_VoE_initial_associations_DR3_4_only

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE_no_zero_median_both_conditions.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_no_zero_median_both_conditions/${line}_no_zero_median_both_conditions_output
done < input_folders_VoE_initial_associations_DR3_4_only


while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff/${line}tenPerc_cutoff_output 0.10
done < input_folders_VoE_initial_associations_DR3_4_only

# now lets do all samples

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-*all_HLA | while read line; do basename $line; done > input_folders_VoE_initial_associations_all_HLA

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE_no_zero_median_both_conditions.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_no_zero_median_both_conditions/${line}_no_zero_median_both_conditions_output
done < input_folders_VoE_initial_associations_all_HLA

# restart healthy_pre-t1d-24month-all_HLA_no_zero_median_both_conditions_output. for some reason it didn't finish
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE_no_zero_median_both_conditions.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-24month-all_HLA /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_no_zero_median_both_conditions/healthy_pre-t1d-24month-all_HLA_no_zero_median_both_conditions_output


while read line
do
sbatch -c 1 -t 0-18:00 -p medium --mem=10G scripts/run_regressions_noVoE.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff/${line}tenPerc_cutoff_output 0.10
done < input_folders_VoE_initial_associations_all_HLA





## run associations for antibody tests

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre* | grep -v ".csv" | grep -v "pre-t1d" | while read line; do basename $line; done > input_folders_VoE_initial_associations_healthy_pre_antibodies


while read line
do
sbatch -c 1 -t 0-18:00 -p medium --mem=10G scripts/run_regressions_noVoE.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff/${line}tenPerc_cutoff_output 0.10
done < input_folders_VoE_initial_associations_healthy_pre_antibodies


# run associations but only on training sets

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2

ls -d /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre*/ | while read line; do basename $line; done > input_folders_VoE_initial_association

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE_training_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff_training_only/${line}_tenPerc_cutoff_output 0.10
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/input_folders_VoE_initial_association

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE_training_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_oneHundredPerc_cutoff_training_only/${line}_oneHundredPerc_cutoff_output 1
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/input_folders_VoE_initial_association

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE_training_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/${line}_ninetyFivePerc_cutoff_output 0.95
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/input_folders_VoE_initial_association

while read line
do
sbatch -c 1 -t 0-11:59 -p short --mem=10G scripts/run_regressions_noVoE_training_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/${line} /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyPerc_cutoff_training_only/${line}_ninetyPerc_cutoff_output 0.90
done < /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/input_folders_VoE_initial_association

```

#Step 9. combine initial association output

```{bash}
#Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_ten_perc_cutoff

#Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_same_median


#Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_both_conditions_no_zero_median

#Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-DR3_DR4_only_output_variance_cutoff

conda activate r_env

for x in initial_association_output_tenPerc_cutoff/*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done


for x in initial_association_output_tenPerc_cutoff/healthy_pre-GAD*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_tenPerc_cutoff/healthy_pre-IA2A*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_tenPerc_cutoff/healthy_pre-MIAA*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_tenPerc_cutoff/healthy_pre-MIAA*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_tenPerc_cutoff/healthy_pre-sero*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_tenPerc_cutoff_training_only/*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_oneHundredPerc_cutoff_training_only/*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_ninetyFivePerc_cutoff_training_only/*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done

for x in initial_association_output_ninetyPerc_cutoff_training_only/*_output
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${x}
done


# don't include healthy_pre-t1d-24month-all_HLA_no_zero_median_both_conditions_output

ls -d initial_association_output_no_zero_median_both_conditions/* | grep -v "healthy_pre-t1d-24month-all_HLA_no_zero_median_both_conditions_output" > combine_nozero_median_data

while read line
do
Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R ${line}
done < combine_nozero_median_data

Rscript /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/parse_initial_association_output_noVoE.R initial_association_output_no_zero_median_both_conditions/healthy_pre-t1d-24month-all_HLA_no_zero_median_both_conditions_output
```

#Step 10. 
#Make tables to see how many genes are sig in each timepoint and category

```{r}
get_sig_gene_table = function(files,suffix,output_folder) {
  gad_files = files[grep("healthy_pre-GAD",files)]
  IA2A_files = files[grep("healthy_pre-IA2A",files)]
  MIAA_files = files[grep("healthy_pre-MIAA",files)]
  sero_files = files[grep("healthy_pre-sero",files)]
  T1D_files = files[grep("healthy_pre-t1d",files)]
  T1D_files_all_HLA = T1D_files[grep("all_HLA",T1D_files)]
  T1D_DR3_4_files = T1D_files[grep("DR3_DR4_only",T1D_files)]
  T1D_nontDR3_4_files = T1D_files[grep("not_DR3_DR4_",T1D_files)]
  all_files_list = list(GAD=gad_files,IA2A=IA2A_files,MIAA=MIAA_files,seroconversion=sero_files,T1D_all_HLA=T1D_files_all_HLA,T1D_DR3_4=T1D_DR3_4_files,T1D_not_DR3_4 = T1D_nontDR3_4_files)
  
  all_sig_gene_counts = lapply(seq(1,length(all_files_list)), function(comparison_type_num) {
    comparison_type = all_files_list[[comparison_type_num]]
    comparison_type_name = names(all_files_list)[comparison_type_num]
    sig_genes_nums = lapply(comparison_type, function(comparison_folder) {
      # get the number of subjects in the comparison
      base_type = sapply(strsplit(basename(comparison_folder),split="_"),function(x) paste(x[-seq(length(x)-2,length(x))],collapse="_"))
      train_subjects = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/",base_type,".train_subjects.txt",sep="")
      subjects = read.table(train_subjects,header=FALSE,sep="\t")
      pval_file = list.files(comparison_folder,pattern="^healthy_",full.names = TRUE)
      pval_df = readRDS(pval_file)
      num_genes = nrow(pval_df)
      num_subjects = length(subjects[,1])
      BY_sig = sum(pval_df$BY < 0.1)
      bonferroni_sig = sum(pval_df$bonferroni < 0.1)
      BH_sig = sum(pval_df$BH < 0.1)
      qvalue_sig = sum(pval_df$qvalue < 0.1,na.rm = TRUE)
      return(c(num_genes,num_subjects,BY_sig,bonferroni_sig,BH_sig,qvalue_sig))
    })
    sig_genes_nums_df = do.call("rbind",sig_genes_nums)
    rownames(sig_genes_nums_df) = basename(comparison_type)
    sig_genes_nums_df = sig_genes_nums_df[c(7,8,9,2,3,4,5,6,1),]
    colnames(sig_genes_nums_df) = c("num_genes","num_subjects","BY","bonferroni","BH","qvalue")
    
    write.csv(sig_genes_nums_df,file=paste(output_folder,"/",comparison_type_name,suffix,".csv",sep=""))
    return(sig_genes_nums_df)
  })
  return(all_sig_gene_counts)
}

all_files_ninety_five = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only",pattern="healthy_",full.names = TRUE)
all_files_onehundred = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_oneHundredPerc_cutoff_training_only",pattern="healthy_",full.names = TRUE)
all_files_ninety = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyPerc_cutoff_training_only",pattern="healthy_",full.names = TRUE)


ninety_five_sig_genes = get_sig_gene_table(all_files_ninety_five,suffix="ninetyFivePerc_cutoff",output_folder = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only")
one_hundred_sig_genes = get_sig_gene_table(all_files_onehundred,suffix="oneHundredPerc_cutoff",output_folder = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_oneHundredPerc_cutoff_training_only")
ninety_sig_genes = get_sig_gene_table(all_files_ninety,suffix="ninetyPerc_cutoff",output_folder = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyPerc_cutoff_training_only")



initial_association_output_ninetyPerc_cutoff_training_only
```

#Lets look at the functions of the significant MIAA genes

```{r}
library(data.table)
prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=FALSE)

miaa_pvals = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/healthy_pre-MIAA_ninetyFivePerc_cutoff_output/healthy_pre-MIAA_ninetyFivePerc_cutoff_output_full_association_output_adjusted.rds")

miaa_pvals_sig = miaa_pvals[miaa_pvals$BY < 0.1,]
miaa_pvals_sig_ordered = miaa_pvals_sig[order(miaa_pvals_sig$p.value),]

prokka_annotations_sig = prokka_annotations[match(miaa_pvals_sig_ordered$feature,prokka_annotations[,1]),]
miaa_pvals_sig_ordered$fun = prokka_annotations_sig$V7
write.table(miaa_pvals_sig_ordered$feature,"/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes.txt",col.names=FALSE,row.names=FALSE,quote=FALSE)
```

#Get sequences of sig genes

```{bash}

sbatch -c 1 -t 0-01:00 --mem=50G -p short sig_gene_analysis/scripts/get_sequences.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes.txt /n/scratch3/users/a/adk9/_RESTORE/adk9/TEDDY/all_seqs_rep_30_db_ted_merged_seqs.fasta /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes.fasta

```

#Search for mimics

```{bash}
sbatch -c 1 -t 0-01:00 -p short --mem=30G scripts/search_for_mimics.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes.fasta /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes_human_proteome_alignments.txt

# get genes 
awk '{print $2}' /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes_human_proteome_alignments.txt | awk -F '|' '{print $7}' | sort | uniq

grep "SLC30A8" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes_human_proteome_alignments.txt

grep "GADL1" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes_human_proteome_alignments.txt

grep "ABAT" /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/miaa_sig_genes_human_proteome_alignments.txt
```

#Step 11. 

#Make function to select top 5% of genes and do lasso

```{r}

library(glmnet)
library(ggplot2)
library(data.table)
library(RNOmni)
gene_to_file_mapping = fread("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE,data.table = FALSE)
prokka_annotations = fread("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/get_consensus_seq_annotations/all_consensus_gene_prokka_annotations_CDS_only.tsv",sep="\t",header=FALSE,data.table=FALSE)

# example of pval file /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/healthy_pre-MIAA_ninetyFivePerc_cutoff_output/healthy_pre-MIAA_ninetyFivePerc_cutoff_output_full_association_output_adjusted.rds
# example of abundance_data_folder is /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA
# correction_method options are BY, BH, bonferroni, qvalue

do_feature_selection = function(pval_file,correction_method,abundance_data_folder,pdf_name,lasso_sig_gene_out,lasso_sig_sample_gene_abundances_out) {
  myrds = readRDS(pval_file)
  myrds_pval_sorted = myrds[order(myrds$p.value),]
  #annotate genes
  myrds_pval_sorted$func = prokka_annotations[match(myrds_pval_sorted$feature,prokka_annotations[,1]),"V7"]
  # get sig genes
  myrds_pval_sorted_sig = myrds_pval_sorted[myrds_pval_sorted[[correction_method]] < 0.1,]
  myrds_pval_sorted_sig = myrds_pval_sorted_sig[order(abs(myrds_pval_sorted_sig$estimate),decreasing = TRUE),]
  # get genes in top 5% qunatiles
  ninetyfivePerQuantile = quantile(abs(myrds_pval_sorted_sig$estimate),0.95)
  topfiveperce_estimate = myrds_pval_sorted_sig[abs(myrds_pval_sorted_sig$estimate)>ninetyfivePerQuantile,]
  sigGenes = topfiveperce_estimate$feature
  gene_to_file_mapping_sig = gene_to_file_mapping[match(sigGenes,gene_to_file_mapping$V1),]

  suffixes = gene_to_file_mapping_sig[,2]
  suffixes = gsub(".csv","",suffixes)
  suffixes = unique(suffixes)
  
  abundance_files = list.files(abundance_data_folder,".rds",full.names = TRUE)
  abundance_files = abundance_files[grep("train_",abundance_files)]
  metadata_files = abundance_files[grep("_1_metadata",abundance_files)]
  abundance_files = abundance_files[-grep("metadata",abundance_files)]

  abundances_of_sig_genes = lapply(abundance_files, function(x) {
    abundance_temp = readRDS(x)
    abundance_temp_sig = abundance_temp[,colnames(abundance_temp)%in%sigGenes,drop=FALSE]
    return(abundance_temp_sig)
  })

  abundances_of_sig_genes_df = do.call("cbind",abundances_of_sig_genes)

  # for each gene add rankNorm
  abundances_of_sig_genes_rankNorm = apply(abundances_of_sig_genes_df,2, function(x) {
    x = RankNorm(x)
    return(x)
  })

  abundance_temp = readRDS(abundance_files[1])
  rownames(abundances_of_sig_genes_rankNorm) = abundance_temp$SubjectID

  #metadata = readRDS("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/healthy_pre-t1d-all_HLA/healthy_pre-t1d-all_HLA_1_metadata_filtered.rds")
  metadata = readRDS(metadata_files)

  metadata = metadata[match(rownames(abundances_of_sig_genes_rankNorm),metadata$SubjectID),]

  # first thing to do is to find the correct lambda penalty
  set.seed(123)
  lambda_seq <- 10^seq(2, -2, by = -.1)

  cv_output <- cv.glmnet(abundances_of_sig_genes_rankNorm, metadata$condition,
                       alpha = 1, lambda = lambda_seq, 
                       nfolds = 5,family="binomial")

  # identifying best lamda
  best_lam <- cv_output$lambda.min

  lasso_best <- glmnet(abundances_of_sig_genes_rankNorm, metadata$condition, alpha = 1, lambda = best_lam,family="binomial")
  # get non 0 variables

  best_vars = coef(lasso_best)

  myVars = best_vars[abs(best_vars[,1])>0,] # 16

  lasso_sig_genes = names(myVars)[-1]

  saveRDS(myVars,lasso_sig_gene_out)
  pdf(pdf_name)
  for(topgene in lasso_sig_genes) {
    temp_df = data.frame(value=abundances_of_sig_genes_rankNorm[,topgene],condition=as.factor(metadata$condition))
    myplot = ggplot(temp_df,aes(x=condition,y=value)) + geom_boxplot() + geom_jitter() + ggtitle(topgene)
    print(myplot)
  }
  dev.off()
  
  
  # now get abundances at sample level
  gene_to_file_mapping_lasso = gene_to_file_mapping[match(lasso_sig_genes,gene_to_file_mapping$V1),]

  suffixes = gene_to_file_mapping_lasso[,2]
  suffixes = gsub(".csv","",suffixes)
  suffixes = unique(suffixes)


  abundance_files_samplelevel = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/",suffixes,".csv",sep="")

  library(dplyr)
  library(data.table)
  abundances_of_sig_genes_test = lapply(abundance_files_samplelevel, function(x) {
    print(x)
    d_small = fread(x,sep=",",header=TRUE,data.table=FALSE,nrow=1) %>% select(-V1)
    ## aa.csv will have an extra row for the gene names so lets remove that
    has_extra_row = d_small[1,1]=="genename"
    if(has_extra_row == TRUE) {
      d = fread(x,sep=",",header=TRUE,data.table=FALSE,skip=1)
      d = d[,-1]
    } else {
      d = fread(x,sep=",",header=TRUE,data.table=FALSE) %>% select(-V1)
    }
    rownames(d) = d$genename
    d = d[,-1]
    found_genes = lasso_sig_genes[lasso_sig_genes%in%rownames(d)]
    abundance_temp_sig = d[rownames(d)%in%found_genes,,drop=FALSE]
    rm(d)
    return(abundance_temp_sig)
  })
  abundances_of_sig_genes_test_df = do.call("rbind",abundances_of_sig_genes_test)
  write.csv(abundances_of_sig_genes_test_df,file=lasso_sig_sample_gene_abundances_out)
}

do_feature_selection(pval_file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/healthy_pre-MIAA_ninetyFivePerc_cutoff_output/healthy_pre-MIAA_ninetyFivePerc_cutoff_output_full_association_output_adjusted.rds",correction_method="BY",abundance_data_folder="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA",pdf_name="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/healthy_pre-MIAA_ninetyFivePerc_cutoff_output/top_lass_sig_genes_vis.pdf",lasso_sig_gene_out="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/healthy_pre-MIAA_ninetyFivePerc_cutoff_output/top_lasso_sig_genes_MIAA.rds",lasso_sig_sample_gene_abundances_out = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/healthy_pre-MIAA_ninetyFivePerc_cutoff_output/top_lasso_sig_genes_sample_abundances_MIAA.csv")


library(data.table)
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff_training_only")

x = "healthy_pre-t1d-all_HLA_tenPerc_cutoff_output/healthy_pre-t1d-all_HLA_tenPerc_cutoff_output_full_association_output_adjusted.rds"

myrds = readRDS(x)
myrds_pval_sorted = myrds[order(myrds$p.value),]
myrds_pval_sorted_sig = myrds_pval_sorted[myrds_pval_sorted$BH < 0.1,] # 514072
myrds_pval_sorted_sig = myrds_pval_sorted_sig[order(abs(myrds_pval_sorted_sig$estimate),decreasing = TRUE),]

#fivePerQuantile = quantile(abs(myrds_pval_sorted_sig$estimate),0.05)
ninetyfivePerQuantile = quantile(abs(myrds_pval_sorted_sig$estimate),0.95)
topfiveperce_estimate = myrds_pval_sorted_sig[abs(myrds_pval_sorted_sig$estimate)>ninetyfivePerQuantile,]

dim(topfiveperce_estimate) # 25704

write.table(topfiveperce_estimate,"healthy_pre-t1d-all_HLA_tenPerc_cutoff_output/healthy_pre-t1d-all_HLAtenPerc_cutoff_output_full_association_output_adjusted_sigGenes_topFivePercORs.tsv",sep="\t",col.names=TRUE,row.names=FALSE,quote=FALSE)

```

#Lets get the genes sig diff at 95% identity and see how they change as a function of time

```{r}
library(data.table)
library(RNOmni)
library(ggplot2)
gene_to_file_mapping = fread("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE,data.table = FALSE)
sig_genes = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only/healthy_pre-MIAA_ninetyFivePerc_cutoff_output/top_lasso_sig_genes_MIAA.rds")
most_sig_genes =  names(sort(abs(sig_genes[-1]),decreasing=TRUE))
# now get p-value of this gene if available at every timepoint

all_folders = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_ninetyFivePerc_cutoff_training_only",pattern="healthy_pre-MIAA",full.names = TRUE)


lapply(most_sig_genes, function(most_sig_gene) {
  print(most_sig_gene)
  diff_between_subject_mean_CIs = sapply(all_folders,function(x) {
    pval_file = list.files(x,pattern="^healthy_pre-MIAA",full.names = TRUE)
    mypval_file = readRDS(pval_file)
    mypval_file_ordered = mypval_file[match(most_sig_gene,mypval_file$feature),]
    pval = mypval_file_ordered[,"p.value"]
    estimate = mypval_file_ordered[,"estimate"]
    BY = mypval_file_ordered[,"BY"]
    base_name = sapply(strsplit(basename(x),split="_"), function(y) paste(y[seq(1,length(y)-3)],collapse="_"))
    abundance_folder = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/",base_name,sep="")
    abundance_file = gene_to_file_mapping[match(most_sig_gene,gene_to_file_mapping$V1),2]
    abundance_file = paste("train_",gsub(".csv",".rds",abundance_file),sep="")
    abundance_file = list.files(abundance_folder,pattern=gsub(".csv",".rds",abundance_file),full.names=TRUE)
    metadata_file = list.files(abundance_folder,pattern="train_1_metadata",full.names = TRUE)
    abundance_data = readRDS(abundance_file)
    metadata = readRDS(metadata_file)
    abundance_data = abundance_data[,c("SubjectID",most_sig_gene)]
    metadata_ordered = metadata[match(abundance_data$SubjectID,metadata$SubjectID),]
    abundance_data[,2] = RankNorm(abundance_data[[most_sig_gene]])
    temp_df = data.frame(value=abundance_data[[most_sig_gene]],condition=as.factor(metadata_ordered$condition))
    # calculate means and CIs for each timepoint
    temp_dt = as.data.table(temp_df)
    temp_dt_mean = temp_dt[,mean(value),by=condition]
    temp_dt_sd = temp_dt[,sd(value),by=condition]
    temp_dt_samplesize = temp_dt[,.N,by=condition]
    samplesize1 = temp_dt_samplesize[1,2]$N
    samplesize2 = temp_dt_samplesize[2,2]$N
    mean1 = temp_dt_mean[1,2]$V1
    mean2 = temp_dt_mean[2,2]$V1
    sd1 = temp_dt_sd[1,2]$V1
    sd2 = temp_dt_sd[2,2]$V1
    var1 = sd1 * sd1
    var2 = sd2 * sd2
    numerator = ((samplesize1-1) * var1) + ((samplesize2-1) * var2)
    denominator = samplesize1 + samplesize2 - 2
    num_over_den = numerator/denominator
    num_over_den = sqrt(num_over_den)
    parta = sqrt((1/samplesize1) + (1/samplesize2)) * num_over_den * 1.96 
    upperCI = (mean1 - mean2) + parta
    lowerCI = (mean1 - mean2) - parta
    diff_means = mean1 - mean2
    total_samplesize = samplesize1 + samplesize2
    header = paste(base_name,"\n","pvalue:",round(pval,4)," BY:",round(BY,4)," estimate",round(estimate,4),sep="")
    pdf(paste(most_sig_gene,"_",base_name,".pdf",sep=""))
    myplot = ggplot(temp_df,aes(x=condition,y=value)) + geom_boxplot() + geom_jitter() + ggtitle(header)
    print(myplot)
    dev.off()
    return(c(diff_means,lowerCI,upperCI,total_samplesize,base_name,estimate$estimate,BY$BY,pval$p.value))
  })
  diff_between_subject_mean_CIs = t(diff_between_subject_mean_CIs)
  colnames(diff_between_subject_mean_CIs) = c("diff_means","lowerCI","upperCI","sample_size","time_group","coef","BY","pvalue")
  diff_between_subject_mean_CIs = as.data.frame(diff_between_subject_mean_CIs)
  diff_between_subject_mean_CIs$diff_means = as.numeric(diff_between_subject_mean_CIs$diff_means)
  diff_between_subject_mean_CIs$lowerCI = as.numeric(diff_between_subject_mean_CIs$lowerCI)
  diff_between_subject_mean_CIs$upperCI = as.numeric(diff_between_subject_mean_CIs$upperCI)
  diff_between_subject_mean_CIs$sample_size = as.numeric(diff_between_subject_mean_CIs$sample_size)
  diff_between_subject_mean_CIs$time_group = gsub("healthy_pre-MIAA","",diff_between_subject_mean_CIs$time_group)
  diff_between_subject_mean_CIs$time_group = gsub("^-","",diff_between_subject_mean_CIs$time_group)
  diff_between_subject_mean_CIs$time_group = gsub("month","",diff_between_subject_mean_CIs$time_group)
  diff_between_subject_mean_CIs$time_group[diff_between_subject_mean_CIs$time_group==""] = "all"
  diff_between_subject_mean_CIs$time_group = factor(diff_between_subject_mean_CIs$time_group,levels=c("3","6","6-12","12","12-18","18","18-24","24","all"))
  diff_between_subject_mean_CIs$coef = as.numeric(diff_between_subject_mean_CIs$coef)
  diff_between_subject_mean_CIs$pvalue = as.numeric(diff_between_subject_mean_CIs$pvalue)
  diff_between_subject_mean_CIs$BY = as.numeric(diff_between_subject_mean_CIs$BY)
  diff_between_subject_mean_CIs$text = paste("subjects:",diff_between_subject_mean_CIs$sample_size,"\ncoef:",round(diff_between_subject_mean_CIs$coef,4),"\npvalue:",round(diff_between_subject_mean_CIs$pvalue,4),"\nBY:",round(diff_between_subject_mean_CIs$BY,4))
  pdf(paste(most_sig_gene,"_diff_means_subject_level.pdf",sep=""),width=20)
  diff_means_subject_level_plot = ggplot(diff_between_subject_mean_CIs,aes(x=time_group,y=diff_means,group=1,label=text)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=lowerCI,ymax=upperCI)) + geom_text(nudge_x = 0.2,nudge_y = 0.5) + geom_hline(yintercept=0,color="red") + theme_classic()
  print(diff_means_subject_level_plot)
  dev.off()

#also plot the abundance of the gene as a function of time. if the above is true, I expect an initial increase in T1D then a decrease in T1D subjects and the difference between the 2 become steadily more pronounced. 

  abundance_file = gene_to_file_mapping[match(most_sig_gene,gene_to_file_mapping$V1),2]
  abundance_file = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/",abundance_file,sep="")
  abundance_file_sample_level = fread(abundance_file,data.table=FALSE)
  abundance_file_sample_level = abundance_file_sample_level[,-1]
  rownames(abundance_file_sample_level) = abundance_file_sample_level[,1]
  abundance_file_sample_level = abundance_file_sample_level[,-1]
  abundance_file_sample_level_top_gene = abundance_file_sample_level[match(most_sig_gene,rownames(abundance_file_sample_level)),]
  # samples in metadata and abundance
  metadata = read.csv("teddy_metadata_20190821.csv")
  controls = metadata[is.na(metadata$age_first_MIAA),]
  cases = metadata[!is.na(metadata$age_first_MIAA) & metadata$age_at_collection < metadata$age_first_MIAA,]
  metadata = rbind(controls,cases)

  training_subjects = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA.train_subjects.txt",header=FALSE)
  training_subjects = training_subjects$V1
  #samples_in_metadata_abundance = intersect(colnames(abundance_file_sample_level_top_gene),metadata$Run)
  #metadata_ordered = metadata[match(samples_in_metadata_abundance,metadata$Run),]
  #abundance_ordered = abundance_file_sample_level_top_gene[,match(samples_in_metadata_abundance,colnames(abundance_file_sample_level_top_gene))]

  #condition = as.factor(as.numeric(metadata_ordered$MIAA_pos))
  #time = metadata_ordered$age_at_collection
  #mydf = data.frame(abundance=as.numeric(abundance_ordered),time,condition)
  #mydf$abundance = RankNorm(mydf$abundance)
  #pdf(paste(most_sig_gene,"_healthy_pre-MIAA_all_timepoints.pdf",sep=""))
  #ggplot(mydf,aes(x=time,y=abundance,col=condition)) + geom_point() + geom_smooth()
  #dev.off()

  metadata_training_only = metadata[metadata$maskid%in%training_subjects,]
  samples_in_metadata_abundance_train = intersect(colnames(abundance_file_sample_level_top_gene),metadata_training_only$Run)
  metadata_ordered_train = metadata_training_only[match(samples_in_metadata_abundance_train,metadata_training_only$Run),]
  abundance_ordered_train = abundance_file_sample_level_top_gene[,match(samples_in_metadata_abundance_train,colnames(abundance_file_sample_level_top_gene))]

  condition_train = as.factor(as.numeric(metadata_ordered_train$MIAA_pos))
  time_train = metadata_ordered_train$age_at_collection
  mydf_train = data.frame(abundance=as.numeric(abundance_ordered_train),time_train,condition_train)
  mydf_train$abundance = RankNorm(mydf_train$abundance)
  pdf(paste(most_sig_gene,"_healthy_pre-MIAA_all_timepoints_train.pdf",sep=""))
  all_timepoints_train_plot = ggplot(mydf_train,aes(x=time_train,y=abundance,col=condition_train)) + geom_point() + geom_smooth()
  print(all_timepoints_train_plot)
  dev.off()

  times_df = data.frame(c(92,""),c(183,""),c(183,365),c(365,""),c(365,548),c(548,""),c(548,730),c(730,""),c("",""))
  times_df = t(times_df)
  times_df = as.data.frame(times_df)

  mydf_train_dt = as.data.table(mydf_train)
#averages_each_timepoint = apply(times_df, 1, function(mytime_temp) {
#  start_time_temp = as.numeric(mytime_temp[1])
#  end_time_temp = as.numeric(mytime_temp[2])
#  if(!is.na(start_time_temp) & !is.na(end_time_temp)) {
#    mydf_train_dt_mean = mydf_train_dt[time_train >= start_time_temp & time_train <= end_time_temp,mean(abundance),by=condition_train]
#  } else if(!is.na(start_time_temp) & is.na(end_time_temp)) {
#    mydf_train_dt_mean = mydf_train_dt[time_train <= start_time_temp,mean(abundance),by=condition_train]
#  } else {
#    mydf_train_dt_mean = mydf_train_dt[,mean(abundance),by=condition_train]
#  }
#  mydf_train_dt_mean$time = paste(start_time_temp,end_time_temp,sep="_")
#  return(mydf_train_dt_mean)
#})
#averages_each_timepoint_df = do.call("rbind",averages_each_timepoint)
#averages_each_timepoint_df$time = factor(averages_each_timepoint_df$time,levels=unique(averages_each_timepoint_df$time))
#averages_each_timepoint_df$condition_train = as.factor(averages_each_timepoint_df$condition_train)
#pdf("training_averages_over_time_groups.pdf")
#ggplot(averages_each_timepoint_df,aes(x=time,y=V1,group=condition_train,color=condition_train)) + geom_point() + geom_line()
#dev.off()

# get differences between means
  diff_averages_each_timepoint = apply(times_df, 1, function(mytime_temp) {
    start_time_temp = as.numeric(mytime_temp[1])
    end_time_temp = as.numeric(mytime_temp[2])
    if(!is.na(start_time_temp) & !is.na(end_time_temp)) {
       mydf_train_dt_time_filt = mydf_train_dt[time_train >= start_time_temp & time_train <= end_time_temp]
    } else if(!is.na(start_time_temp) & is.na(end_time_temp)) {
      mydf_train_dt_time_filt = mydf_train_dt[time_train <= start_time_temp]
    } else {
      mydf_train_dt_time_filt = mydf_train_dt
    }
    mydf_train_dt_mean = mydf_train_dt_time_filt[,mean(abundance),by=condition_train]
    mydf_train_dt_sd = mydf_train_dt_time_filt[,sd(abundance),by=condition_train]
    mydf_train_dt_samplesize = mydf_train_dt_time_filt[,.N,by=condition_train]
    samplesize1 = mydf_train_dt_samplesize[1,2]$N
    samplesize2 = mydf_train_dt_samplesize[2,2]$N
    mean1 = mydf_train_dt_mean[1,2]$V1
    mean2 = mydf_train_dt_mean[2,2]$V1
    sd1 = mydf_train_dt_sd[1,2]$V1
    sd2 = mydf_train_dt_sd[2,2]$V1
    var1 = sd1 * sd1
    var2 = sd2 * sd2
    numerator = ((samplesize1-1) * var1) + ((samplesize2-1) * var2)
    denominator = samplesize1 + samplesize2 - 2
    num_over_den = numerator/denominator
    num_over_den = sqrt(num_over_den)
    parta = sqrt((1/samplesize1) + (1/samplesize2)) * num_over_den * 1.96 
    upperCI = (mean1 - mean2) + parta
    lowerCI = (mean1 - mean2) - parta
    time_vals = paste(start_time_temp,end_time_temp,sep="_")
    diff = mean1 - mean2
    total_samplesize = samplesize1 + samplesize2
    return(c(time_vals,diff,lowerCI,upperCI,total_samplesize))
  })
  diff_averages_each_timepoint = t(diff_averages_each_timepoint)
  colnames(diff_averages_each_timepoint) = c("time_group","mean_difference_in_abundnace","lowerCI","upperCI","sample_size")
  diff_averages_each_timepoint = as.data.frame(diff_averages_each_timepoint)
  diff_averages_each_timepoint$mean_difference_in_abundnace = as.numeric(diff_averages_each_timepoint$mean_difference_in_abundnace)
  diff_averages_each_timepoint$time_group = factor(diff_averages_each_timepoint$time_group,levels=diff_averages_each_timepoint$time_group)
  diff_averages_each_timepoint$lowerCI = as.numeric(diff_averages_each_timepoint$lowerCI)
  diff_averages_each_timepoint$upperCI = as.numeric(diff_averages_each_timepoint$upperCI)

  pdf(paste(most_sig_gene,"_diff_in_training_averages_over_time_groups.pdf",sep=""))
  diff_in_training_averages_over_time_groups_plot = ggplot(diff_averages_each_timepoint,aes(x=time_group,y=mean_difference_in_abundnace,group=1,label=sample_size)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=lowerCI,ymax=upperCI)) + geom_text(nudge_x = 0.2,nudge_y = 0.3)
  print(diff_in_training_averages_over_time_groups_plot)
  dev.off()
})
# lets average over subjects. make sure I did it correctly

#times_df = data.frame(c(92,""),c(183,""),c(183,365),c(365,""),c(365,548),c(548,""),c(548,730),c(730,""),c("",""))
#times_df = t(times_df)
#times_df = as.data.frame(times_df)
#train_subjects_each_timepoint = c("healthy_pre-MIAA-3month.train_subjects.txt",
#                                  "healthy_pre-MIAA-6month.train_subjects.txt",
#                                  "healthy_pre-MIAA-6month-12month.train_subjects.txt",
#                                  "healthy_pre-MIAA-12month.train_subjects.txt",
#                                  "healthy_pre-MIAA-12month-18month.train_subjects.txt",
#                                  "healthy_pre-MIAA-18month.train_subjects.txt",
#                                  "healthy_pre-MIAA-18month-24month.train_subjects.txt",
#                                  "healthy_pre-MIAA-24month.train_subjects.txt",
#                                  "healthy_pre-MIAA.train_subjects.txt")
#train_subjects_each_timepoint = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2",train_subjects_each_timepoint,sep="/")

#metadata = read.csv("teddy_metadata_20190821.csv")
#abundance_file = gene_to_file_mapping[match(most_sig_gene,gene_to_file_mapping$V1),2]
#abundance_file = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/",abundance_file,sep="")
#abundance_file_sample_level = fread(abundance_file,data.table=FALSE)
#abundance_file_sample_level = abundance_file_sample_level[,-1]
#rownames(abundance_file_sample_level) = abundance_file_sample_level[,1]
#abundance_file_sample_level = abundance_file_sample_level[,-1]
#abundance_file_sample_level_top_gene = abundance_file_sample_level[match(most_sig_gene,rownames(abundance_file_sample_level)),]

#all_folders_time_ordered = all_folders[c(7,8,9,2,3,4,5,6,1)]
#plots_validation = sapply(1:length(all_folders_time_ordered),function(index) {
#  time = times_df[index,]
#  start_time = as.numeric(time[1,1])
#  stop_time = as.numeric(time[1,2])
#  # get the train subjects for metadata
#  train_subjects_file_temp = train_subjects_each_timepoint[index]
#  train_subjects_temp = read.table(train_subjects_file_temp)
#  train_subjects_temp = train_subjects_temp$V1
#  metadata_temp = metadata[metadata$maskid%in%train_subjects_temp,]
  
#  controls = metadata_temp[is.na(metadata_temp$age_first_MIAA),]
#  cases = metadata_temp[!is.na(metadata_temp$age_first_MIAA) & metadata_temp$age_at_collection < metadata_temp$age_first_MIAA,]
#  metadata_temp = rbind(controls,cases)
#  if(!is.na(start_time) & !is.na(stop_time)) {
#    metadata_temp = metadata_temp[metadata_temp$age_at_collection >= start_time & metadata_temp$age_at_collection <= stop_time,]
#  } else if(!is.na(start_time) & is.na(stop_time)) {
#    metadata_temp = metadata_temp[metadata_temp$age_at_collection <= start_time,]
#  } 
#  # now find samples in both metadata and abundance
#  samples_metadata_abundance_temp = intersect(metadata_temp$Run,colnames(abundance_file_sample_level_top_gene))
#  metadata_temp_order = metadata_temp[match(samples_metadata_abundance_temp,metadata_temp$Run),]
#  metadata_temp_order_subject_level_condition = unique(metadata_temp_order[,c("MIAA_pos","maskid")])

#  abundance_file_sample_level_top_gene_temp = abundance_file_sample_level_top_gene[,match(samples_metadata_abundance_temp,colnames(abundance_file_sample_level_top_gene))]
#  abundance_subjects_ordered = metadata_temp_order[match(colnames(abundance_file_sample_level_top_gene_temp),metadata_temp_order$Run),"maskid"]
#  abundance_run_subject_df = data.frame(abundance=as.numeric(abundance_file_sample_level_top_gene_temp),run=colnames(abundance_file_sample_level_top_gene_temp),subject=abundance_subjects_ordered)
#  abundance_run_subject_dt = as.data.table(abundance_run_subject_df)
#  abundance_avgs = abundance_run_subject_dt[,mean(abundance),by=subject]
#  abundance_avgs = as.data.frame(abundance_avgs)
#  abundance_avgs$V1 = RankNorm(abundance_avgs$V1)
#  abundance_avgs_ordered = abundance_avgs[match(metadata_temp_order_subject_level_condition$maskid,abundance_avgs$subject),]
#  metadata_temp_order_subject_level_condition$abundance = abundance_avgs_ordered$V1
  #metadata_temp_order_subject_level_condition$MIAA_pos = as.factor(as.numeric(metadata_temp_order_subject_level_condition$MIAA_pos))
#  metadata_temp_order_subject_level_condition$MIAA_pos = as.numeric(metadata_temp_order_subject_level_condition$MIAA_pos)
#  # run lm
#  lm_res = lm(abundance ~ MIAA_pos, data=metadata_temp_order_subject_level_condition)
#  lm_res_coefs = summary(lm_res)$coeff["MIAA_pos",]
#  estimate = lm_res_coefs[1]
#  pval = lm_res_coefs[4]
#  # make plot of abundances
#  base_name = basename(train_subjects_file_temp)
#  base_name = gsub(".train_subjects.txt","",base_name)
#  header = paste(base_name,"\n","pvalue:",round(pval,4)," estimate",round(estimate,4),sep="")
#  pdf(paste(most_sig_gene,"_",base_name,"_validation.pdf",sep=""))
#  myplot = ggplot(metadata_temp_order_subject_level_condition,aes(x=as.factor(MIAA_pos),y=abundance)) + geom_boxplot() + geom_jitter() + ggtitle(header)
#  print(myplot)
#  dev.off()
#  return(lm_res_coefs)
#})
#colnames(plots_validation) = all_folders_time_ordered

```




```{r}
library(data.table)
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff")

myfiles = list.files(pattern="^healthy",recursive = TRUE)
myfiles = myfiles[grep(".rds$",myfiles)]

for (x in myfiles) {
  myrds = readRDS(x)
  testName = gsub("tenPerc_cutoff_output_full_association_output_adjusted.rds","",basename(x))
  myrds_pval_sorted = myrds[order(myrds$p.value),]
  myrds_coef_sorted = myrds[order(abs(myrds$estimate),decreasing=TRUE),]
  cdf_fun_estimate = ecdf(abs(myrds_coef_sorted$estimate))
  cdf_fun_pvalue = ecdf(myrds_pval_sorted$p.value)
  jpeg(paste("cdfs/",testName,"_cdf_pvalue_plots.jpg",sep=""))
  plot(cdf_fun_pvalue,main=paste(testName,":pvalue"))
  dev.off()
  jpeg(paste("cdfs/",testName,"_cdf_estimate_plots.jpg",sep=""))
  plot(cdf_fun_estimate,main=paste(testName,":estimate"))
  dev.off()
}


```

# make_CDF_plots.bash just runs the R code above

```{bash}

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

sbatch -c 1 -t 0-11:59 -p short --mem=20G scripts/make_CDF_plots.bash 
```

##Make CDF plot for only healthy_pre-t1d-all_HLA. get FDR 0.1 and sort by coefficient. get top 5%

```{r}
library(data.table)
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff")

myfiles = list.files(pattern="^healthy",recursive = TRUE)
myfiles = myfiles[grep(".rds$",myfiles)]

x = "healthy_pre-t1d-all_HLAtenPerc_cutoff_output/healthy_pre-t1d-all_HLAtenPerc_cutoff_output_full_association_output_adjusted.rds"

myrds = readRDS(x)
testName = gsub("tenPerc_cutoff_output_full_association_output_adjusted.rds","",basename(x))
myrds_pval_sorted = myrds[order(myrds$p.value),]
myrds_pval_sorted_sig = myrds_pval_sorted[myrds_pval_sorted$BH < 0.1,]
myrds_pval_sorted_sig = myrds_pval_sorted_sig[order(abs(myrds_pval_sorted_sig$estimate),decreasing = TRUE),]

#fivePerQuantile = quantile(abs(myrds_pval_sorted_sig$estimate),0.05)
ninetyfivePerQuantile = quantile(abs(myrds_pval_sorted_sig$estimate),0.95)
topfiveperce_estimate = myrds_pval_sorted_sig[abs(myrds_pval_sorted_sig$estimate)>ninetyfivePerQuantile,]

dim(topfiveperce_estimate) # 38695

cdf_fun = ecdf(abs(topfiveperce_estimate$estimate))
jpeg("cdfs/healthy_pre-t1d-all_HLA_FDRsig_top5percEstimat_cdf_pvalue_plots.jpg")
plot(cdf_fun,main=paste(testName,":estimate"))
dev.off()

write.table(topfiveperce_estimate,"healthy_pre-t1d-all_HLAtenPerc_cutoff_output/healthy_pre-t1d-all_HLAtenPerc_cutoff_output_full_association_output_adjusted_sigGenes_topFivePercORs.tsv",sep="\t",col.names=TRUE,row.names=FALSE,quote=FALSE)
```

#Do this on training data only

```{r}
library(data.table)
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff_training_only")

x = "healthy_pre-t1d-all_HLA_tenPerc_cutoff_output/healthy_pre-t1d-all_HLA_tenPerc_cutoff_output_full_association_output_adjusted.rds"

myrds = readRDS(x)
myrds_pval_sorted = myrds[order(myrds$p.value),]
myrds_pval_sorted_sig = myrds_pval_sorted[myrds_pval_sorted$BH < 0.1,] # 514072
myrds_pval_sorted_sig = myrds_pval_sorted_sig[order(abs(myrds_pval_sorted_sig$estimate),decreasing = TRUE),]

#fivePerQuantile = quantile(abs(myrds_pval_sorted_sig$estimate),0.05)
ninetyfivePerQuantile = quantile(abs(myrds_pval_sorted_sig$estimate),0.95)
topfiveperce_estimate = myrds_pval_sorted_sig[abs(myrds_pval_sorted_sig$estimate)>ninetyfivePerQuantile,]

dim(topfiveperce_estimate) # 25704

write.table(topfiveperce_estimate,"healthy_pre-t1d-all_HLA_tenPerc_cutoff_output/healthy_pre-t1d-all_HLAtenPerc_cutoff_output_full_association_output_adjusted_sigGenes_topFivePercORs.tsv",sep="\t",col.names=TRUE,row.names=FALSE,quote=FALSE)

```


```{r}
library(caret)
library(data.table)
library(reticulate)
library(pROC)
library(timeROC)
library(glmnet)
library(doMC)
library(survival)
library(purrr)
library(caTools)

use_condaenv("/home/sez10/miniconda3_2/envs/python_env")
source_python('/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/scripts/read_df.py')

#genes_to_analyze = read.table("my_coefs_to_test.txt")
#genes_to_analyze = genes_to_analyze[,1]

genes_to_analyze = c(FEFJCKDH_42967=0.005554533,BBAILGEE_73880=-0.019688489,GCMFKMGD_17694=0.015541747,IOOOGNCF_18481=-0.001427191,KLEMEKPG_29198=-0.006284288,CPOEEFPF_74752=0.008336380,LOFFELGB_01297=0.020594740,KGDGHEHL_00321=0.047641944,GJJCIKKG_40269=0.026036219,OFELPHDK_22706=0.020072298,ICMMPIOE_17102=0.014381039,CBBNJBJC_25006=0.002894255,JCFJEBFB_26978=-0.025314092,OMBPLDJI_26362=0.016961886,MBABLCOI_30227=-0.019769198,DCIJHKJF_35315=-0.009218531,PGNJIEIC_01694=0.032672666,EPBJHANO_15234=0.023654213,LHKGDGGP_16064=0.070167285,PGFCPDFG_32254=0.064270693,ENODKFAC_65694=0.013121533,OADGOEGN_18050=0.042148683,MBOCKNIE_14280=0.113949908,AILMFONN_03113=-0.011122537,BFJBMLID_32426=0.030544875)

genes_to_analyze = genes_to_analyze[order(abs(genes_to_analyze),decreasing = TRUE)]

test_abundance_data = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5filtered_transformed_abundance_test.csv"

train_abundance_data = "/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5filtered_transformed_abundance_train.csv"

test_abundance_data_df = readFile(test_abundance_data)
rownames(test_abundance_data_df) = test_abundance_data_df[,1]
test_abundance_data_df = test_abundance_data_df[,-1]

train_abundance_data_df = readFile(train_abundance_data)
rownames(train_abundance_data_df) = train_abundance_data_df[,1]
train_abundance_data_df = train_abundance_data_df[,-1]

test_abundance_data_df_sig = test_abundance_data_df[,names(genes_to_analyze)]

train_abundance_data_df_sig = train_abundance_data_df[,names(genes_to_analyze)]


test_abundance_data_df_sig_melt = reshape2::melt(as.matrix(test_abundance_data_df_sig))

train_abundance_data_df_sig_melt = reshape2::melt(as.matrix(train_abundance_data_df_sig))

train_metadata = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5_train_1_metadata_filtered_baseline_183.rds")

test_metadata = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5_test_1_metadata_filtered_baseline_183.rds")

train_abundance_data_df_sig_melt = cbind(train_abundance_data_df_sig_melt,condition=train_metadata[match(train_abundance_data_df_sig_melt[,1],rownames(train_metadata)),"getsCondition"])

test_abundance_data_df_sig_melt = cbind(test_abundance_data_df_sig_melt,condition=test_metadata[match(test_abundance_data_df_sig_melt[,1],rownames(test_metadata)),"getsCondition"])

test_abundance_data_df_sig_melt = cbind(test_abundance_data_df_sig_melt,split=rep("test",nrow(test_abundance_data_df_sig_melt)))

train_abundance_data_df_sig_melt = cbind(train_abundance_data_df_sig_melt,split=rep("train",nrow(train_abundance_data_df_sig_melt)))


all_abundance_data_df_sig_melt = rbind(test_abundance_data_df_sig_melt,train_abundance_data_df_sig_melt)
all_abundance_data_df_sig_melt$condition = as.factor(all_abundance_data_df_sig_melt$condition)

pdf("testing_boxplots.pdf")
for(x in 1:length(genes_to_analyze)) {
  chosen_gene = names(genes_to_analyze)[x]
  coef = genes_to_analyze[x]
  all_abundance_data_df_sig_melt_temp = all_abundance_data_df_sig_melt[all_abundance_data_df_sig_melt$Var2 == chosen_gene,]
  myplot = ggplot(all_abundance_data_df_sig_melt_temp,aes(x=split,y=value,fill=condition)) + geom_boxplot(outlier.shape=NA) + theme_classic() + geom_point(position=position_jitterdodge()) + ggtitle(paste(chosen_gene,":",coef,sep=""))
  print(myplot)
}
dev.off()


library(data.table)
abundance_data_raw = fread("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/az.csv",data.table=FALSE)
abundance_data_raw = abundance_data_raw[,-1]
rownames(abundance_data_raw) = abundance_data_raw[,1]
abundance_data_raw = abundance_data_raw[,-1]
abundance_data_raw = abundance_data_raw["FEFJCKDH_42967",]

mapping_data = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA.mapping.csv")

runs_to_keep = intersect(mapping_data[,2],colnames(abundance_data_raw))
abundance_data_raw = abundance_data_raw[,runs_to_keep]
mapping_data = mapping_data[match(runs_to_keep,mapping_data[,2]),]


colnames(abundance_data_raw) = mapping_data[match(colnames(abundance_data_raw),mapping_data[,2]),1]
abundance_data_raw_melt = reshape2::melt(as.matrix(abundance_data_raw))
abundance_data_raw_melt = as.data.table(abundance_data_raw_melt)
abundance_data_raw_melt_avg = abundance_data_raw_melt[,mean(value),by=.(Var1,Var2)]



train_metadata_filt = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5_train_1_metadata_filtered_baseline_183.rds")

test_metadata_filt = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5_test_1_metadata_filtered_baseline_183.rds")

train_metadata_nofilt = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5_train_1_metadata_filtered.rds")

test_metadata_nofilt = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_3/healthy_pre-sero-6month-all_HLA_proportion_train_0.5/healthy_pre-sero-6month-all_HLA_proportion_train_0.5_test_1_metadata_filtered.rds")

abundance_data_raw_melt_avg_train_meta_filt = abundance_data_raw_melt_avg[match(rownames(train_metadata_filt),abundance_data_raw_melt_avg$Var2),]

abundance_data_raw_melt_avg_train_meta = abundance_data_raw_melt_avg[match(train_metadata_nofilt$SubjectID,abundance_data_raw_melt_avg$Var2),]

sum(abundance_data_raw_melt_avg_train_meta_filt$V1>0)/nrow(abundance_data_raw_melt_avg_train_meta_filt) # 95% values greater than 0

sum(abundance_data_raw_melt_avg_train_meta$V1>0)/nrow(abundance_data_raw_melt_avg_train_meta) #96% of values greater than 0

abundance_data_raw_melt_avg_train_meta_filt = as.data.frame(abundance_data_raw_melt_avg_train_meta_filt)[,-1]

abundance_data_raw_melt_avg_train_meta = as.data.frame(abundance_data_raw_melt_avg_train_meta)[,-1]

write.csv(abundance_data_raw_melt_avg_train_meta_filt,"abundance_data_raw_melt_avg_train_meta_filt_FEFJCKDH_42967.csv",quote=FALSE,row.names=FALSE)

write.csv(abundance_data_raw_melt_avg_train_meta,"abundance_data_raw_melt_avg_train_meta_FEFJCKDH_42967.csv",quote=FALSE,row.names=FALSE)

```

```{python}
import sys
import pandas as pd
import numpy as np
from sklearn.preprocessing import QuantileTransformer
import datatable as dt

mydf = pd.read_csv("abundance_data_raw_melt_avg_train_meta_filt_FEFJCKDH_42967.csv")

mydf = mydf.drop(columns=['Var2'])

qt = QuantileTransformer(random_state=123,output_distribution='normal')

fit = qt.fit(mydf)

train_pd_transform = fit.transform(mydf)

train_transform_pd = pd.DataFrame(train_pd_transform,columns=mydf.columns.tolist(),index=mydf.index.tolist())

train_transform_pd.to_csv("testing_transformed.csv",index=True,header=True)
```

```{r}
testing_df = read.csv("testing_transformed.csv")

before_transform = read.csv("abundance_data_raw_melt_avg_train_meta_filt_FEFJCKDH_42967.csv")
before_transform$cat = "train"
testing_df$cat = "train"

pdf("testing_boxplot_transformed.pdf")
ggplot(testing_df,aes(x=cat,y=V1)) +geom_boxplot(outlier.shape=NA) + theme_classic() + geom_point()
dev.off()

pdf("testing_boxplot_nottransformed.pdf")
ggplot(before_transform,aes(x=cat,y=V1)) +geom_boxplot(outlier.shape=NA) + theme_classic() + geom_point()
dev.off()

before_transform_log = before_transform 

before_transform_log$V1 = log(before_transform_log$V1 + min(before_transform_log$V1[before_transform_log$V1>0]))

pdf("testing_boxplot_log_transformed.pdf")
ggplot(before_transform_log,aes(x=cat,y=V1)) +geom_boxplot(outlier.shape=NA) + theme_classic() + geom_point()
dev.off()


before_transform
library(RNOmni)

before_transform_ranknorm = before_transform

before_transform_ranknorm$V1 = RankNorm(before_transform_ranknorm$V1)

pdf("testing_boxplot_rankNorm.pdf")
ggplot(before_transform_ranknorm,aes(x=cat,y=V1)) +geom_boxplot(outlier.shape=NA) + theme_classic() + geom_point()
dev.off()

RankNorm_fit = function(u,k=0.375,ties.method="average") {
  if (!is.vector(u)) {
    stop("A numeric vector is expected for u.")
  }
  if ((k < 0) || (k > 0.5)) {
    stop("Select the offset within the interval (0,0.5).")
  }
  if (sum(is.na(u)) > 0) {
    stop("Please exclude observations with missing measurements.")
  }
  n <- length(u)
  r <- rank(u, ties.method = ties.method)
  out <- stats::qnorm((r - k)/(n - 2 * k + 1))
}

```






#Next step is to get the normalized gene abundance data for our genes and do lasso regression

```{r}
library(glmnet)
library(ggplot2)
sigGenes = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff/healthy_pre-t1d-all_HLAtenPerc_cutoff_output/healthy_pre-t1d-all_HLAtenPerc_cutoff_output_full_association_output_adjusted_sigGenes_topFivePercORs.tsv",sep="\t",header=TRUE)

sigGenes = sigGenes$feature

gene_to_file_mapping = read.table("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE)
gene_to_file_mapping_sig = gene_to_file_mapping[match(sigGenes,gene_to_file_mapping$V1),]

suffixes = gene_to_file_mapping_sig[,2]
suffixes = gsub(".csv","",suffixes)
suffixes = unique(suffixes)

abundance_files = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/healthy_pre-t1d-all_HLA/healthy_pre-t1d-all_HLA_",suffixes,".rds",sep="")

abundances_of_sig_genes = lapply(abundance_files, function(x) {
  print(x)
  abundance_temp = readRDS(x)
  abundance_temp_sig = abundance_temp[,colnames(abundance_temp)%in%sigGenes,drop=FALSE]
  return(abundance_temp_sig)
})

abundances_of_sig_genes_df = do.call("cbind",abundances_of_sig_genes)
saveRDS(abundances_of_sig_genes_df,"abundances_of_sig_genes_df_health_pre-t1d-all_HLA.rds")

abundances_of_sig_genes_df = readRDS("abundances_of_sig_genes_df_health_pre-t1d-all_HLA.rds")
# for each gene add minimum non zero and log transform
abundances_of_sig_genes_df_log = apply(abundances_of_sig_genes_df,2, function(x) {
  min_val = min(x[x>0])
  x = x + min_val
  x = log(x)
  return(x)
})

# spearman correlations
#cor_matrix = cor(abundances_of_sig_genes_df_log,method="spearman")
# now make heatmap
#library(pheatmap)
#pdf("sig_genes_correlation_matrix_pret1d_vs_healthy.pdf")
#pheatmap(cor_matrix,show_rownames=FALSE,show_colnames=FALSE)
#dev.off()

# get metadata
abundance_temp = readRDS(abundance_files[1])
rownames(abundances_of_sig_genes_df_log) = abundance_temp$SubjectID

metadata = readRDS("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/healthy_pre-t1d-all_HLA/healthy_pre-t1d-all_HLA_1_metadata_filtered.rds")

metadata = metadata[match(rownames(abundances_of_sig_genes_df_log),metadata$SubjectID),]

# first thing to do is to find the correct lambda penalty
set.seed(123)
lambda_seq <- 10^seq(2, -2, by = -.1)

cv_output <- cv.glmnet(abundances_of_sig_genes_df_log, metadata$condition,
                       alpha = 1, lambda = lambda_seq, 
                       nfolds = 5,family="binomial")

# identifying best lamda
best_lam <- cv_output$lambda.min

lasso_best <- glmnet(abundances_of_sig_genes_df_log, metadata$condition, alpha = 1, lambda = best_lam,family="binomial")
# get non 0 variables

best_vars = coef(lasso_best)

myVars = best_vars[best_vars[,1]>0,]

saveRDS(myVars,"/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/lasso_coefficients.rds")

myVars = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/lasso_coefficients.rds")

lasso_sig_genes = names(myVars)[-1]

pdf("healthy_pre-t1d-all_HLA_lasso_sig_genes.pdf")
for(topgene in lasso_sig_genes) {
  temp_df = data.frame(value=abundances_of_sig_genes_df_log[,topgene],condition=as.factor(metadata$condition))
  myplot = ggplot(temp_df,aes(x=condition,y=value)) + geom_boxplot() + geom_jitter() + ggtitle(topgene)
  print(myplot)
}
dev.off()

# make volcano plot illustrating genes that were in top 34,000 and then the genes that were lasso sig

myfiles = list.files("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff",pattern="^healthy",recursive = TRUE,full.names = TRUE)
myfiles = myfiles[grep(".rds$",myfiles)]

x = myfiles[grep("healthy_pre-t1d-all_HLAtenPerc_cutoff_output_full_association_output_adjusted.rds",myfiles)]

myrds = readRDS(x)
myrds$col = "not-sig"
myrds = as.data.frame(myrds)
myrds$col[myrds$feature%in%sigGenes] = "sig"
myrds$col[myrds$feature%in%lasso_sig_genes] = "lasso-sig"
myrds$col = as.factor(myrds$col)
order_list = rep(1,length(myrds$feature))
order_list[myrds$feature%in%lasso_sig_genes] = 2
myrds$plot_order = order_list
myrds$logPval = -log10(myrds$BH)
volcano_plot = ggplot(myrds,aes(x=estimate,y=logPval,order=plot_order,color=col)) + geom_point() + geom_hline(yintercept=-log10(0.1), color = "red") + theme_classic() + theme(legend.text = element_text(size=20),legend.title=element_text(size=20)) + labs(y = "", x = "",color="Gene Status") #+ #guides(fill=guide_legend(title="Gene Status"))

jpeg("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/sig_gene_volcano_plot.jpg",width=600)
print(volcano_plot)
#plot(myrds$estimate,-log10(myrds$BH),col=myrds$col,pch=pch_list)
#abline(h=-log10(0.1),col="red")
dev.off()

library(caret)

# need to do some cross validation

#folds <- cut(seq(1,nrow(abundances_of_sig_genes_df_log)),breaks = 5,labels = FALSE)

#set.seed(3456)
#trainIndex <- createDataPartition(metadata$condition, p = .8, 
#                                  list = FALSE, 
#                                  times = 5)

abundances_of_sig_genes_df_log_lasso_sig = abundances_of_sig_genes_df_log[,lasso_sig_genes]

library(dplyr)
#for (k in (1:5)) {
#  print(k)
#  trainIndexes = trainIndex[,k]
#  testData <- abundances_of_sig_genes_df_log_lasso_sig[-trainIndexes, ]
#  trainData <- abundances_of_sig_genes_df_log_lasso_sig[trainIndexes, ]
#  condition_test = metadata$condition[-trainIndexes]
#  condition_train = metadata$condition[trainIndexes]
# trainData = cbind(trainData,condition=condition_train)
#  testData = cbind(testData,condition=condition_test)
#  #trainData$condition = condition_train
#  #testData$condition = condition_test
#  rf_fit_teddy <- train(as.factor(condition) ~ ., data = trainData)
#  # predict outcomes
#  pred_outcome <- predict(rf_fit_teddy, testData[,-match("condition",colnames(testData))])
#  print(confusionMatrix(pred_outcome, as.factor(testData[,"condition"])))
#}

# now lets add in grs2, family history, and number of autoantibodies before T1D onset and do predictions

# read in sample level metadata
sample_metadata = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv")
# onluy include subjects we have data for
sample_metadata = sample_metadata[sample_metadata$m138_maskid%in%rownames(abundances_of_sig_genes_df_log_lasso_sig),]
# now only include samples that have not gotten T1D yet.
samps_to_keep = apply(sample_metadata, 1, function(myrow) {
  if(!is.na(myrow["age_t1d"])) {
    isBeforeT1D = as.numeric(myrow["age_at_collection"]) < as.numeric(myrow["age_t1d"])
  } else {
    # if age_t1d is NA then they never get T1D
    isBeforeT1D = TRUE
  }
  return(isBeforeT1D)
})
sample_metadata = sample_metadata[samps_to_keep,]
# now for each subject get number of autoantibodies, family history, and grs2 score
grs2_fdr_num_autoantibodies_per_subj = lapply(split(sample_metadata,sample_metadata$m138_maskid), function(df_temp) {
  grs2_temp = unique(df_temp$GRS_score)
  family_hist = as.numeric(unique(df_temp$fdr))
  # since we are doing average of all microbes, take oldest sample
  oldest_samp = unlist(df_temp[which.max(as.numeric(df_temp$age_at_collection)),])
  oldest_age = as.numeric(oldest_samp["age_at_collection"])
  had_MIAA <- oldest_age >= as.numeric(oldest_samp["age_first_MIAA"])
  if(is.na(had_MIAA)) {
    had_MIAA = FALSE
  }
  had_GAD <- oldest_age >= as.numeric(oldest_samp["age_first_GAD"])
  if(is.na(had_GAD)) {
    had_GAD = FALSE
  }
  had_IA2A <- oldest_age >= as.numeric(oldest_samp["age_first_IA2A"])
  if(is.na(had_IA2A)) {
    had_IA2A = FALSE
  }
  num_autoantibodies_had = sum(as.numeric(c(had_MIAA,had_GAD,had_IA2A)))
  return(c(grs2=grs2_temp,fdr=family_hist,autoantibody_num=num_autoantibodies_had))
})
grs2_fdr_num_autoantibodies_per_subj_df = do.call("rbind",grs2_fdr_num_autoantibodies_per_subj)

grs2_fdr_num_autoantibodies_per_subj_df_ordered = grs2_fdr_num_autoantibodies_per_subj_df[match(rownames(abundances_of_sig_genes_df_log_lasso_sig),rownames(grs2_fdr_num_autoantibodies_per_subj_df)),]
grs2_fdr_num_autoantibodies_per_subj_df_ordered = as.data.frame(grs2_fdr_num_autoantibodies_per_subj_df_ordered)
grs2_fdr_num_autoantibodies_per_subj_df_ordered$fdr = as.factor(grs2_fdr_num_autoantibodies_per_subj_df_ordered$fdr)
grs2_fdr_num_autoantibodies_per_subj_df_ordered$autoantibody_num = as.factor(grs2_fdr_num_autoantibodies_per_subj_df_ordered$autoantibody_num)
grs2_fdr_num_autoantibodies_per_subj_df_ordered$condition = metadata$condition
#grs2_fdr_num_autoantibodies_per_subj_df_ordered$condition = as.factor(grs2_fdr_num_autoantibodies_per_subj_df_ordered$condition)
dummies <- dummyVars(condition ~ ., data = grs2_fdr_num_autoantibodies_per_subj_df_ordered)
grs2_fdr_num_autoantibodies_per_subj_df_ordered_dummies = predict(dummies, newdata = grs2_fdr_num_autoantibodies_per_subj_df_ordered)
grs2_fdr_num_autoantibodies_per_subj_df_ordered_dummies = as.data.frame(grs2_fdr_num_autoantibodies_per_subj_df_ordered_dummies)
grs2_fdr_num_autoantibodies_per_subj_df_ordered_dummies$grs2 = scale(grs2_fdr_num_autoantibodies_per_subj_df_ordered_dummies$grs2,center = TRUE,scale=TRUE)

abundances_of_sig_genes_df_log_lasso_sig_scaled = scale(abundances_of_sig_genes_df_log_lasso_sig,center=TRUE,scale=TRUE)

abundances_of_sig_genes_df_log_lasso_sig_withMetadata = cbind(abundances_of_sig_genes_df_log_lasso_sig_scaled,grs2_fdr_num_autoantibodies_per_subj_df_ordered_dummies)
missing_indexes = which(is.na(abundances_of_sig_genes_df_log_lasso_sig_withMetadata$grs2))
metadata_noMissing = metadata[-missing_indexes,]
# remove columns with missing GRS2 scores
abundances_of_sig_genes_df_log_lasso_sig_withMetadata = abundances_of_sig_genes_df_log_lasso_sig_withMetadata[-missing_indexes,]

all.equal(metadata_noMissing$SubjectID,rownames(abundances_of_sig_genes_df_log_lasso_sig_withMetadata))

set.seed(3456)
trainIndex <- createDataPartition(metadata_noMissing$condition, p = 0.75, 
                                  list = FALSE, 
                                  times = 5)

library(pROC)

library(dplyr)
roc_list = c()
f1score_list = c()
confuMat_list = list()
var_imp_df = matrix(0,ncol=5,nrow=ncol(abundances_of_sig_genes_df_log_lasso_sig_withMetadata))
rownames(var_imp_df) = colnames(abundances_of_sig_genes_df_log_lasso_sig_withMetadata)
for (k in (1:5)) {
  print(k)
  trainIndexes = trainIndex[,k]
  testData <- abundances_of_sig_genes_df_log_lasso_sig_withMetadata[-trainIndexes, ]
  trainData <- abundances_of_sig_genes_df_log_lasso_sig_withMetadata[trainIndexes, ]
  condition_test = metadata_noMissing$condition[-trainIndexes]
  condition_train = metadata_noMissing$condition[trainIndexes]
  down_train <- downSample(x = trainData,
                         y = as.factor(condition_train))
  
  #trainData = cbind(trainData,condition=condition_train)
  testData = cbind(testData,condition=condition_test)
  fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5)

  rf_fit_teddy <- train(Class ~ ., data = down_train,trControl = fitControl,method="rf")
  var_imp_temp = varImp(rf_fit_teddy)
  var_imp_temp = var_imp_temp$importance
  var_imp_temp = var_imp_temp[order(var_imp_temp,decreasing = TRUE),,drop=FALSE]
  var_imp_df[,k] = var_imp_temp[rownames(var_imp_df),]
  # predict outcomes
  testData$grs2 = as.numeric(testData$grs2)
  pred_outcome <- predict(rf_fit_teddy, testData[,-match("condition",colnames(testData))],type="prob")
  
  roc_elem = roc(response=testData[,"condition"],predictor=pred_outcome[,2],levels=c("0","1"),auc=TRUE,ci=TRUE)
  roc_list[[k]] = roc_elem
  
  pred_outcome_bin = as.factor(as.numeric(pred_outcome[,2]> 0.5))
  
  confMat = confusionMatrix(pred_outcome_bin, as.factor(testData[,"condition"]))
  F1_scores = confMat$byClass["F1"]
  f1score_list <- c(f1score_list,F1_scores)
  confuMat_list[[k]] = confMat
}

color_list = c("blue","green","red","orange","black")
auc_values = sapply(roc_list,function(x) as.numeric(x$auc))

pdf("roc_curve_lasso_sig_genes_metadata.pdf")
plot(roc_list[[1]],col=color_list[1])
for(x in 2:length(roc_list)) {
  lines(roc_list[[x]],col=color_list[x])
}
legend("bottomright",legend=paste("AUC:",round(auc_values,2),sep=""),fill=color_list)
dev.off()

# now do the same thing with only metadata

fdr_grs2_autoantibodyOnly = abundances_of_sig_genes_df_log_lasso_sig_withMetadata[,c("grs2","fdr.0","fdr.1","autoantibody_num.0","autoantibody_num.1","autoantibody_num.2","autoantibody_num.3")]

roc_list2 = c()
confuMat_list2 = list()
f1_score_list2 = c()
for (k in (1:5)) {
  print(k)
  trainIndexes = trainIndex[,k]
  testData <- fdr_grs2_autoantibodyOnly[-trainIndexes, ]
  trainData <- fdr_grs2_autoantibodyOnly[trainIndexes, ]
  condition_test = metadata_noMissing$condition[-trainIndexes]
  condition_train = metadata_noMissing$condition[trainIndexes]
  down_train <- downSample(x = trainData,
                        y = as.factor(condition_train))

  trainData = cbind(trainData,condition=condition_train)
  testData = cbind(testData,condition=condition_test)
  fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5)

  rf_fit_teddy <- train(Class ~ ., data = down_train,trControl = fitControl,method="rf")
  # predict outcomes
  testData$grs2 = as.numeric(testData$grs2)
  pred_outcome <- predict(rf_fit_teddy, testData[,-match("condition",colnames(testData))],type="prob")
  
  roc_elem = roc(testData[,"condition"],pred_outcome[,2],levels=c("0","1"),auc=TRUE,ci=TRUE)
  roc_list2[[k]] = roc_elem
  
  #confMat = confusionMatrix(pred_outcome, as.factor(testData[,"condition"]))
  #F1_scores = confMat$byClass["F1"]
  #f1_score_list2 <- c(f1_score_list2,F1_scores)
  #confuMat_list2[[k]] = confMat
}

color_list = c("blue","green","red","orange","black")
auc_values2 = sapply(roc_list2,function(x) as.numeric(x$auc))

pdf("roc_curve_metadata_only.pdf")
plot(roc_list2[[1]],col=color_list[1])
for(x in 2:length(roc_list2)) {
  lines(roc_list2[[x]],col=color_list[x])
}
legend("bottomright",legend=paste("AUC:",round(auc_values2,2),sep=""),fill=color_list)
dev.off()

newAuc_list = c()
color_list2 = rep(color_list,each=2)
pdf("roc_curve_metadata_only_vs_with_lassoSig.pdf")
plot(roc_list[[1]],col=color_list[1])
newAuc_list = c(newAuc_list,auc_values[1])
lines(roc_list2[[1]],col=color_list[1],lty=2)
newAuc_list = c(newAuc_list,auc_values2[1])
for(x in 2:length(roc_list2)) {
  lines(roc_list[[x]],col=color_list[x])
  newAuc_list = c(newAuc_list,auc_values[x])
  lines(roc_list2[[x]],col=color_list[x],lty=2)
  newAuc_list = c(newAuc_list,auc_values2[x])
}
legend("bottomright",legend=paste("AUC:",round(newAuc_list,2),sep=""),fill=color_list2,lty=rep(c(1,2),5))
dev.off()





#trainIndex2 <- createDataPartition(metadata$condition, p = .8, 
#                                  list = FALSE, 
#                                  times = 1)

#trainIndexes = trainIndex2[,1]
#testData <- abundances_of_sig_genes_df_log_lasso_sig[-trainIndexes, ]
#trainData <- abundances_of_sig_genes_df_log_lasso_sig[trainIndexes, ]
#condition_test = metadata$condition[-trainIndexes]
#condition_train = metadata$condition[trainIndexes]
#trainData = cbind(trainData,condition=condition_train)
#testData = cbind(testData,condition=condition_test)
#rf_fit_teddy <- train(as.factor(condition) ~ ., data = trainData)
#pred_outcome <- predict(rf_fit_teddy, testData[,-match("condition",colnames(testData))])
#print(confusionMatrix(pred_outcome, as.factor(testData[,"condition"])))

# lets see if we scramble lables and redo analysis how often by chance to we get coefficients from lasso sig genes higher than what we got with real data

boot_coeficient_list = list()

set.seed(123)
for (boot_num in 1:50) {
  print(boot_num)
  condition_boot = sample(metadata$condition)
  #cv_output_boot <- cv.glmnet(abundances_of_sig_genes_df_log, condition_boot,
  cv_output_boot <- cv.glmnet(abundances_of_sig_genes_df_log_lasso_sig, condition_boot,
                       alpha = 1, lambda = lambda_seq, 
                       nfolds = 5,family="binomial")
  # identifying best lamda
  best_lam_boot <- cv_output_boot$lambda.min
  lasso_best_boot <- glmnet(abundances_of_sig_genes_df_log_lasso_sig, condition_boot, alpha = 1, lambda = best_lam_boot,family="binomial")
  best_vars_boot = coef(lasso_best_boot)
  boot_coeficient_list[[boot_num]] = best_vars_boot[lasso_sig_genes,]
}

# get non 0 variables

# check to make sure all the names are in the same order

names_temp = names(boot_coeficient_list[[1]])
table(unlist(lapply(boot_coeficient_list, function(x) all.equal(names(x),names_temp))))
# conver to matrix
boot_coeficient_df = do.call("cbind",boot_coeficient_list)

myVars_nointercept = myVars[-1]
all.equal(names(myVars_nointercept),rownames(boot_coeficient_df))
# calculate percent of random samples that have greater coeficient than other values
boot_pvals = c()
for(x in 1:nrow(boot_coeficient_df)) {
  true_coef = myVars_nointercept[x]
  pval_boot = sum(abs(boot_coeficient_df[x,]) > abs(true_coef))/ncol(boot_coeficient_df)
  boot_pvals[x] = pval_boot
}
sum(boot_pvals<0.05) # in all 61 genes proportion of coefficients greater than real data is less than 5% of samples. 60 out of 61 genes robust
pdf("lasso_bootstrap_pvals_v2.pdf")
hist(boot_pvals,xlim=c(0,0.1))
abline(v=0.05,col="red")
dev.off()

# now do a different bootstrap method where I am going to take a random 50% of 0s and 50% of T1D samples 

T1Dsamples = which(metadata$condition==1)
nonT1Dsamples = which(metadata$condition==0)

boot_coeficient_list2 = list()

set.seed(123)
for (boot_num in 1:50) {
  print(boot_num)
  
  T1Dsamples_boot = sample(T1Dsamples,length(T1Dsamples)/2)
  nonT1Dsamples_boot = sample(nonT1Dsamples,length(nonT1Dsamples)/2)
  chosen_samples_boot = sort(c(T1Dsamples_boot,nonT1Dsamples_boot))
  abundances_of_sig_genes_df_log_boot_samps = abundances_of_sig_genes_df_log_lasso_sig[chosen_samples_boot,]
  condition_boot = metadata$condition[chosen_samples_boot]

  cv_output_boot <- cv.glmnet(abundances_of_sig_genes_df_log_boot_samps, condition_boot,
                       alpha = 1, lambda = lambda_seq, 
                       nfolds = 5,family="binomial")
  # identifying best lamda
  best_lam_boot <- cv_output_boot$lambda.min
  lasso_best_boot <- glmnet(abundances_of_sig_genes_df_log_boot_samps, condition_boot, alpha = 1, lambda = best_lam_boot,family="binomial")
  best_vars_boot = coef(lasso_best_boot)
  boot_coeficient_list2[[boot_num]] = best_vars_boot[lasso_sig_genes,] > 0
}
# now we hopefully see that in more than 95% of cases most of the genes have a coef greater than 0
boot_coeficient_df2 = do.call("cbind",boot_coeficient_list2)
table(rowSums(boot_coeficient_df2)/ncol(boot_coeficient_df2) > 0.95) # none meet this criteria
table(rowSums(boot_coeficient_df2)/ncol(boot_coeficient_df2) > 0.50) # 25 genes get over 50% the same

proportion_coefs_gt_one = rowSums(boot_coeficient_df2)/ncol(boot_coeficient_df2)

pdf("proportion_coef_gt_0_lasso_boot.pdf")
hist(proportion_coefs_gt_one,xlim=c(0,1))
abline(v=0.95,col="red")
dev.off()

## now we are going to measure the variance of each gene at different timepoints

gene_to_file_mapping_lasso_genes = gene_to_file_mapping[match(lasso_sig_genes,gene_to_file_mapping[,1]),]
lasso_sig_gene_locations = unique(gene_to_file_mapping_lasso_genes[,2])

library(data.table)
lasso_sig_gene_abundances_all_samples = lapply(lasso_sig_gene_locations, function(x) {
  print(x)
  abundance_temp = fread(paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data",x,sep="/"),sep=",",header=TRUE)
  abundance_temp_sig = abundance_temp[genename%in%lasso_sig_genes,]
  return(abundance_temp_sig)
})

lasso_sig_gene_abundances_all_samples_dt = rbindlist(lasso_sig_gene_abundances_all_samples)
lasso_sig_gene_abundances_all_samples_df = as.data.frame(lasso_sig_gene_abundances_all_samples_dt)
rownames(lasso_sig_gene_abundances_all_samples_df) = lasso_sig_gene_abundances_all_samples_df[,2]
lasso_sig_gene_abundances_all_samples_df = lasso_sig_gene_abundances_all_samples_df[,-c(1,2)]
write.csv(lasso_sig_gene_abundances_all_samples_df,file="lasso_sig_gene_abundances_all_samples_df.csv")

#lasso_sig_gene_abundances_all_samples_df = read.csv("lasso_sig_gene_abundances_all_samples_df.csv",header=TRUE,row.names=1)

lasso_sig_gene_abundances_all_samples_df_log = apply(lasso_sig_gene_abundances_all_samples_df,1, function(x) {
  min_val = min(x[x>0])
  x = x + min_val
  x = log(x)
  return(x)
})
lasso_sig_gene_abundances_all_samples_df_log = t(lasso_sig_gene_abundances_all_samples_df_log)

# load in metadata for all samples
tedd_metadata_allsamples = read.csv("teddy_metadata_20190821.csv")
tedd_metadata_allsamples = tedd_metadata_allsamples[tedd_metadata_allsamples$Run%in%colnames(lasso_sig_gene_abundances_all_samples_df_log),]

# make abundance of each gene as a function of time
pdf("lasso_abundances_function_time.pdf")
sapply(rownames(lasso_sig_gene_abundances_all_samples_df_log), function(geneName) {
  gene_temp_abundance = lasso_sig_gene_abundances_all_samples_df_log[geneName,,drop=FALSE]
  samples_Ihavemetadatafor = intersect(colnames(gene_temp_abundance),tedd_metadata_allsamples$Run)
  gene_temp_abundance = gene_temp_abundance[,samples_Ihavemetadatafor]
  metadata_matched = tedd_metadata_allsamples[match(samples_Ihavemetadatafor,tedd_metadata_allsamples$Run),]
  # remove people who already have T1D
  currently_hasT1D = metadata_matched$age_at_collection >= metadata_matched$age_t1d
  metadata_matched_t1d_status = metadata_matched$t1d_sero_control
  abundance_temp_dataframe = data.frame(abundance=as.numeric(gene_temp_abundance),age=metadata_matched$age_at_collection,t1d_status=metadata_matched_t1d_status,currently_hasT1D)
  # remove people who already have T1D
  abundance_temp_dataframe = abundance_temp_dataframe[-which(abundance_temp_dataframe$currently_hasT1D == TRUE),]
  myplot = ggplot(abundance_temp_dataframe,aes(x=age,y=abundance,color=t1d_status)) + geom_point() + geom_smooth(method="loess") + ggtitle(label=geneName)
  print(myplot)
})
dev.off()

### now do the predictions as we did above but do for different time points. e.g. children younger than 3 months old, 6 months, 12 months, 18 months ... etc


sample_metadata = read.csv("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv")
# only include samples that have not gotten T1D yet
#samps_to_keep = apply(sample_metadata, 1, function(myrow) {
#  if(!is.na(myrow["age_t1d"])) {
#    isBeforeT1D = as.numeric(myrow["age_at_collection"]) < as.numeric(myrow["age_t1d"])
#  } else {
#    # if age_t1d is NA then they never get T1D
#    isBeforeT1D = TRUE
#  }
#  return(isBeforeT1D)
#})
#sample_metadata = sample_metadata[samps_to_keep,]
# now get samples we have metadaa and gene abundance data for
lasso_sig_gene_abundances_all_samples_df = read.csv("lasso_sig_gene_abundances_all_samples_df.csv",header=TRUE,row.names=1)

metadata_abundance_samples= intersect(colnames(lasso_sig_gene_abundances_all_samples_df),sample_metadata$Run)

lasso_sig_gene_abundances_all_samples_df_intersection = lasso_sig_gene_abundances_all_samples_df[,metadata_abundance_samples]
sample_metadata_ordered = sample_metadata[match(metadata_abundance_samples,sample_metadata$Run),]

# now make function to do the prediction
library(caret)
days_vec = c(92, 183, 365, 548, 730,912,1095)

predictions_diff_times = function(days, abundance_data_temp,metadata_temp) {
  # only keep samples below number of days
  young_enough = metadata_temp$age_at_collection <= days
  # also remove samples that have had T1D
  not_have_T1D = metadata_temp$T1D_Outcome != "After"
  
  metadata_temp = metadata_temp[young_enough & not_have_T1D,]
  # also remove samples that have had T1D
  
  abundance_data_temp = abundance_data_temp[,metadata_temp$Run]
  # now get the grs2, fdr and numberr of autoantibodies per subject
  grs2_fdr_num_autoantibodies_per_subj = lapply(split(metadata_temp,metadata_temp$m138_maskid), function(df_temp) {
    grs2_temp = unique(df_temp$GRS_score)
    family_hist = as.numeric(unique(df_temp$fdr))
    willGetT1D <- unique(df_temp$T1D_Outcome)
    willGetT1D = as.numeric(willGetT1D == "Before")
    oldest_samp = unlist(df_temp[which.max(as.numeric(df_temp$age_at_collection)),])
    oldest_age = as.numeric(oldest_samp["age_at_collection"])
    
    had_MIAA <- oldest_age >= as.numeric(oldest_samp["age_first_MIAA"])
    if(is.na(had_MIAA)) {
      had_MIAA = FALSE
    }
    had_GAD <- oldest_age >= as.numeric(oldest_samp["age_first_GAD"])
    if(is.na(had_GAD)) {
      had_GAD = FALSE
    }
    had_IA2A <- oldest_age >= as.numeric(oldest_samp["age_first_IA2A"])
    if(is.na(had_IA2A)) {
      had_IA2A = FALSE
    }
    num_autoantibodies_had = sum(as.numeric(c(had_MIAA,had_GAD,had_IA2A)))
    return(c(grs2=grs2_temp,fdr=family_hist,autoantibody_num=num_autoantibodies_had,T1D_status=willGetT1D))
  })
  grs2_fdr_num_autoantibodies_per_subj_df = do.call("rbind",grs2_fdr_num_autoantibodies_per_subj)
  grs2_fdr_num_autoantibodies_per_subj_df = as.data.frame(grs2_fdr_num_autoantibodies_per_subj_df)
  grs2_fdr_num_autoantibodies_per_subj_df$grs2 = as.numeric(grs2_fdr_num_autoantibodies_per_subj_df$grs2)
  grs2_fdr_num_autoantibodies_per_subj_df$fdr = as.numeric(grs2_fdr_num_autoantibodies_per_subj_df$fdr)
  grs2_fdr_num_autoantibodies_per_subj_df$autoantibody_num = as.numeric(grs2_fdr_num_autoantibodies_per_subj_df$autoantibody_num)

  # now do the same thing but average the abundances for each subject
  abundance_data_temp_subject = lapply(split(metadata_temp,metadata_temp$m138_maskid), function(df_temp) {
    runs_temp = df_temp$Run
    return(rowMeans(abundance_data_temp[,runs_temp,drop=FALSE]))
  })
  abundance_data_temp_subject_df = do.call("rbind",abundance_data_temp_subject)
  
  # filter out genes not abundant in at least 10% of samples
  genes_to_keep = (colSums(abundance_data_temp_subject_df>0)/nrow(abundance_data_temp_subject_df))*100 >=10
  abundance_data_temp_subject_df = abundance_data_temp_subject_df[,genes_to_keep]
  
  all.equal(rownames(abundance_data_temp_subject_df),rownames(grs2_fdr_num_autoantibodies_per_subj_df))
  # now add dummy variables 
  if(length(unique(grs2_fdr_num_autoantibodies_per_subj_df$fdr)) >1) {
    grs2_fdr_num_autoantibodies_per_subj_df$fdr = as.factor(grs2_fdr_num_autoantibodies_per_subj_df$fdr)
  }
  if(length(unique(grs2_fdr_num_autoantibodies_per_subj_df$autoantibody_num)) >1) {
    grs2_fdr_num_autoantibodies_per_subj_df$autoantibody_num = as.factor(grs2_fdr_num_autoantibodies_per_subj_df$autoantibody_num)
  }
  dummies <- dummyVars(T1D_status ~ ., data = grs2_fdr_num_autoantibodies_per_subj_df)
  
  t1D_status_temp = grs2_fdr_num_autoantibodies_per_subj_df$T1D_status
  
  grs2_fdr_num_autoantibodies_per_subj_df_dummies = predict(dummies, newdata = grs2_fdr_num_autoantibodies_per_subj_df)
  grs2_fdr_num_autoantibodies_per_subj_df_dummies = as.data.frame(grs2_fdr_num_autoantibodies_per_subj_df_dummies)
  grs2_fdr_num_autoantibodies_per_subj_df_dummies$grs2 = scale(grs2_fdr_num_autoantibodies_per_subj_df_dummies$grs2,center = TRUE,scale=TRUE)
  abundance_data_temp_subject_df = scale(abundance_data_temp_subject_df,center=TRUE,scale=TRUE)
  abundance_data_temp_subject_withMetadata = cbind(abundance_data_temp_subject_df,grs2_fdr_num_autoantibodies_per_subj_df_dummies)
  missing_indexes = which(is.na(abundance_data_temp_subject_withMetadata$grs2))
  if(length(missing_indexes)>0) {
    abundance_data_temp_subject_withMetadata = abundance_data_temp_subject_withMetadata[-missing_indexes,]
    t1D_status_temp = t1D_status_temp[-missing_indexes]
  }
  
  # now its time to do predictions!!
  
  
  set.seed(3456)
  trainIndex <- createDataPartition(as.factor(t1D_status_temp), p = 0.75, 
                                  list = FALSE, 
                                  times = 5)

  library(pROC)

  library(dplyr)
  roc_list = c()
  roc_list2 = c()
  f1score_list = c()
  f1score_list2 = c()
  confuMat_list = list()
  confuMat_list2 = list()
  var_imp_df = matrix(0,ncol=5,nrow=ncol(abundance_data_temp_subject_withMetadata))
  rownames(var_imp_df) = colnames(abundance_data_temp_subject_withMetadata)
  for (k in (1:5)) {
    print(k)
    trainIndexes = trainIndex[,k]
    testData <- abundance_data_temp_subject_withMetadata[-trainIndexes, ]
    trainData <- abundance_data_temp_subject_withMetadata[trainIndexes, ]
    condition_test = t1D_status_temp[-trainIndexes]
    condition_train = t1D_status_temp[trainIndexes]
    trainData = cbind(trainData,condition=condition_train)
    testData = cbind(testData,condition=condition_test)
    fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5)

    rf_fit_teddy <- train(as.factor(condition) ~ ., data = trainData,trControl = fitControl,method="rf")
    rf_fit_teddy_metaOnly <- train(as.factor(condition) ~ ., data = trainData[,c(colnames(grs2_fdr_num_autoantibodies_per_subj_df_dummies),"condition")],trControl = fitControl,method="rf")
    
    var_imp_temp = varImp(rf_fit_teddy)
    var_imp_temp = var_imp_temp$importance
    var_imp_temp = var_imp_temp[order(var_imp_temp,decreasing = TRUE),,drop=FALSE]
    var_imp_df[,k] = var_imp_temp[rownames(var_imp_df),]
    # predict outcomes
    pred_outcome <- predict(rf_fit_teddy, testData[,-match("condition",colnames(testData))])
    pred_outcomeMetaOnly <- predict(rf_fit_teddy_metaOnly, testData[,-match("condition",colnames(testData))])
  
    roc_elem = roc(testData[,"condition"],as.numeric(as.character(pred_outcome)),levels=c("0","1"),auc=TRUE,ci=TRUE)
    roc_list[[k]] = roc_elem
    roc_elem_metaOnly = roc(testData[,"condition"],as.numeric(as.character(pred_outcomeMetaOnly)),levels=c("0","1"),auc=TRUE,ci=TRUE)
    roc_list2[[k]] = roc_elem_metaOnly
    confMat = confusionMatrix(pred_outcome, as.factor(testData[,"condition"]))
    F1_scores = confMat$byClass["F1"]
    f1score_list <- c(f1score_list,F1_scores)
    confuMat_list[[k]] = confMat
    
    confMatMetaOnly = confusionMatrix(pred_outcomeMetaOnly, as.factor(testData[,"condition"]))
    F1_scoresMetaOnly = confMatMetaOnly$byClass["F1"]
    f1score_list2 <- c(f1score_list2,F1_scoresMetaOnly)
    confuMat_list2[[k]] = confMatMetaOnly
  }
  
  color_list = c("blue","green","red","orange","black")
  auc_values = sapply(roc_list,function(x) as.numeric(x$auc))
  auc_values2 = sapply(roc_list2,function(x) as.numeric(x$auc))
  auc_values_new = c()

  pdf(paste("roc_curve_lasso_sig_genes_metadata_day",days,".pdf",sep=""))
    plot(roc_list[[1]],col=color_list[1])
    auc_values_new = c(auc_values_new,auc_values[1])
    lines(roc_list2[[1]],col=color_list[1])
    auc_values_new = c(auc_values_new,auc_values2[1])
      for(x in 2:length(roc_list)) {
      lines(roc_list[[x]],col=color_list[x])
      auc_values_new = c(auc_values_new,auc_values[x])
      lines(roc_list2[[x]],col=color_list[x])
      auc_values_new = c(auc_values_new,auc_values2[x])
    }
  legend("bottomright",legend=paste("AUC:",round(auc_values_new,2),sep=""),fill=rep(color_list,each=2))
  dev.off()
  
  return(list(roc_list,roc_list2,f1score_list,f1score_list2,confuMat_list,confuMat_list2,var_imp_df))
}

day_92_res = predictions_diff_times(days=92, abundance_data_temp=lasso_sig_gene_abundances_all_samples_df_intersection,metadata_temp=sample_metadata_ordered)

day_183_res = predictions_diff_times(days=183, abundance_data_temp=lasso_sig_gene_abundances_all_samples_df_intersection,metadata_temp=sample_metadata_ordered)

day_365_res = predictions_diff_times(days=365, abundance_data_temp=lasso_sig_gene_abundances_all_samples_df_intersection,metadata_temp=sample_metadata_ordered)

day_548_res = predictions_diff_times(days=548, abundance_data_temp=lasso_sig_gene_abundances_all_samples_df_intersection,metadata_temp=sample_metadata_ordered)

day_730_res = predictions_diff_times(days=730, abundance_data_temp=lasso_sig_gene_abundances_all_samples_df_intersection,metadata_temp=sample_metadata_ordered)

day_912_res = predictions_diff_times(days=912, abundance_data_temp=lasso_sig_gene_abundances_all_samples_df_intersection,metadata_temp=sample_metadata_ordered)

day_1095_res = predictions_diff_times(days=1095, abundance_data_temp=lasso_sig_gene_abundances_all_samples_df_intersection,metadata_temp=sample_metadata_ordered)


times_df = data.frame(c("",""),c(92,""),c(183,""),c(183,365),c(365,""),c(365,548),c(548,""),c(548,730),c(730,""))
times_df = t(times_df)
times_df = as.data.frame(times_df)
T1D_status = c(TRUE,FALSE,"NA")

library(dplyr)
get_variance = function(times,hasT1D,metadata_temp,abundances_temp) {
  # select samples I want to get varance for
  if(hasT1D == TRUE) {
    metadata_temp = metadata_temp %>% filter(t1d == hasT1D) 
  } else if (hasT1D == FALSE) {
    metadata_temp = metadata_temp %>% filter(t1d == hasT1D)
  }
  if (length(times) == 1) {
    metadata_temp = metadata_temp %>% filter(age_at_collection<=times[1])
  } else if (length(times) == 2) {
    metadata_temp = metadata_temp %>% filter(age_at_collection>=times[1], age_at_collection<=times[2])
  }
  # mow get variance
  abundances_temp_filtered = abundances_temp[,metadata_temp$Run]
  variances=apply(abundances_temp_filtered, 1, sd)
  return(variances)
}

df_list = list()
counter = 1

for (hasT1D in T1D_status) {
  for(x in 1:nrow(times_df)) {
    my_times = times_df[x,]
    my_times = unlist(my_times)
    my_times = my_times[my_times!=""]
    my_times = as.numeric(my_times)
    variances_temp = get_variance(times=my_times,hasT1D=hasT1D,metadata_temp=tedd_metadata_allsamples,abundances_temp=lasso_sig_gene_abundances_all_samples_df)
    if(length(my_times) == 0) {
      my_times_str = "all_times"
    } else {
    my_times_str = round(my_times/30)
    my_times_str = paste(my_times_str,"months",sep="")
    my_times_str = paste(my_times_str,collapse="_")
    }

    variance_df = data.frame(names(variances_temp),variances_temp,T1D_status=hasT1D,time=my_times_str)
    df_list[[counter]] = variance_df
    counter = counter + 1
  }
}

df_list_var_df = do.call("rbind",df_list)
# make a line plot for each variable

times_order = c("all_times","3months","6months","12months","18months","24months","6months_12months","12months_18months","18months_24months")

pdf("variance_line_plots_lasso_sig_genes.pdf",width=12)
for (mygene in lasso_sig_genes) {
  df_list_var_df_temp = df_list_var_df[mygene == df_list_var_df[,1],]
  
  df_list_var_df_temp$time = factor(df_list_var_df_temp$time,levels=times_order)
  myplot = ggplot(df_list_var_df_temp,aes(x=time,y=variances_temp,group=T1D_status)) + ggtitle(unique(df_list_var_df_temp[,1])) + geom_line(aes(x=time,color=T1D_status)) + geom_point(aes(x=time,color=T1D_status))
  print(myplot)
}
dev.off()
```

#Lets get genes that are significant after doing lasso regression

```{r}
library(glmnet)
library(ggplot2)
library(RNOmni)
sigGenes = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output_tenPerc_cutoff_training_only/healthy_pre-t1d-all_HLA_tenPerc_cutoff_output/healthy_pre-t1d-all_HLAtenPerc_cutoff_output_full_association_output_adjusted_sigGenes_topFivePercORs.tsv",sep="\t",header=TRUE)

sigGenes = sigGenes$feature

gene_to_file_mapping = read.table("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/gene_locs.txt",header=FALSE)
gene_to_file_mapping_sig = gene_to_file_mapping[match(sigGenes,gene_to_file_mapping$V1),]

suffixes = gene_to_file_mapping_sig[,2]
suffixes = gsub(".csv","",suffixes)
suffixes = unique(suffixes)

abundance_files = paste("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-all_HLA/healthy_pre-t1d-all_HLA_train_",suffixes,".rds",sep="")

abundances_of_sig_genes = lapply(abundance_files, function(x) {
  print(x)
  abundance_temp = readRDS(x)
  abundance_temp_sig = abundance_temp[,colnames(abundance_temp)%in%sigGenes,drop=FALSE]
  return(abundance_temp_sig)
})

abundances_of_sig_genes_df = do.call("cbind",abundances_of_sig_genes)

# for each gene add minimum non zero and log transform
abundances_of_sig_genes_df_log = apply(abundances_of_sig_genes_df,2, function(x) {
  #min_val = min(x[x>0])
  #x = x + min_val
  #x = log(x)
  x = RankNorm(x)
  return(x)
})

# spearman correlations
#cor_matrix = cor(abundances_of_sig_genes_df_log,method="spearman")
# now make heatmap
#library(pheatmap)
#pdf("sig_genes_correlation_matrix_pret1d_vs_healthy.pdf")
#pheatmap(cor_matrix,show_rownames=FALSE,show_colnames=FALSE)
#dev.off()

# get metadata
abundance_temp = readRDS(abundance_files[1])
rownames(abundances_of_sig_genes_df_log) = abundance_temp$SubjectID

metadata = readRDS("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-all_HLA/healthy_pre-t1d-all_HLA_train_1_metadata_filtered.rds")

metadata = metadata[match(rownames(abundances_of_sig_genes_df_log),metadata$SubjectID),]

# first thing to do is to find the correct lambda penalty
set.seed(123)
lambda_seq <- 10^seq(2, -2, by = -.1)

cv_output <- cv.glmnet(abundances_of_sig_genes_df_log, metadata$condition,
                       alpha = 1, lambda = lambda_seq, 
                       nfolds = 5,family="binomial")

# identifying best lamda
best_lam <- cv_output$lambda.min

lasso_best <- glmnet(abundances_of_sig_genes_df_log, metadata$condition, alpha = 1, lambda = best_lam,family="binomial")
# get non 0 variables

best_vars = coef(lasso_best)

myVars = best_vars[best_vars[,1]>0,]

saveRDS(myVars,"/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/lasso_coefficients_training_data_healthy_pre-t1d-all_HLA.rds")

# now get lasso sig genes abundance only

var_names = names(myVars)

# now get sample level data lasso sig genes

gene_to_file_mapping_lasso = gene_to_file_mapping[match(var_names,gene_to_file_mapping$V1),]

suffixes = gene_to_file_mapping_lasso[,2]
suffixes = gsub(".csv","",suffixes)
suffixes = unique(suffixes)


abundance_files_samplelevel = paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/",suffixes,".csv",sep="")

library(dplyr)
library(data.table)
abundances_of_sig_genes_test = lapply(abundance_files_samplelevel, function(x) {
  print(x)
  d_small = fread(x,sep=",",header=TRUE,data.table=FALSE,nrow=1) %>% select(-V1)
  ## aa.csv will have an extra row for the gene names so lets remove that
  has_extra_row = d_small[1,1]=="genename"
  if(has_extra_row == TRUE) {
    d = fread(x,sep=",",header=TRUE,data.table=FALSE,skip=1)
    d = d[,-1]
  } else {
    d = fread(x,sep=",",header=TRUE,data.table=FALSE) %>% select(-V1)
  }
  rownames(d) = d$genename
  d = d[,-1]
  found_genes = var_names[var_names%in%rownames(d)]
  abundance_temp_sig = d[rownames(d)%in%found_genes,,drop=FALSE]
  rm(d)
  return(abundance_temp_sig)
})
abundances_of_sig_genes_test_df = do.call("rbind",abundances_of_sig_genes_test)

write.csv(abundances_of_sig_genes_test_df,file="/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/lasso_sig_genes_from_training_data_raw_abundance_data_healthy_pre-t1d-all_HLA.csv")

```

###Lets try doing a zero-inflated mixed effect model

#First thing to do is count the number of aligned reads in each sample

```{bash}
#for x in /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments/*_alignment_data.tsv.gz
#do
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis
#awk 'NR>1 {for(i=1;i<=NF;i++)$i=(a[i]+=$i)}END{print}' /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_abundance_matrix_all_samples.tsv > num_aligned_reads_per_sample.txt

ls /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_input_files_*_output.tsv

mkdir aligned_read_per_sample

sbatch -c 1 -t 0-00:30 -p short --mem=75G  scripts/calculateAligned_read.bash /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_input_files_aa_output.tsv aligned_read_per_sample/raw_input_files_aa_output.tsv

for x in /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_input_files_a[b-z]_output.tsv;
do
outputFile=$(basename ${x})
sbatch -c 1 -t 0-00:20 -p short --mem=70G  scripts/calculateAligned_read.bash ${x} aligned_read_per_sample/${outputFile}
done

cat *_output.tsv > all_samples_aligned_read.tsv
```


```{r}
args = commandArgs(trailingOnly=TRUE)
input_abundances = args[1] # e.g. /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_abundance_matrix_all_samples_split_by_genes/raw_abundance_matrix_all_samples_split_mb
output = args[2]
sample_metadata=args[3]  # /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv
num_aliged_reads_file=args[4] # /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/aligned_read_per_sample/all_samples_aligned_read.tsv
thread_number=as.numeric(args[5])

library(NBZIMM)
library(nlme)
library(data.table)
library(parallel)
library(dplyr)

# first format metadata

metadata = read.csv(sample_metadata)
num_aliged_reads_file_df = read.table(num_aliged_reads_file,sep="\t",header=FALSE)
metadata$num_aligned_reads = num_aliged_reads_file_df[match(metadata$Run,num_aliged_reads_file_df[,1]),2]
# filter metadata samples to only include those that don't have or never get T1D
metadata = metadata[metadata$T1D_Outcome != "After",]
# make the group variable numeric
metadata$group = as.numeric(metadata$t1d)

# read in gene abundance data
raw_gene_counts = fread(input_abundances,sep="\t",header=FALSE,data.table=FALSE,nrow=1)
if(raw_gene_counts[1,1] == "gene") {
  raw_gene_counts = fread(input_abundances,sep="\t",header=TRUE,data.table=FALSE)
} else {
  raw_gene_counts = fread(input_abundances,sep="\t",header=FALSE,data.table=FALSE)
  header = fread("/n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_abundance_matrix_all_samples_split_by_genes/raw_abundance_matrix_all_samples_split_aa",header=FALSE,data.table=FALSE,nrows=1)
  header = as.character(header[1,])
  colnames(raw_gene_counts) = header
}
# get the names of the header
rownames(raw_gene_counts) = raw_gene_counts[,1]
raw_gene_counts = raw_gene_counts[,-1]
raw_gene_counts = t(raw_gene_counts)
# find the runs that are in both abundance and metadata
samples_both_counts_metadata = intersect(rownames(raw_gene_counts),metadata$Run)
raw_gene_counts_filt = raw_gene_counts[samples_both_counts_metadata,]
metadata_filt = metadata[match(samples_both_counts_metadata,metadata$Run),]
# remove genes with low percent of abundances
gene_to_keep = colSums(raw_gene_counts_filt>0)/nrow(raw_gene_counts_filt) > 0.10
raw_gene_counts_filt = raw_gene_counts_filt[,gene_to_keep]

if(thread_number>1) {
  model_results = mclapply(colnames(raw_gene_counts_filt), function(gene_name_temp) {
    metadata_temp = cbind.data.frame(metadata_filt,gene_abundance=raw_gene_counts_filt[,gene_name_temp])
    myModel = glmm.zinb(gene_abundance ~ group + age_at_collection+offset(log(num_aligned_reads)),data = metadata_temp,random = ~ 1|m138_maskid, zi_fixed = ~group, zi_random =NULL,na.action=na.omit)
    results = summary(myModel)$tTable["group",]
    results = c(results,feature=gene_name_temp)
    return(results)
  },mc.cores=thread_number)
} else {
  model_results = lapply(colnames(raw_gene_counts_filt), function(gene_name_temp) {
    metadata_temp = cbind.data.frame(metadata_filt,gene_abundance=raw_gene_counts_filt[,gene_name_temp])
    myModel = glmm.zinb(gene_abundance ~ group + offset(log(num_aligned_reads)),data = metadata_temp,random = ~ 1|m138_maskid, zi_fixed = ~1, zi_random =NULL,na.action=na.omit)
    results = summary(myModel)$tTable["group",]
    results = c(results,feature=gene_name_temp)
    return(results)
  })
}
model_results_df = bind_rows(model_results)
colnames(model_results_df) = c("Value","Std.Error","DF","tvalue","pvalue","feature")

write.table(model_results_df,file=output,sep="\t",col.names=TRUE,row.names=FALSE,quote=FALSE)
```

```{bash}

mkdir zinb_ouptut

ls /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_abundance_matrix_all_samples_split_by_genes/raw_abundance_matrix_all_samples_split_* | grep -v "_filtered.tsv" > zinb_inputfiles.txt

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/

head -n 1 zinb_inputfiles.txt > test_zinb.txt
while read line
do
outputFile=$(basename ${line})
sbatch -c 10 -t 0-11:59 -p short --mem=30G scripts/run_zinb.bash ${line} zinb_ouptut/${outputFile}_zinb_out.tsv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/aligned_read_per_sample/all_samples_aligned_read.tsv 10
done < test_zinb.txt
# skip first line. we already did it
tail -n +2 zinb_inputfiles.txt > zinb_inputfiles2.txt

while read line
do
outputFile=$(basename ${line})
sbatch -c 10 -t 0-11:59 -p short --mem=30G scripts/run_zinb.bash ${line} zinb_ouptut/${outputFile}_zinb_out.tsv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/aligned_read_per_sample/all_samples_aligned_read.tsv 10
done < zinb_inputfiles2.txt

```

```{r}
#Get jobs that didn't finish

output_files = list.files("zinb_ouptut")
output_files = gsub("_zinb_out.tsv","",output_files)

inputFiles = read.table("zinb_inputfiles.txt",header=FALSE)
inputFiles$base = basename(inputFiles[,1])

timedOut = setdiff(inputFiles$base,output_files)

filesToDo = inputFiles[match(timedOut,inputFiles$base),]
write.table(filesToDo[,1],col.names=FALSE,row.names=FALSE,quote=FALSE,file="zinb_inputfiles_timedOut.txt")
```

#Now rerun timed out

```{bash}
while read line
do
outputFile=$(basename ${line})
sbatch -c 15 -t 1-00:00 -p medium --mem=30G scripts/run_zinb.bash ${line} zinb_ouptut/${outputFile}_zinb_out.tsv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/teddy_metadata_20190821_with_GRS2.csv /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/aligned_read_per_sample/all_samples_aligned_read.tsv 15
done < zinb_inputfiles_timedOut.txt

```



#Get the genes in each file so we can get the abundances of individual genes easily

```{bash}
cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis
ls /n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_abundance_matrix_all_samples_split_by_genes/raw_abundance_matrix_all_samples_split_* | grep -v "_filtered.tsv" | while read line; do awk '{print $1,FILENAME}' ${line} ; done > raw_abundance_mapping_files.txt
```


#Next lets see if we have significant genes?!

```{r}
library(data.table)
setwd("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis")
myfiles = list.files("zinb_ouptut",pattern="_zinb_out.tsv",full.names = TRUE)

dt_list = lapply(myfiles, function(x) fread(x,sep="\t",header=TRUE))

all_dts = rbindlist(dt_list)
all_df = as.data.frame(all_dts)
length(which(is.na(all_df[,1]))) # 22444 NAs. didn't converge
length(which(is.na(all_df[,1])))/nrow(all_df) # 22444 NAs. about 0.265%

all_df = all_df[!is.na(all_df[,1]),]

all_df = all_df[order(all_df$pvalue),]
all_df$BH = p.adjust(all_df$pvalue,method="BH")
all_df$bonferroni = p.adjust(all_df$pvalue,method="bonferroni")
all_df$BY = p.adjust(all_df$pvalue,method="BY")

# get BY significant less than 0.1
all_df_sig = all_df[all_df$BY < 0.1,] # 216 significant genes

# lets see what a "significant gene actually loooks like"


gene_to_file_mapping = fread("raw_abundance_mapping_files.txt",header=FALSE,data.table=FALSE)
# remove "gene"
gene_to_file_mapping = gene_to_file_mapping[-match("gene",gene_to_file_mapping[,1]),]
num_aliged_reads_per_sample = read.table("/n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/aligned_read_per_sample/all_samples_aligned_read.tsv",sep="\t",header=FALSE)
gene_to_file_mapping_sig = gene_to_file_mapping[match(all_df_sig$feature,gene_to_file_mapping$V1),]
gene_to_file_mapping_sig_split = split(gene_to_file_mapping_sig,gene_to_file_mapping_sig$V2)
suffixes = gene_to_file_mapping_sig[,2]
suffixes = unique(suffixes)

myMat = matrix(0,nrow=nrow(all_df_sig),ncol=13159)
geneName_list = rep('',nrow(all_df_sig))

geneCounter = 0

for (prefixNum in 1:length(suffixes)) {
  print(prefixNum)
  x = gene_to_file_mapping_sig_split[[prefixNum]]
  genes_temp = x[,1]
  genes_temp = paste(genes_temp,collapse="|")
  genes_temp = paste("'",genes_temp,"'",sep="")
  prefix_temp = unique(x[,2])
  #df_temp = fread(cmd = paste("grep -E ",genes_temp," /n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data/",prefix_temp,sep=""),sep=",",header=FALSE,data.table=FALSE)
  df_temp = fread(cmd = paste("grep -E ",genes_temp," ",prefix_temp,sep=""),sep="\t",header=FALSE,data.table=FALSE)
  rownames(df_temp) = df_temp[,1]
  df_temp = df_temp[,-1]
  df_temp = as.matrix(df_temp)
  if(geneCounter == 0) {
    myMat[1:nrow(df_temp),] = df_temp
    geneName_list[1:nrow(df_temp)] = rownames(df_temp)
  } else {
    myMat[seq(geneCounter+1,geneCounter+nrow(df_temp)),] = df_temp
    geneName_list[seq(geneCounter+1,geneCounter+nrow(df_temp))] = rownames(df_temp)
  }
  geneCounter = geneCounter + nrow(df_temp)
}

rownames(myMat) = geneName_list
myMat = as.data.frame(myMat)
# load in column Names
abundance_colNames = fread("/n/scratch3/users/l/ldp9/_RESTORE/TEDDY_raw_alignments_combined/raw_abundance_matrix_all_samples_split_by_genes/raw_abundance_matrix_all_samples_split_aa",sep="\t",nrow=1,header=FALSE,data.table=FALSE)
abundance_colNames = abundance_colNames[,-1]
colnames(myMat) = abundance_colNames
write.csv(myMat,file="zinb_sig_genes_normalized_abundance.csv")
# normalize data
num_aliged_reads_per_sample_ordered = num_aliged_reads_per_sample[match(colnames(myMat),num_aliged_reads_per_sample[,1]),]
myMat_normalized = matrix(0,ncol=ncol(myMat),nrow=nrow(myMat))
for(x in 1:ncol(myMat_normalized)) {
  myMat_normalized[,x] = myMat[,x]/num_aliged_reads_per_sample_ordered[x,2]
}
rownames(myMat_normalized) = rownames(myMat)
colnames(myMat_normalized) = colnames(myMat)


# make sure that I did this correctly. lets get some random genes and compare to final df above

#sig_gene_abundances_all_samples = lapply(suffixes[1:5], function(x) {
#  print(x)
#  abundance_temp = fread(paste("/n/scratch3/users/s/sez10/_RESTORE/TEDDY/parsed_data",x,sep="/"),sep=",",header=TRUE)
#  abundance_temp_sig = abundance_temp[genename%in%all_df_sig$feature,]
#  return(abundance_temp_sig)
#})

#sig_gene_abundances_all_samples = rbindlist(sig_gene_abundances_all_samples)
#sig_gene_abundances_all_samples = as.data.frame(sig_gene_abundances_all_samples)
#sig_gene_abundances_all_samples = sig_gene_abundances_all_samples[,-1]
#rownames(sig_gene_abundances_all_samples) = sig_gene_abundances_all_samples[,1]
#sig_gene_abundances_all_samples = sig_gene_abundances_all_samples[,-1]

#all.equal(sig_gene_abundances_all_samples,myMat[rownames(sig_gene_abundances_all_samples),])

# rank normalize
library(RNOmni)
myMat_rankNorm = apply(myMat_normalized,1, function(x) {
    return(RankNorm(x))
  })
myMat_rankNorm = t(myMat_rankNorm)

# first adjust values to log normalize
myMat_log = apply(myMat_normalized,1, function(x) {
  min_val = min(x[x>0])
  x = x + min_val
  x = log(x)
  return(x)
})
myMat_log = t(myMat_log)

all.equal(colnames(myMat_log),colnames(myMat_rankNorm))
all.equal(rownames(myMat_log),rownames(myMat_rankNorm))

# load in metadata for all samples
tedd_metadata_allsamples = read.csv("teddy_metadata_20190821_with_GRS2.csv")

# find samples we have abundance data and metadata for
metadata_abundance_samples = intersect(tedd_metadata_allsamples$Run,colnames(myMat_log))

tedd_metadata_allsamples = tedd_metadata_allsamples[match(metadata_abundance_samples,tedd_metadata_allsamples$Run),]
myMat_log = myMat_log[,metadata_abundance_samples]
all.equal(tedd_metadata_allsamples$Run,colnames(myMat_log))
#order by significance
myMat_log = myMat_log[all_df_sig$feature,]
# make abundance of each gene as a function of time
library(ggplot2)

# do the same thing for the non-log normalized values
myMat_normalized_sub = myMat_normalized[,metadata_abundance_samples]
myMat_normalized_sub = myMat_normalized_sub[all_df_sig$feature,]
# also do the same thing for Rank Normalized

myMat_rankNorm = myMat_rankNorm[,metadata_abundance_samples]
myMat_rankNorm = myMat_rankNorm[all_df_sig$feature,]


pdf("zinb_sig_hist.pdf")
sapply(rownames(myMat_log), function(geneName) {
  gene_temp_abundance = myMat_log[geneName,,drop=FALSE]
  # remove people who already have T1D
  currently_hasT1D = tedd_metadata_allsamples$age_at_collection >= tedd_metadata_allsamples$age_t1d
  metadata_t1d_status = tedd_metadata_allsamples$t1d_sero_control
  metadata_t1d_status[metadata_t1d_status == "seroconverted"] = "control"
  abundance_temp_dataframe = data.frame(abundance=as.numeric(gene_temp_abundance),age=tedd_metadata_allsamples$age_at_collection,t1d_status=metadata_t1d_status,currently_hasT1D)
  # remove people who already have T1D
  abundance_temp_dataframe = abundance_temp_dataframe[-which(abundance_temp_dataframe$currently_hasT1D == TRUE),]
  myplot = ggplot(abundance_temp_dataframe,aes(x=abundance,after_stat(density),color=t1d_status)) + ggtitle(label=geneName) + geom_freqpoly(binwidth = 1)
  print(myplot)
})
dev.off()

pdf("zinb_sig_zeroCounts.pdf")
sapply(rownames(myMat_normalized_sub), function(geneName) {
  gene_temp_abundance = myMat_normalized_sub[geneName,,drop=FALSE]
  # remove people who already have T1D
  currently_hasT1D = tedd_metadata_allsamples$age_at_collection >= tedd_metadata_allsamples$age_t1d
  metadata_t1d_status = tedd_metadata_allsamples$t1d_sero_control
  metadata_t1d_status[metadata_t1d_status == "seroconverted"] = "control"
  abundance_temp_dataframe = data.frame(abundance=as.numeric(gene_temp_abundance),age=tedd_metadata_allsamples$age_at_collection,t1d_status=metadata_t1d_status,currently_hasT1D)
  abundance_temp_dataframe = abundance_temp_dataframe[-which(abundance_temp_dataframe$currently_hasT1D == TRUE),]
  control_abundances = abundance_temp_dataframe[abundance_temp_dataframe$t1d_status=="control","abundance"]
  t1D_abundances = abundance_temp_dataframe[abundance_temp_dataframe$t1d_status=="t1d","abundance"]
  prop_zeros_control = sum(control_abundances==0)/length(control_abundances)
  prop_zeros_t1d = sum(t1D_abundances==0)/length(t1D_abundances)
  barplot(c(prop_zeros_control,prop_zeros_t1d),main=paste("Proportion of 0 counts",geneName), names.arg=c("Ctrl","T1D"))
})
dev.off()


pdf("zinb_sig_scatter.pdf")
sapply(rownames(myMat_log), function(geneName) {
  gene_temp_abundance = myMat_log[geneName,,drop=FALSE]
  # remove people who already have T1D
  currently_hasT1D = tedd_metadata_allsamples$age_at_collection >= tedd_metadata_allsamples$age_t1d
  metadata_t1d_status = tedd_metadata_allsamples$t1d_sero_control
  metadata_t1d_status[metadata_t1d_status == "seroconverted"] = "control"
  abundance_temp_dataframe = data.frame(abundance=as.numeric(gene_temp_abundance),age=tedd_metadata_allsamples$age_at_collection,t1d_status=metadata_t1d_status,currently_hasT1D)
  # remove people who already have T1D
  abundance_temp_dataframe = abundance_temp_dataframe[-which(abundance_temp_dataframe$currently_hasT1D == TRUE),]
  myplot = ggplot(abundance_temp_dataframe,aes(x=age,y=abundance,color=t1d_status)) + geom_point(size=0.3) + geom_smooth(method="loess") + ggtitle(label=geneName)
  print(myplot)
})
dev.off()


pdf("zinb_sig_boxplot.pdf")
sapply(rownames(myMat_log), function(geneName) {
  gene_temp_abundance = myMat_log[geneName,,drop=FALSE]
  # remove people who already have T1D
  currently_hasT1D = tedd_metadata_allsamples$age_at_collection >= tedd_metadata_allsamples$age_t1d
  metadata_t1d_status = tedd_metadata_allsamples$t1d_sero_control
  metadata_t1d_status[metadata_t1d_status == "seroconverted"] = "control"
  abundance_temp_dataframe = data.frame(abundance=as.numeric(gene_temp_abundance),age=tedd_metadata_allsamples$age_at_collection,t1d_status=metadata_t1d_status,currently_hasT1D)
  # remove people who already have T1D
  abundance_temp_dataframe = abundance_temp_dataframe[-which(abundance_temp_dataframe$currently_hasT1D == TRUE),]
  myplot = ggplot(abundance_temp_dataframe,aes(x=t1d_status,y=abundance)) + geom_boxplot() + ggtitle(label=geneName)
  print(myplot)
})
dev.off()

pdf("zinb_sig_boxplot_rankNorm.pdf")
sapply(rownames(myMat_rankNorm), function(geneName) {
  gene_temp_abundance = myMat_rankNorm[geneName,,drop=FALSE]
  # remove people who already have T1D
  currently_hasT1D = tedd_metadata_allsamples$age_at_collection >= tedd_metadata_allsamples$age_t1d
  metadata_t1d_status = tedd_metadata_allsamples$t1d_sero_control
  metadata_t1d_status[metadata_t1d_status == "seroconverted"] = "control"
  abundance_temp_dataframe = data.frame(abundance=as.numeric(gene_temp_abundance),age=tedd_metadata_allsamples$age_at_collection,t1d_status=metadata_t1d_status,currently_hasT1D)
  # remove people who already have T1D
  abundance_temp_dataframe = abundance_temp_dataframe[-which(abundance_temp_dataframe$currently_hasT1D == TRUE),]
  myplot = ggplot(abundance_temp_dataframe,aes(x=t1d_status,y=abundance)) + geom_boxplot() + ggtitle(label=geneName)
  print(myplot)
})
dev.off()


```


#Lets try not doing any linear regression. Just straight survival. we are going to try this 3 different ways. 

#Method one. Different survival for each baseline

```{bash}

cd /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis

sbatch -n 1 -c 1 --mem=130G -p short -t 0-03:00 scripts/run_survival_rankNorm.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-24month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 730 fdr,grs2,number_autoantibodies fdr,grs2,number_autoantibodies,microbiome_gene_abundances healthy_pre-t1d-24month-all_HLA_clinical_microbiome_pred.rds 1

sbatch -n 1 -c 1 --mem=130G -p short -t 0-06:00 scripts/run_survival_rankNorm_multi_predictions.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-24month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 730 vars_to_keep_file.txt healthy_pre-t1d-24month 1


sbatch -n 1 -c 5 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-3month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 92 5

sbatch -n 1 -c 5 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-6month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 183 5

sbatch -n 1 -c 5 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-6month-12month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 365 5

sbatch -n 1 -c 5 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-12month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 365 5

sbatch -n 1 -c 5 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-12month-18month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 548 5

sbatch -n 1 -c 5 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-18month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 548 5

sbatch -n 1 -c 5 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-18month-24month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 730 5

sbatch -n 1 -c 10 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-24month-all_HLA 0.9 teddy_metadata_20190821_with_GRS2.csv T1D 730 10

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-3month-all_HLA 92 vars_to_keep_file.txt healthy_pre-t1d-3month 5 1 0.9

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-6month-all_HLA 183 vars_to_keep_file.txt healthy_pre-t1d-6month 5 1 0.9

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-6month-12month-all_HLA 365 vars_to_keep_file.txt healthy_pre-t1d-6month-12month 5 1 0.9

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-12month-all_HLA 365 vars_to_keep_file.txt healthy_pre-t1d-12month 5 1 0.9

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-12month-18month-all_HLA 548 vars_to_keep_file.txt healthy_pre-t1d-12month-18month-all_HLA 5 1 0.9

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-18month-all_HLA 548 vars_to_keep_file.txt healthy_pre-t1d-18month-all_HLA 5 1 0.9

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-18month-24month-all_HLA 730 vars_to_keep_file.txt healthy_pre-t1d-18month-24month-all_HLA 5 1 0.9

sbatch -n 1 -c 7 --mem=300G -p highmem -t 0-6:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-24month-all_HLA 730 vars_to_keep_file.txt healthy_pre-t1d-24month 5 1 0.9



# lets use each clinical variable individually for fun :)

sbatch -n 1 -c 1 --mem=100G -p short -t 0-1:00 scripts/run_survival_rankNorm_multi_predictions_v3.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-24month-all_HLA 730 clinical_pred_only.txt healthy_pre-t1d-24month 1 1 0.9



sbatch -n 1 -c 10 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA-24month 0.90 teddy_metadata_20190821_with_GRS2.csv MIAA 730 10

sbatch -n 1 -c 10 --mem=50G -p short -t 0-02:00 scripts/filter_normalize_abundance_metadata_data.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA-24month 0.95 teddy_metadata_20190821_with_GRS2.csv MIAA 730 10

sbatch -n 1 -c 5 --mem=300G -p highmem -t 0-06:00 scripts/run_survival_rankNorm_multi_predictions_v2.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA-24month 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month 5

# lasso regression
sbatch -n 1 -c 5 --mem=300G -p highmem -t 1-00:00 scripts/run_survival_rankNorm_multi_predictions_v2.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA-24month 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month 5 1

sbatch -n 1 -c 5 --mem=300G -p highmem -t 1-00:00 scripts/run_survival_rankNorm_multi_predictions_v2.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA-24month 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month 5 1

sbatch -n 1 -c 5 --mem=300G -p highmem -t 1-00:00 scripts/run_survival_rankNorm_multi_predictions_v2.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA-24month 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month 5 1


sbatch -n 1 -c 1 --mem=50G -p short -t 0-01:00 scripts/prep_for_elasticNet.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-MIAA-24month 730 healthy_pre-MIAA-24month 0.95

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 1 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.1 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.2 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.3 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.4 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.5 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.6 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.7 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.8 3

sbatch -n 1 -c 5 --mem=150G -p short -t 0-03:00 scripts/train_elastic_net.bash healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95 0.9 3

sbatch -n 1 -c 1 --mem=20G -p short -t 0-00:10 scripts/get_best_elastic_model.bash healthy_pre-MIAA-24month_0.95 healthy_pre-MIAA-24month_0.95_best_model.rds

sbatch -n 1 -c 1 --mem=50G -p short -t 0-00:20 scripts/test_elastic_net.bash healthy_pre-MIAA-24month_0.95_best_model.rds healthy_pre-MIAA-24month_0.95_output_list.rds 730 vars_to_keep_file_antibodies.txt healthy_pre-MIAA-24month_0.95_predictions.rds


#sbatch -n 1 -c 6 --mem=400G -p highmem -t 1-00:00 scripts/run_survival_rankNorm_multi_predictions_v2_big.bash /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/parsed_data_2/healthy_pre-t1d-24month-all_HLA 730 vars_to_keep_file.txt healthy_pre-t1d-24month_big 4




```

## get results

```{r}
library(glmnet)

gene_abundances_optional_results = list.files(pattern="_gene_abundances_optional")
gene_abundances_optional_results = gene_abundances_optional_results[grep("_alpha_1",gene_abundances_optional_results)]

clinical_only_results = list.files(pattern="_all_clincal_alpha_1_")

abundances_only = list.files(pattern="_all_gene_abundances_")
abundances_only = abundances_only[grep("healthy_pre-t1d-",abundances_only)]

getAUCs = function(myrdsList,mode) {
  all_res = lapply(myrdsList, function(myrds) {
      mylist = readRDS(myrds)
      train_AUCs = mylist[[1]]$AUC
      test_AUCs  = mylist[[2]]$AUC
      res_df = data.frame(train=train_AUCs,test=test_AUCs)
      res_df$horizon = c("1","3","5","8")
      time = strsplit(myrds,split="_")[[1]][c(2)]
      time = gsub("pre-t1d-","",time)
      time = gsub("month","",time)
      res_df$landscape = time
      res_df$mode = mode
      return(res_df)
  }) 
  all_res_df = do.call("rbind",all_res)
  all_res_df$landscape = gsub("-all","",all_res_df$landscape)
  all_res_df$landscape = factor(all_res_df$landscape,levels=c("3","6","6-12","12","12-18","18","18-24","24"))
  all_res_df$landscape_horizon = paste(all_res_df$landscape,all_res_df$horizon,sep="_")
  all_res_df$horizon = paste(all_res_df$horizon,"year")
  all_res_df$horizon = as.factor(all_res_df$horizon)
  return(all_res_df)
}

gene_abundances_optional_df = getAUCs(myrdsList=gene_abundances_optional_results,mode="AA+FDR+GRS+Micro")
clinical_results_df = getAUCs(myrdsList=clinical_only_results,mode="AA+FDR+GRS")
gene_abundances_only_results_df = getAUCs(myrdsList=abundances_only,mode="AA+FDR+GRS")

gene_abundances_optional_df_order = gene_abundances_optional_df[match(clinical_results_df$landscape_horizon,gene_abundances_optional_df$landscape_horizon),]

clinical_results_df$AUC_difference = clinical_results_df$test - gene_abundances_optional_df_order$test

library(ggplot2)
pdf("healthy_pre-t1d-all_HLA_clincal_vs_clinicalPlusMicrobiome.pdf")
ggplot(clinical_results_df,aes(x=landscape,y=AUC_difference,group=horizon,color=horizon)) + geom_point(size=3) + geom_line() + theme_bw() + geom_hline(yintercept=0,color="red") + ylab("Difference in AUC") + xlab("Landscape (Months)")
dev.off()

pdf("healthy_pre-t1d-all_HLA_clincal.pdf")
ggplot(clinical_results_df,aes(x=landscape,y=test,group=horizon,color=horizon)) + geom_point(size=3) + theme_bw() + theme_classic() + ylim(0.5,1) + geom_line() + ylab("AUC") + xlab("Landscape (Months)")
dev.off()

pdf("healthy_pre-t1d-all_HLA_microbiome.pdf")
ggplot(gene_abundances_only_results_df,aes(x=landscape,y=test,group=horizon,color=horizon)) + geom_point(size=3) + theme_bw() + theme_classic() + ylim(0,1) + geom_line() + ylab("AUC") + xlab("Landscape (Months)")
dev.off()

pdf("healthy_pre-t1d-all_HLA_microbiome_train.pdf")
ggplot(gene_abundances_only_results_df,aes(x=landscape,y=train,group=horizon,color=horizon)) + geom_point(size=3) + theme_bw() + theme_classic() + ylim(0,1) + geom_line(position=position_jitter(w=0.02, h=0)) + ylab("AUC") + xlab("Landscape (Months)")
dev.off()

```






#Upload files to bucket after putting them in tar files

```{bash}
sbatch -c 1 -t 0-11:59 -p short --mem=10G upload_to_bucket.bash healthy_pre-t1d-all_HLA.tar
sbatch -c 1 -t 0-11:59 -p short --mem=10G upload_to_bucket.bash healthy_pre-t1d-DR3_DR4_only.tar
sbatch -c 1 -t 0-11:59 -p short --mem=10G upload_to_bucket.bash healthy_pre-t1d-not_DR3_DR4.tar

# put files from bucket onto cloud example
curl -X GET https://objectstorage.eu-zurich-1.oraclecloud.com/p/_x7tCMxB5He1XJ9UI6SVAMrdXYCClDBVzSx-PRqycPCzi6PAVmiJH9qnfQ691FWw/n/zrjwsvatolrg/b/TEDDY_VOE_input_March19_2022/o/healthy_pre-t1d-DR3_DR4_only.tar --output healthy_pre-t1d-DR3_DR4_only.tar
```

```{bash}
# on cloud. run quantvoe initial regressions. using node with 64 cpus and 683 memory.

# mount commands
sudo mkdir -p /mnt/NFS
sudo mount 10.0.1.131:/NFS /mnt/NFS
cd /mnt/NFS/TEDDY_voe_only_march_19_2022

# screen into node with command "screen"
#./config.sh healthy_pre-t1d-all_HLA 105 # this command is one per instance
./config.sh healthy_pre-t1d-all_HLA 126 # this command is one per instance
./config.sh healthy_pre-t1d-DR3_DR4_only 63
./config.sh healthy_pre-t1d-not_DR3_DR4 63
# exit screen with ctrl+a d

# to resume screen screen -r if multiple you can do with id e.g. screen -r 10835
# screen IDs screen -ls

```

#Put things from cloud to bucket

```{bash}
# this is done from the cloud
./put_output_in_bucket.bash healthy_pre-t1d-all_HLA healthy_pre-t1d-all_HLA_output 
./put_output_in_bucket.bash healthy_pre-t1d-DR3_DR4_only healthy_pre-t1d-DR3_DR4_only_output 
./put_output_in_bucket.bash healthy_pre-t1d-not_DR3_DR4 healthy_pre-t1d-not_DR3_DR4_output 

# this is done on O2 cluster when I want to move the folder in the bucket to O2
#./scripts/get_files_from_bucket.bash association_output_full_names.txt healthy_pre-t1d-all_HLA_output /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output
#source activate r_env
#Rscript scripts/parse_initial_association_output.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-all_HLA_output

sbatch -c 1 -t 0-0:20 -p short --mem=10G scripts/get_files_from_bucket_parse_initial_associations.bash association_output_full_names.txt healthy_pre-t1d-all_HLA_output /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output

sbatch -c 1 -t 0-00:20 -p short --mem=10G scripts/get_files_from_bucket_parse_initial_associations.bash association_output_full_names.txt healthy_pre-t1d-DR3_DR4_only_output /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output

sbatch -c 1 -t 0-00:20 -p short --mem=10G scripts/get_files_from_bucket_parse_initial_associations.bash association_output_full_names.txt healthy_pre-t1d-not_DR3_DR4_output /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output


# compute vibrations
run_vibrations.R /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_alignments/normalized_alignments/parsed_data/healthy_pre-t1d-all_HLA /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/initial_association_output/healthy_pre-t1d-all_HLA_output/healthy_pre-t1d-all_HLA_output_full_association_output_adjusted.rds /n/data1/joslin/icrb/kostic/szimmerman/TEDDY_analysis/vibrations_output/healthy_pre-t1d-all_HLA
```

